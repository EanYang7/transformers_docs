
<!doctype html>
<html lang="zh" class="no-js">
  <head>
    
      <meta charset="utf-8">
      <meta name="viewport" content="width=device-width,initial-scale=1">
      
        <meta name="description" content="transformers_docs">
      
      
        <meta name="author" content="Ean Yang">
      
      
        <link rel="canonical" href="https://eanyang7.github.io/transformers_docs/main_classes/deepspeed/">
      
      
        <link rel="prev" href="../../4-%E6%A6%82%E5%BF%B5%E6%8C%87%E5%8D%97/task_summary/">
      
      
        <link rel="next" href="../model/">
      
      
      <link rel="icon" href="../../assets/favicon.jpg">
      <meta name="generator" content="mkdocs-1.5.3, mkdocs-material-9.4.12">
    
    
      
        <title>Deepspeed - Transformers 文档</title>
      
    
    
      <link rel="stylesheet" href="../../assets/stylesheets/main.fad675c6.min.css">
      
        
        <link rel="stylesheet" href="../../assets/stylesheets/palette.356b1318.min.css">
      
      


    
    
      
    
    
      
        
        
        <link rel="preconnect" href="https://fonts.gstatic.com" crossorigin>
        <link rel="stylesheet" href="https://fonts.googleapis.com/css?family=Roboto:300,300i,400,400i,700,700i%7CRoboto+Mono:400,400i,700,700i&display=fallback">
        <style>:root{--md-text-font:"Roboto";--md-code-font:"Roboto Mono"}</style>
      
    
    
    <script>__md_scope=new URL("../..",location),__md_hash=e=>[...e].reduce((e,_)=>(e<<5)-e+_.charCodeAt(0),0),__md_get=(e,_=localStorage,t=__md_scope)=>JSON.parse(_.getItem(t.pathname+"."+e)),__md_set=(e,_,t=localStorage,a=__md_scope)=>{try{t.setItem(a.pathname+"."+e,JSON.stringify(_))}catch(e){}}</script>
    
      

    
    
    
  </head>
  
  
    
    
      
    
    
    
    
    <body dir="ltr" data-md-color-scheme="default" data-md-color-primary="red" data-md-color-accent="deep-purple">
  
    
    
      <script>var palette=__md_get("__palette");if(palette&&"object"==typeof palette.color)for(var key of Object.keys(palette.color))document.body.setAttribute("data-md-color-"+key,palette.color[key])</script>
    
    <input class="md-toggle" data-md-toggle="drawer" type="checkbox" id="__drawer" autocomplete="off">
    <input class="md-toggle" data-md-toggle="search" type="checkbox" id="__search" autocomplete="off">
    <label class="md-overlay" for="__drawer"></label>
    <div data-md-component="skip">
      
        
        <a href="#deepspeed" class="md-skip">
          跳转至
        </a>
      
    </div>
    <div data-md-component="announce">
      
    </div>
    
    
      

  

<header class="md-header md-header--shadow" data-md-component="header">
  <nav class="md-header__inner md-grid" aria-label="页眉">
    <a href="../.." title="Transformers 文档" class="md-header__button md-logo" aria-label="Transformers 文档" data-md-component="logo">
      
  <img src="../../assets/logo.jpg" alt="logo">

    </a>
    <label class="md-header__button md-icon" for="__drawer">
      
      <svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 24 24"><path d="M3 6h18v2H3V6m0 5h18v2H3v-2m0 5h18v2H3v-2Z"/></svg>
    </label>
    <div class="md-header__title" data-md-component="header-title">
      <div class="md-header__ellipsis">
        <div class="md-header__topic">
          <span class="md-ellipsis">
            Transformers 文档
          </span>
        </div>
        <div class="md-header__topic" data-md-component="header-topic">
          <span class="md-ellipsis">
            
              Deepspeed
            
          </span>
        </div>
      </div>
    </div>
    
      
        <form class="md-header__option" data-md-component="palette">
  
    
    
    
    <input class="md-option" data-md-color-media="(prefers-color-scheme: light)" data-md-color-scheme="default" data-md-color-primary="red" data-md-color-accent="deep-purple"  aria-label="切换为暗黑模式"  type="radio" name="__palette" id="__palette_1">
    
      <label class="md-header__button md-icon" title="切换为暗黑模式" for="__palette_2" hidden>
        <svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 24 24"><path d="M12 7a5 5 0 0 1 5 5 5 5 0 0 1-5 5 5 5 0 0 1-5-5 5 5 0 0 1 5-5m0 2a3 3 0 0 0-3 3 3 3 0 0 0 3 3 3 3 0 0 0 3-3 3 3 0 0 0-3-3m0-7 2.39 3.42C13.65 5.15 12.84 5 12 5c-.84 0-1.65.15-2.39.42L12 2M3.34 7l4.16-.35A7.2 7.2 0 0 0 5.94 8.5c-.44.74-.69 1.5-.83 2.29L3.34 7m.02 10 1.76-3.77a7.131 7.131 0 0 0 2.38 4.14L3.36 17M20.65 7l-1.77 3.79a7.023 7.023 0 0 0-2.38-4.15l4.15.36m-.01 10-4.14.36c.59-.51 1.12-1.14 1.54-1.86.42-.73.69-1.5.83-2.29L20.64 17M12 22l-2.41-3.44c.74.27 1.55.44 2.41.44.82 0 1.63-.17 2.37-.44L12 22Z"/></svg>
      </label>
    
  
    
    
    
    <input class="md-option" data-md-color-media="(prefers-color-scheme: dark)" data-md-color-scheme="slate" data-md-color-primary="deep-purple" data-md-color-accent="red"  aria-label="切换为浅色模式"  type="radio" name="__palette" id="__palette_2">
    
      <label class="md-header__button md-icon" title="切换为浅色模式" for="__palette_1" hidden>
        <svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 24 24"><path d="m17.75 4.09-2.53 1.94.91 3.06-2.63-1.81-2.63 1.81.91-3.06-2.53-1.94L12.44 4l1.06-3 1.06 3 3.19.09m3.5 6.91-1.64 1.25.59 1.98-1.7-1.17-1.7 1.17.59-1.98L15.75 11l2.06-.05L18.5 9l.69 1.95 2.06.05m-2.28 4.95c.83-.08 1.72 1.1 1.19 1.85-.32.45-.66.87-1.08 1.27C15.17 23 8.84 23 4.94 19.07c-3.91-3.9-3.91-10.24 0-14.14.4-.4.82-.76 1.27-1.08.75-.53 1.93.36 1.85 1.19-.27 2.86.69 5.83 2.89 8.02a9.96 9.96 0 0 0 8.02 2.89m-1.64 2.02a12.08 12.08 0 0 1-7.8-3.47c-2.17-2.19-3.33-5-3.49-7.82-2.81 3.14-2.7 7.96.31 10.98 3.02 3.01 7.84 3.12 10.98.31Z"/></svg>
      </label>
    
  
</form>
      
    
    
    
      <label class="md-header__button md-icon" for="__search">
        
        <svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 24 24"><path d="M9.5 3A6.5 6.5 0 0 1 16 9.5c0 1.61-.59 3.09-1.56 4.23l.27.27h.79l5 5-1.5 1.5-5-5v-.79l-.27-.27A6.516 6.516 0 0 1 9.5 16 6.5 6.5 0 0 1 3 9.5 6.5 6.5 0 0 1 9.5 3m0 2C7 5 5 7 5 9.5S7 14 9.5 14 14 12 14 9.5 12 5 9.5 5Z"/></svg>
      </label>
      <div class="md-search" data-md-component="search" role="dialog">
  <label class="md-search__overlay" for="__search"></label>
  <div class="md-search__inner" role="search">
    <form class="md-search__form" name="search">
      <input type="text" class="md-search__input" name="query" aria-label="搜索" placeholder="搜索" autocapitalize="off" autocorrect="off" autocomplete="off" spellcheck="false" data-md-component="search-query" required>
      <label class="md-search__icon md-icon" for="__search">
        
        <svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 24 24"><path d="M9.5 3A6.5 6.5 0 0 1 16 9.5c0 1.61-.59 3.09-1.56 4.23l.27.27h.79l5 5-1.5 1.5-5-5v-.79l-.27-.27A6.516 6.516 0 0 1 9.5 16 6.5 6.5 0 0 1 3 9.5 6.5 6.5 0 0 1 9.5 3m0 2C7 5 5 7 5 9.5S7 14 9.5 14 14 12 14 9.5 12 5 9.5 5Z"/></svg>
        
        <svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 24 24"><path d="M20 11v2H8l5.5 5.5-1.42 1.42L4.16 12l7.92-7.92L13.5 5.5 8 11h12Z"/></svg>
      </label>
      <nav class="md-search__options" aria-label="查找">
        
          <a href="javascript:void(0)" class="md-search__icon md-icon" title="分享" aria-label="分享" data-clipboard data-clipboard-text="" data-md-component="search-share" tabindex="-1">
            
            <svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 24 24"><path d="M18 16.08c-.76 0-1.44.3-1.96.77L8.91 12.7c.05-.23.09-.46.09-.7 0-.24-.04-.47-.09-.7l7.05-4.11c.54.5 1.25.81 2.04.81a3 3 0 0 0 3-3 3 3 0 0 0-3-3 3 3 0 0 0-3 3c0 .24.04.47.09.7L8.04 9.81C7.5 9.31 6.79 9 6 9a3 3 0 0 0-3 3 3 3 0 0 0 3 3c.79 0 1.5-.31 2.04-.81l7.12 4.15c-.05.21-.08.43-.08.66 0 1.61 1.31 2.91 2.92 2.91 1.61 0 2.92-1.3 2.92-2.91A2.92 2.92 0 0 0 18 16.08Z"/></svg>
          </a>
        
        <button type="reset" class="md-search__icon md-icon" title="清空当前内容" aria-label="清空当前内容" tabindex="-1">
          
          <svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 24 24"><path d="M19 6.41 17.59 5 12 10.59 6.41 5 5 6.41 10.59 12 5 17.59 6.41 19 12 13.41 17.59 19 19 17.59 13.41 12 19 6.41Z"/></svg>
        </button>
      </nav>
      
        <div class="md-search__suggest" data-md-component="search-suggest"></div>
      
    </form>
    <div class="md-search__output">
      <div class="md-search__scrollwrap" data-md-scrollfix>
        <div class="md-search-result" data-md-component="search-result">
          <div class="md-search-result__meta">
            正在初始化搜索引擎
          </div>
          <ol class="md-search-result__list" role="presentation"></ol>
        </div>
      </div>
    </div>
  </div>
</div>
    
    
      <div class="md-header__source">
        <a href="https://github.com/EanYang7/transformers_docs" title="前往仓库" class="md-source" data-md-component="source">
  <div class="md-source__icon md-icon">
    
    <svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 480 512"><!--! Font Awesome Free 6.4.2 by @fontawesome - https://fontawesome.com License - https://fontawesome.com/license/free (Icons: CC BY 4.0, Fonts: SIL OFL 1.1, Code: MIT License) Copyright 2023 Fonticons, Inc.--><path d="M186.1 328.7c0 20.9-10.9 55.1-36.7 55.1s-36.7-34.2-36.7-55.1 10.9-55.1 36.7-55.1 36.7 34.2 36.7 55.1zM480 278.2c0 31.9-3.2 65.7-17.5 95-37.9 76.6-142.1 74.8-216.7 74.8-75.8 0-186.2 2.7-225.6-74.8-14.6-29-20.2-63.1-20.2-95 0-41.9 13.9-81.5 41.5-113.6-5.2-15.8-7.7-32.4-7.7-48.8 0-21.5 4.9-32.3 14.6-51.8 45.3 0 74.3 9 108.8 36 29-6.9 58.8-10 88.7-10 27 0 54.2 2.9 80.4 9.2 34-26.7 63-35.2 107.8-35.2 9.8 19.5 14.6 30.3 14.6 51.8 0 16.4-2.6 32.7-7.7 48.2 27.5 32.4 39 72.3 39 114.2zm-64.3 50.5c0-43.9-26.7-82.6-73.5-82.6-18.9 0-37 3.4-56 6-14.9 2.3-29.8 3.2-45.1 3.2-15.2 0-30.1-.9-45.1-3.2-18.7-2.6-37-6-56-6-46.8 0-73.5 38.7-73.5 82.6 0 87.8 80.4 101.3 150.4 101.3h48.2c70.3 0 150.6-13.4 150.6-101.3zm-82.6-55.1c-25.8 0-36.7 34.2-36.7 55.1s10.9 55.1 36.7 55.1 36.7-34.2 36.7-55.1-10.9-55.1-36.7-55.1z"/></svg>
  </div>
  <div class="md-source__repository">
    github仓库
  </div>
</a>
      </div>
    
  </nav>
  
</header>
    
    <div class="md-container" data-md-component="container">
      
      
        
          
        
      
      <main class="md-main" data-md-component="main">
        <div class="md-main__inner md-grid">
          
            
              
              <div class="md-sidebar md-sidebar--primary" data-md-component="sidebar" data-md-type="navigation" >
                <div class="md-sidebar__scrollwrap">
                  <div class="md-sidebar__inner">
                    



<nav class="md-nav md-nav--primary" aria-label="导航栏" data-md-level="0">
  <label class="md-nav__title" for="__drawer">
    <a href="../.." title="Transformers 文档" class="md-nav__button md-logo" aria-label="Transformers 文档" data-md-component="logo">
      
  <img src="../../assets/logo.jpg" alt="logo">

    </a>
    Transformers 文档
  </label>
  
    <div class="md-nav__source">
      <a href="https://github.com/EanYang7/transformers_docs" title="前往仓库" class="md-source" data-md-component="source">
  <div class="md-source__icon md-icon">
    
    <svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 480 512"><!--! Font Awesome Free 6.4.2 by @fontawesome - https://fontawesome.com License - https://fontawesome.com/license/free (Icons: CC BY 4.0, Fonts: SIL OFL 1.1, Code: MIT License) Copyright 2023 Fonticons, Inc.--><path d="M186.1 328.7c0 20.9-10.9 55.1-36.7 55.1s-36.7-34.2-36.7-55.1 10.9-55.1 36.7-55.1 36.7 34.2 36.7 55.1zM480 278.2c0 31.9-3.2 65.7-17.5 95-37.9 76.6-142.1 74.8-216.7 74.8-75.8 0-186.2 2.7-225.6-74.8-14.6-29-20.2-63.1-20.2-95 0-41.9 13.9-81.5 41.5-113.6-5.2-15.8-7.7-32.4-7.7-48.8 0-21.5 4.9-32.3 14.6-51.8 45.3 0 74.3 9 108.8 36 29-6.9 58.8-10 88.7-10 27 0 54.2 2.9 80.4 9.2 34-26.7 63-35.2 107.8-35.2 9.8 19.5 14.6 30.3 14.6 51.8 0 16.4-2.6 32.7-7.7 48.2 27.5 32.4 39 72.3 39 114.2zm-64.3 50.5c0-43.9-26.7-82.6-73.5-82.6-18.9 0-37 3.4-56 6-14.9 2.3-29.8 3.2-45.1 3.2-15.2 0-30.1-.9-45.1-3.2-18.7-2.6-37-6-56-6-46.8 0-73.5 38.7-73.5 82.6 0 87.8 80.4 101.3 150.4 101.3h48.2c70.3 0 150.6-13.4 150.6-101.3zm-82.6-55.1c-25.8 0-36.7 34.2-36.7 55.1s10.9 55.1 36.7 55.1 36.7-34.2 36.7-55.1-10.9-55.1-36.7-55.1z"/></svg>
  </div>
  <div class="md-source__repository">
    github仓库
  </div>
</a>
    </div>
  
  <ul class="md-nav__list" data-md-scrollfix>
    
      
      
  
  
  
    <li class="md-nav__item">
      <a href="../.." class="md-nav__link">
        
  
  <span class="md-ellipsis">
    Home
  </span>
  

      </a>
    </li>
  

    
      
      
  
  
  
    <li class="md-nav__item">
      <a href="../../autoclass_tutorial/" class="md-nav__link">
        
  
  <span class="md-ellipsis">
    Autoclass tutorial
  </span>
  

      </a>
    </li>
  

    
      
      
  
  
  
    <li class="md-nav__item">
      <a href="../../awesome-transformers/" class="md-nav__link">
        
  
  <span class="md-ellipsis">
    Awesome projects built with Transformers
  </span>
  

      </a>
    </li>
  

    
      
      
  
  
  
    <li class="md-nav__item">
      <a href="../../big_models/" class="md-nav__link">
        
  
  <span class="md-ellipsis">
    Big models
  </span>
  

      </a>
    </li>
  

    
      
      
  
  
  
    <li class="md-nav__item">
      <a href="../../debugging/" class="md-nav__link">
        
  
  <span class="md-ellipsis">
    Debugging
  </span>
  

      </a>
    </li>
  

    
      
      
  
  
  
    <li class="md-nav__item">
      <a href="../../hpo_train/" class="md-nav__link">
        
  
  <span class="md-ellipsis">
    Hpo train
  </span>
  

      </a>
    </li>
  

    
      
      
  
  
  
    <li class="md-nav__item">
      <a href="../../llm_tutorial/" class="md-nav__link">
        
  
  <span class="md-ellipsis">
    Llm tutorial
  </span>
  

      </a>
    </li>
  

    
      
      
  
  
  
    <li class="md-nav__item">
      <a href="../../model_sharing/" class="md-nav__link">
        
  
  <span class="md-ellipsis">
    Model sharing
  </span>
  

      </a>
    </li>
  

    
      
      
  
  
  
    <li class="md-nav__item">
      <a href="../../peft/" class="md-nav__link">
        
  
  <span class="md-ellipsis">
    Peft
  </span>
  

      </a>
    </li>
  

    
      
      
  
  
  
    <li class="md-nav__item">
      <a href="../../perf_hardware/" class="md-nav__link">
        
  
  <span class="md-ellipsis">
    Perf hardware
  </span>
  

      </a>
    </li>
  

    
      
      
  
  
  
    <li class="md-nav__item">
      <a href="../../perf_torch_compile/" class="md-nav__link">
        
  
  <span class="md-ellipsis">
    Perf torch compile
  </span>
  

      </a>
    </li>
  

    
      
      
  
  
  
    <li class="md-nav__item">
      <a href="../../performance/" class="md-nav__link">
        
  
  <span class="md-ellipsis">
    Performance
  </span>
  

      </a>
    </li>
  

    
      
      
  
  
  
    <li class="md-nav__item">
      <a href="../../run_scripts/" class="md-nav__link">
        
  
  <span class="md-ellipsis">
    Run scripts
  </span>
  

      </a>
    </li>
  

    
      
      
  
  
  
    <li class="md-nav__item">
      <a href="../../tf_xla/" class="md-nav__link">
        
  
  <span class="md-ellipsis">
    Tf xla
  </span>
  

      </a>
    </li>
  

    
      
      
  
  
  
    <li class="md-nav__item">
      <a href="../../tokenizer_summary/" class="md-nav__link">
        
  
  <span class="md-ellipsis">
    Tokenizer summary
  </span>
  

      </a>
    </li>
  

    
      
      
  
  
  
    
    
    
    
    
      
      
    
    <li class="md-nav__item md-nav__item--pruned md-nav__item--nested">
      
        
  
  
    <a href="../../1-%E5%BC%80%E5%A7%8B%E4%BD%BF%E7%94%A8/" class="md-nav__link">
      
  
  <span class="md-ellipsis">
    1 开始使用
  </span>
  

      
        <span class="md-nav__icon md-icon"></span>
      
    </a>
  

      
    </li>
  

    
      
      
  
  
  
    
    
    
    
    
      
      
    
    <li class="md-nav__item md-nav__item--pruned md-nav__item--nested">
      
        
  
  
    <a href="../../2-%E6%95%99%E7%A8%8B/accelerate/" class="md-nav__link">
      
  
  <span class="md-ellipsis">
    2 教程
  </span>
  

      
        <span class="md-nav__icon md-icon"></span>
      
    </a>
  

      
    </li>
  

    
      
      
  
  
  
    
    
    
    
    
      
      
    
    <li class="md-nav__item md-nav__item--pruned md-nav__item--nested">
      
        
  
  
    <a href="../../3-%E5%BC%80%E5%8F%91%E8%80%85%E6%8C%87%E5%8D%97/create_a_model/" class="md-nav__link">
      
  
  <span class="md-ellipsis">
    3 开发者指南
  </span>
  

      
        <span class="md-nav__icon md-icon"></span>
      
    </a>
  

      
    </li>
  

    
      
      
  
  
  
    
    
    
    
    
      
      
    
    <li class="md-nav__item md-nav__item--pruned md-nav__item--nested">
      
        
  
  
    <a href="../../4-%E6%A6%82%E5%BF%B5%E6%8C%87%E5%8D%97/task_summary/" class="md-nav__link">
      
  
  <span class="md-ellipsis">
    4 概念指南
  </span>
  

      
        <span class="md-nav__icon md-icon"></span>
      
    </a>
  

      
    </li>
  

    
      
      
  
  
    
  
  
    
    
    
    
    
    <li class="md-nav__item md-nav__item--active md-nav__item--nested">
      
        
        
        
        <input class="md-nav__toggle md-toggle " type="checkbox" id="__nav_20" checked>
        
          
          <label class="md-nav__link" for="__nav_20" id="__nav_20_label" tabindex="0">
            
  
  <span class="md-ellipsis">
    Main classes
  </span>
  

            <span class="md-nav__icon md-icon"></span>
          </label>
        
        <nav class="md-nav" data-md-level="1" aria-labelledby="__nav_20_label" aria-expanded="true">
          <label class="md-nav__title" for="__nav_20">
            <span class="md-nav__icon md-icon"></span>
            Main classes
          </label>
          <ul class="md-nav__list" data-md-scrollfix>
            
              
                
  
  
    
  
  
    <li class="md-nav__item md-nav__item--active">
      
      <input class="md-nav__toggle md-toggle" type="checkbox" id="__toc">
      
      
        
      
      
        <label class="md-nav__link md-nav__link--active" for="__toc">
          
  
  <span class="md-ellipsis">
    Deepspeed
  </span>
  

          <span class="md-nav__icon md-icon"></span>
        </label>
      
      <a href="./" class="md-nav__link md-nav__link--active">
        
  
  <span class="md-ellipsis">
    Deepspeed
  </span>
  

      </a>
      
        

<nav class="md-nav md-nav--secondary" aria-label="目录">
  
  
  
    
  
  
    <label class="md-nav__title" for="__toc">
      <span class="md-nav__icon md-icon"></span>
      目录
    </label>
    <ul class="md-nav__list" data-md-component="toc" data-md-scrollfix>
      
        <li class="md-nav__item">
  <a href="#trainer-deepspeed" class="md-nav__link">
    <span class="md-ellipsis">
      Trainer DeepSpeed 集成
    </span>
  </a>
  
    <nav class="md-nav" aria-label="Trainer DeepSpeed 集成">
      <ul class="md-nav__list">
        
          <li class="md-nav__item">
  <a href="#_1" class="md-nav__link">
    <span class="md-ellipsis">
      安装
    </span>
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#gpu" class="md-nav__link">
    <span class="md-ellipsis">
      多GPU启用
    </span>
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#gpu_1" class="md-nav__link">
    <span class="md-ellipsis">
      单GPU启用
    </span>
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#_2" class="md-nav__link">
    <span class="md-ellipsis">
      多节点启用
    </span>
  </a>
  
    <nav class="md-nav" aria-label="多节点启用">
      <ul class="md-nav__list">
        
          <li class="md-nav__item">
  <a href="#torchdistributedrun" class="md-nav__link">
    <span class="md-ellipsis">
      torch.distributed.run启动器
    </span>
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#deepspeed_1" class="md-nav__link">
    <span class="md-ellipsis">
      deepspeed启动器
    </span>
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#slurm" class="md-nav__link">
    <span class="md-ellipsis">
      在 SLURM 环境中启动
    </span>
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#_3" class="md-nav__link">
    <span class="md-ellipsis">
      使用非共享文件系统
    </span>
  </a>
  
</li>
        
      </ul>
    </nav>
  
</li>
        
          <li class="md-nav__item">
  <a href="#notebooks" class="md-nav__link">
    <span class="md-ellipsis">
      在Notebooks启用
    </span>
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#_4" class="md-nav__link">
    <span class="md-ellipsis">
      传递配置
    </span>
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#_5" class="md-nav__link">
    <span class="md-ellipsis">
      共享配置
    </span>
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#zero" class="md-nav__link">
    <span class="md-ellipsis">
      ZeRO
    </span>
  </a>
  
    <nav class="md-nav" aria-label="ZeRO">
      <ul class="md-nav__list">
        
          <li class="md-nav__item">
  <a href="#zero-2" class="md-nav__link">
    <span class="md-ellipsis">
      ZeRO-2 配置
    </span>
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#zero-3" class="md-nav__link">
    <span class="md-ellipsis">
      ZeRO-3 配置
    </span>
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#zero-0" class="md-nav__link">
    <span class="md-ellipsis">
      ZeRO-0 配置
    </span>
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#zero-1" class="md-nav__link">
    <span class="md-ellipsis">
      ZeRO-1 配置
    </span>
  </a>
  
</li>
        
      </ul>
    </nav>
  
</li>
        
          <li class="md-nav__item">
  <a href="#nvme" class="md-nav__link">
    <span class="md-ellipsis">
      NVMe 支持
    </span>
  </a>
  
    <nav class="md-nav" aria-label="NVMe 支持">
      <ul class="md-nav__list">
        
          <li class="md-nav__item">
  <a href="#zero-2-zero-3" class="md-nav__link">
    <span class="md-ellipsis">
      ZeRO-2 和 ZeRO-3 性能对比
    </span>
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#zero-2_1" class="md-nav__link">
    <span class="md-ellipsis">
      ZeRO-2 示例
    </span>
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#zero-3_1" class="md-nav__link">
    <span class="md-ellipsis">
      ZeRO-3 示例
    </span>
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#zero-stage-offloads" class="md-nav__link">
    <span class="md-ellipsis">
      如何选择最佳性能的ZeRO Stage和 offloads
    </span>
  </a>
  
</li>
        
      </ul>
    </nav>
  
</li>
        
          <li class="md-nav__item">
  <a href="#activation-checkpointing-gradient-checkpointing" class="md-nav__link">
    <span class="md-ellipsis">
      Activation Checkpointing 或 Gradient Checkpointing
    </span>
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#optimizer-scheduler" class="md-nav__link">
    <span class="md-ellipsis">
      Optimizer 和 Scheduler
    </span>
  </a>
  
    <nav class="md-nav" aria-label="Optimizer 和 Scheduler">
      <ul class="md-nav__list">
        
          <li class="md-nav__item">
  <a href="#optimizer" class="md-nav__link">
    <span class="md-ellipsis">
      Optimizer
    </span>
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#scheduler" class="md-nav__link">
    <span class="md-ellipsis">
      Scheduler
    </span>
  </a>
  
</li>
        
      </ul>
    </nav>
  
</li>
        
          <li class="md-nav__item">
  <a href="#fp32" class="md-nav__link">
    <span class="md-ellipsis">
      fp32精度
    </span>
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#_6" class="md-nav__link">
    <span class="md-ellipsis">
      自动混合精度
    </span>
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#fp16" class="md-nav__link">
    <span class="md-ellipsis">
      fp16
    </span>
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#bf16" class="md-nav__link">
    <span class="md-ellipsis">
      bf16
    </span>
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#nccl" class="md-nav__link">
    <span class="md-ellipsis">
      NCCL集合
    </span>
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#apex" class="md-nav__link">
    <span class="md-ellipsis">
      apex
    </span>
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#batch-size" class="md-nav__link">
    <span class="md-ellipsis">
      Batch Size
    </span>
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#gradient-accumulation" class="md-nav__link">
    <span class="md-ellipsis">
      Gradient Accumulation
    </span>
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#gradient-clipping" class="md-nav__link">
    <span class="md-ellipsis">
      Gradient Clipping
    </span>
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#_7" class="md-nav__link">
    <span class="md-ellipsis">
      获取模型权重
    </span>
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#zero-3-infinity-nuances" class="md-nav__link">
    <span class="md-ellipsis">
      ZeRO-3 和 Infinity Nuances
    </span>
  </a>
  
    <nav class="md-nav" aria-label="ZeRO-3 和 Infinity Nuances">
      <ul class="md-nav__list">
        
          <li class="md-nav__item">
  <a href="#_8" class="md-nav__link">
    <span class="md-ellipsis">
      构建大模型
    </span>
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#_9" class="md-nav__link">
    <span class="md-ellipsis">
      参数收集
    </span>
  </a>
  
</li>
        
      </ul>
    </nav>
  
</li>
        
          <li class="md-nav__item">
  <a href="#zero_1" class="md-nav__link">
    <span class="md-ellipsis">
      ZeRO 推理
    </span>
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#_10" class="md-nav__link">
    <span class="md-ellipsis">
      内存要求
    </span>
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#issues" class="md-nav__link">
    <span class="md-ellipsis">
      归档Issues
    </span>
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#troubleshooting" class="md-nav__link">
    <span class="md-ellipsis">
      Troubleshooting
    </span>
  </a>
  
    <nav class="md-nav" aria-label="Troubleshooting">
      <ul class="md-nav__list">
        
          <li class="md-nav__item">
  <a href="#deepspeed_2" class="md-nav__link">
    <span class="md-ellipsis">
      启动时deepspeed进程被终止，没有回溯
    </span>
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#lossnan" class="md-nav__link">
    <span class="md-ellipsis">
      训练和/或评估/预测loss为NaN
    </span>
  </a>
  
</li>
        
      </ul>
    </nav>
  
</li>
        
          <li class="md-nav__item">
  <a href="#_11" class="md-nav__link">
    <span class="md-ellipsis">
      注意事项
    </span>
  </a>
  
</li>
        
      </ul>
    </nav>
  
</li>
      
        <li class="md-nav__item">
  <a href="#non-trainer-deepspeed" class="md-nav__link">
    <span class="md-ellipsis">
      Non-Trainer Deepspeed集成
    </span>
  </a>
  
</li>
      
        <li class="md-nav__item">
  <a href="#hfdeepspeedconfig" class="md-nav__link">
    <span class="md-ellipsis">
      HfDeepSpeedConfig
    </span>
  </a>
  
    <nav class="md-nav" aria-label="HfDeepSpeedConfig">
      <ul class="md-nav__list">
        
          <li class="md-nav__item">
  <a href="#deepspeed-zero" class="md-nav__link">
    <span class="md-ellipsis">
      自定义DeepSpeed ZeRO推理
    </span>
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#generate" class="md-nav__link">
    <span class="md-ellipsis">
      generate 的差异
    </span>
  </a>
  
</li>
        
      </ul>
    </nav>
  
</li>
      
        <li class="md-nav__item">
  <a href="#deepspeed_3" class="md-nav__link">
    <span class="md-ellipsis">
      测试 DeepSpeed 集成
    </span>
  </a>
  
</li>
      
        <li class="md-nav__item">
  <a href="#deepspeed_4" class="md-nav__link">
    <span class="md-ellipsis">
      主要的DeepSpeed资源
    </span>
  </a>
  
</li>
      
    </ul>
  
</nav>
      
    </li>
  

              
            
              
                
  
  
  
    <li class="md-nav__item">
      <a href="../model/" class="md-nav__link">
        
  
  <span class="md-ellipsis">
    Model
  </span>
  

      </a>
    </li>
  

              
            
              
                
  
  
  
    <li class="md-nav__item">
      <a href="../trainer/" class="md-nav__link">
        
  
  <span class="md-ellipsis">
    Trainer
  </span>
  

      </a>
    </li>
  

              
            
          </ul>
        </nav>
      
    </li>
  

    
  </ul>
</nav>
                  </div>
                </div>
              </div>
            
            
              
              <div class="md-sidebar md-sidebar--secondary" data-md-component="sidebar" data-md-type="toc" >
                <div class="md-sidebar__scrollwrap">
                  <div class="md-sidebar__inner">
                    

<nav class="md-nav md-nav--secondary" aria-label="目录">
  
  
  
    
  
  
    <label class="md-nav__title" for="__toc">
      <span class="md-nav__icon md-icon"></span>
      目录
    </label>
    <ul class="md-nav__list" data-md-component="toc" data-md-scrollfix>
      
        <li class="md-nav__item">
  <a href="#trainer-deepspeed" class="md-nav__link">
    <span class="md-ellipsis">
      Trainer DeepSpeed 集成
    </span>
  </a>
  
    <nav class="md-nav" aria-label="Trainer DeepSpeed 集成">
      <ul class="md-nav__list">
        
          <li class="md-nav__item">
  <a href="#_1" class="md-nav__link">
    <span class="md-ellipsis">
      安装
    </span>
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#gpu" class="md-nav__link">
    <span class="md-ellipsis">
      多GPU启用
    </span>
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#gpu_1" class="md-nav__link">
    <span class="md-ellipsis">
      单GPU启用
    </span>
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#_2" class="md-nav__link">
    <span class="md-ellipsis">
      多节点启用
    </span>
  </a>
  
    <nav class="md-nav" aria-label="多节点启用">
      <ul class="md-nav__list">
        
          <li class="md-nav__item">
  <a href="#torchdistributedrun" class="md-nav__link">
    <span class="md-ellipsis">
      torch.distributed.run启动器
    </span>
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#deepspeed_1" class="md-nav__link">
    <span class="md-ellipsis">
      deepspeed启动器
    </span>
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#slurm" class="md-nav__link">
    <span class="md-ellipsis">
      在 SLURM 环境中启动
    </span>
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#_3" class="md-nav__link">
    <span class="md-ellipsis">
      使用非共享文件系统
    </span>
  </a>
  
</li>
        
      </ul>
    </nav>
  
</li>
        
          <li class="md-nav__item">
  <a href="#notebooks" class="md-nav__link">
    <span class="md-ellipsis">
      在Notebooks启用
    </span>
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#_4" class="md-nav__link">
    <span class="md-ellipsis">
      传递配置
    </span>
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#_5" class="md-nav__link">
    <span class="md-ellipsis">
      共享配置
    </span>
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#zero" class="md-nav__link">
    <span class="md-ellipsis">
      ZeRO
    </span>
  </a>
  
    <nav class="md-nav" aria-label="ZeRO">
      <ul class="md-nav__list">
        
          <li class="md-nav__item">
  <a href="#zero-2" class="md-nav__link">
    <span class="md-ellipsis">
      ZeRO-2 配置
    </span>
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#zero-3" class="md-nav__link">
    <span class="md-ellipsis">
      ZeRO-3 配置
    </span>
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#zero-0" class="md-nav__link">
    <span class="md-ellipsis">
      ZeRO-0 配置
    </span>
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#zero-1" class="md-nav__link">
    <span class="md-ellipsis">
      ZeRO-1 配置
    </span>
  </a>
  
</li>
        
      </ul>
    </nav>
  
</li>
        
          <li class="md-nav__item">
  <a href="#nvme" class="md-nav__link">
    <span class="md-ellipsis">
      NVMe 支持
    </span>
  </a>
  
    <nav class="md-nav" aria-label="NVMe 支持">
      <ul class="md-nav__list">
        
          <li class="md-nav__item">
  <a href="#zero-2-zero-3" class="md-nav__link">
    <span class="md-ellipsis">
      ZeRO-2 和 ZeRO-3 性能对比
    </span>
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#zero-2_1" class="md-nav__link">
    <span class="md-ellipsis">
      ZeRO-2 示例
    </span>
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#zero-3_1" class="md-nav__link">
    <span class="md-ellipsis">
      ZeRO-3 示例
    </span>
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#zero-stage-offloads" class="md-nav__link">
    <span class="md-ellipsis">
      如何选择最佳性能的ZeRO Stage和 offloads
    </span>
  </a>
  
</li>
        
      </ul>
    </nav>
  
</li>
        
          <li class="md-nav__item">
  <a href="#activation-checkpointing-gradient-checkpointing" class="md-nav__link">
    <span class="md-ellipsis">
      Activation Checkpointing 或 Gradient Checkpointing
    </span>
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#optimizer-scheduler" class="md-nav__link">
    <span class="md-ellipsis">
      Optimizer 和 Scheduler
    </span>
  </a>
  
    <nav class="md-nav" aria-label="Optimizer 和 Scheduler">
      <ul class="md-nav__list">
        
          <li class="md-nav__item">
  <a href="#optimizer" class="md-nav__link">
    <span class="md-ellipsis">
      Optimizer
    </span>
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#scheduler" class="md-nav__link">
    <span class="md-ellipsis">
      Scheduler
    </span>
  </a>
  
</li>
        
      </ul>
    </nav>
  
</li>
        
          <li class="md-nav__item">
  <a href="#fp32" class="md-nav__link">
    <span class="md-ellipsis">
      fp32精度
    </span>
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#_6" class="md-nav__link">
    <span class="md-ellipsis">
      自动混合精度
    </span>
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#fp16" class="md-nav__link">
    <span class="md-ellipsis">
      fp16
    </span>
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#bf16" class="md-nav__link">
    <span class="md-ellipsis">
      bf16
    </span>
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#nccl" class="md-nav__link">
    <span class="md-ellipsis">
      NCCL集合
    </span>
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#apex" class="md-nav__link">
    <span class="md-ellipsis">
      apex
    </span>
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#batch-size" class="md-nav__link">
    <span class="md-ellipsis">
      Batch Size
    </span>
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#gradient-accumulation" class="md-nav__link">
    <span class="md-ellipsis">
      Gradient Accumulation
    </span>
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#gradient-clipping" class="md-nav__link">
    <span class="md-ellipsis">
      Gradient Clipping
    </span>
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#_7" class="md-nav__link">
    <span class="md-ellipsis">
      获取模型权重
    </span>
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#zero-3-infinity-nuances" class="md-nav__link">
    <span class="md-ellipsis">
      ZeRO-3 和 Infinity Nuances
    </span>
  </a>
  
    <nav class="md-nav" aria-label="ZeRO-3 和 Infinity Nuances">
      <ul class="md-nav__list">
        
          <li class="md-nav__item">
  <a href="#_8" class="md-nav__link">
    <span class="md-ellipsis">
      构建大模型
    </span>
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#_9" class="md-nav__link">
    <span class="md-ellipsis">
      参数收集
    </span>
  </a>
  
</li>
        
      </ul>
    </nav>
  
</li>
        
          <li class="md-nav__item">
  <a href="#zero_1" class="md-nav__link">
    <span class="md-ellipsis">
      ZeRO 推理
    </span>
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#_10" class="md-nav__link">
    <span class="md-ellipsis">
      内存要求
    </span>
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#issues" class="md-nav__link">
    <span class="md-ellipsis">
      归档Issues
    </span>
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#troubleshooting" class="md-nav__link">
    <span class="md-ellipsis">
      Troubleshooting
    </span>
  </a>
  
    <nav class="md-nav" aria-label="Troubleshooting">
      <ul class="md-nav__list">
        
          <li class="md-nav__item">
  <a href="#deepspeed_2" class="md-nav__link">
    <span class="md-ellipsis">
      启动时deepspeed进程被终止，没有回溯
    </span>
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#lossnan" class="md-nav__link">
    <span class="md-ellipsis">
      训练和/或评估/预测loss为NaN
    </span>
  </a>
  
</li>
        
      </ul>
    </nav>
  
</li>
        
          <li class="md-nav__item">
  <a href="#_11" class="md-nav__link">
    <span class="md-ellipsis">
      注意事项
    </span>
  </a>
  
</li>
        
      </ul>
    </nav>
  
</li>
      
        <li class="md-nav__item">
  <a href="#non-trainer-deepspeed" class="md-nav__link">
    <span class="md-ellipsis">
      Non-Trainer Deepspeed集成
    </span>
  </a>
  
</li>
      
        <li class="md-nav__item">
  <a href="#hfdeepspeedconfig" class="md-nav__link">
    <span class="md-ellipsis">
      HfDeepSpeedConfig
    </span>
  </a>
  
    <nav class="md-nav" aria-label="HfDeepSpeedConfig">
      <ul class="md-nav__list">
        
          <li class="md-nav__item">
  <a href="#deepspeed-zero" class="md-nav__link">
    <span class="md-ellipsis">
      自定义DeepSpeed ZeRO推理
    </span>
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#generate" class="md-nav__link">
    <span class="md-ellipsis">
      generate 的差异
    </span>
  </a>
  
</li>
        
      </ul>
    </nav>
  
</li>
      
        <li class="md-nav__item">
  <a href="#deepspeed_3" class="md-nav__link">
    <span class="md-ellipsis">
      测试 DeepSpeed 集成
    </span>
  </a>
  
</li>
      
        <li class="md-nav__item">
  <a href="#deepspeed_4" class="md-nav__link">
    <span class="md-ellipsis">
      主要的DeepSpeed资源
    </span>
  </a>
  
</li>
      
    </ul>
  
</nav>
                  </div>
                </div>
              </div>
            
          
          
            <div class="md-content" data-md-component="content">
              <article class="md-content__inner md-typeset">
                
                  

  
    <a href="https://github.com/EanYang7/transformers_docs/tree/main/docs/main_classes/deepspeed.md" title="编辑此页" class="md-content__button md-icon">
      
      <svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 24 24"><path d="M20.71 7.04c.39-.39.39-1.04 0-1.41l-2.34-2.34c-.37-.39-1.02-.39-1.41 0l-1.84 1.83 3.75 3.75M3 17.25V21h3.75L17.81 9.93l-3.75-3.75L3 17.25Z"/></svg>
    </a>
  
  
    
      
    
    <a href="https://github.com/EanYang7/transformers_docs/tree/main/docs/main_classes/deepspeed.md" title="查看本页的源代码" class="md-content__button md-icon">
      
      <svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 24 24"><path d="M12 9a3 3 0 0 0-3 3 3 3 0 0 0 3 3 3 3 0 0 0 3-3 3 3 0 0 0-3-3m0 8a5 5 0 0 1-5-5 5 5 0 0 1 5-5 5 5 0 0 1 5 5 5 5 0 0 1-5 5m0-12.5C7 4.5 2.73 7.61 1 12c1.73 4.39 6 7.5 11 7.5s9.27-3.11 11-7.5c-1.73-4.39-6-7.5-11-7.5Z"/></svg>
    </a>
  


<!--Copyright 2020 The HuggingFace Team. All rights reserved.

Licensed under the Apache License, Version 2.0 (the "License"); you may not use this file except in compliance with
the License. You may obtain a copy of the License at

http://www.apache.org/licenses/LICENSE-2.0

Unless required by applicable law or agreed to in writing, software distributed under the License is distributed on
an "AS IS" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied. See the License for the
specific language governing permissions and limitations under the License.

⚠️ Note that this file is in Markdown but contain specific syntax for our doc-builder (similar to MDX) that may not be
rendered properly in your Markdown viewer.

-->

<h1 id="deepspeed">DeepSpeed集成<a class="headerlink" href="#deepspeed" title="Permanent link">⚓︎</a></h1>
<p><a href="https://github.com/microsoft/DeepSpeed">DeepSpeed</a>实现了<a href="https://arxiv.org/abs/1910.02054">ZeRO论文</a>中描述的所有内容。目前，它提供对以下功能的全面支持：</p>
<ol>
<li>优化器状态分区（ZeRO stage 1）</li>
<li>梯度分区（ZeRO stage 2）</li>
<li>参数分区（ZeRO stage 3）</li>
<li>自定义混合精度训练处理</li>
<li>一系列基于CUDA扩展的快速优化器</li>
<li>ZeRO-Offload 到 CPU 和 NVMe</li>
</ol>
<p>ZeRO-Offload有其自己的专门论文：<a href="https://arxiv.org/abs/2101.06840">ZeRO-Offload: Democratizing Billion-Scale Model Training</a>。而NVMe支持在论文<a href="https://arxiv.org/abs/2104.07857">ZeRO-Infinity: Breaking the GPU Memory Wall for Extreme Scale Deep Learning</a>中进行了描述。</p>
<p>DeepSpeed ZeRO-2主要用于训练，因为它的特性对推理没有用处。</p>
<p>DeepSpeed ZeRO-3也可以用于推理，因为它允许将单个GPU无法加载的大模型加载到多个GPU上。</p>
<p>🤗 Transformers通过以下两种方式集成了<a href="https://github.com/microsoft/DeepSpeed">DeepSpeed</a>：</p>
<ol>
<li>通过[<code>Trainer</code>]集成核心的DeepSpeed功能。这是一种“为您完成一切”式的集成 - 您只需提供自定义配置文件或使用我们的模板配置文件。本文档的大部分内容都集中在这个功能上。</li>
<li>如果您不使用[<code>Trainer</code>]并希望在自己的Trainer中集成DeepSpeed，那么像<code>from_pretrained</code>和<code>from_config</code>这样的核心功能函数将包括ZeRO stage 3及以上的DeepSpeed的基础部分，如<code>zero.Init</code>。要利用此功能，请阅读有关<a href="#nontrainer-deepspeed-integration">非Trainer DeepSpeed集成</a>的文档。</li>
</ol>
<p>集成的内容：</p>
<p>训练：</p>
<ol>
<li>DeepSpeed ZeRO训练支持完整的ZeRO stages 1、2和3，以及ZeRO-Infinity（CPU和NVMe offload）。</li>
</ol>
<p>推理：</p>
<ol>
<li>DeepSpeed ZeRO推理支持ZeRO stage 3和ZeRO-Infinity。它使用与训练相同的ZeRO协议，但不使用优化器和学习率调度器，只有stage 3与推理相关。更多详细信息请参阅：<a href="#zero-inference">zero-inference</a>。</li>
</ol>
<p>此外还有DeepSpeed推理 - 这是一种完全不同的技术，它使用张量并行而不是ZeRO（即将推出）。</p>
<p><a id='deepspeed-trainer-integration'></a></p>
<h2 id="trainer-deepspeed">Trainer DeepSpeed 集成<a class="headerlink" href="#trainer-deepspeed" title="Permanent link">⚓︎</a></h2>
<p><a id='deepspeed-installation'></a></p>
<h3 id="_1">安装<a class="headerlink" href="#_1" title="Permanent link">⚓︎</a></h3>
<p>通过pypi安装库：</p>
<div class="highlight"><pre><span></span><code>pip<span class="w"> </span>install<span class="w"> </span>deepspeed
</code></pre></div>
<p>或通过 <code>transformers</code> 的 <code>extras</code>安装：</p>
<div class="highlight"><pre><span></span><code>pip<span class="w"> </span>install<span class="w"> </span>transformers<span class="o">[</span>deepspeed<span class="o">]</span>
</code></pre></div>
<p>或在 <a href="https://github.com/microsoft/deepspeed#installation">DeepSpeed 的 GitHub 页面</a> 和
<a href="https://www.deepspeed.ai/tutorials/advanced-install/">高级安装</a> 中查找更多详细信息。</p>
<p>如果构建过程中仍然遇到问题，请首先确保阅读 <a href="trainer#cuda-extension-installation-notes">CUDA 扩展安装注意事项</a>。</p>
<p>如果您没有预先构建扩展而是在运行时构建它们，而且您尝试了以上所有解决方案都无效，下一步可以尝试在安装之前预先构建扩展。</p>
<p>进行 DeepSpeed 的本地构建：</p>
<div class="highlight"><pre><span></span><code>git<span class="w"> </span>clone<span class="w"> </span>https://github.com/microsoft/DeepSpeed/
<span class="nb">cd</span><span class="w"> </span>DeepSpeed
rm<span class="w"> </span>-rf<span class="w"> </span>build
<span class="nv">TORCH_CUDA_ARCH_LIST</span><span class="o">=</span><span class="s2">&quot;8.6&quot;</span><span class="w"> </span><span class="nv">DS_BUILD_CPU_ADAM</span><span class="o">=</span><span class="m">1</span><span class="w"> </span><span class="nv">DS_BUILD_UTILS</span><span class="o">=</span><span class="m">1</span><span class="w"> </span>pip<span class="w"> </span>install<span class="w"> </span>.<span class="w"> </span><span class="se">\</span>
--global-option<span class="o">=</span><span class="s2">&quot;build_ext&quot;</span><span class="w"> </span>--global-option<span class="o">=</span><span class="s2">&quot;-j8&quot;</span><span class="w"> </span>--no-cache<span class="w"> </span>-v<span class="w"> </span><span class="se">\</span>
--disable-pip-version-check<span class="w"> </span><span class="m">2</span>&gt;<span class="p">&amp;</span><span class="m">1</span><span class="w"> </span><span class="p">|</span><span class="w"> </span>tee<span class="w"> </span>build.log
</code></pre></div>
<p>如果您打算使用 NVMe offload，您还需要在上述说明中添加 <code>DS_BUILD_AIO=1</code>（并且还需要在系统范围内安装 <em>libaio-dev</em>）。</p>
<p>编辑 <code>TORCH_CUDA_ARCH_LIST</code> 以插入您打算使用的 GPU 卡的架构代码。假设您的所有卡都是相同的，您可以通过以下方式获取架构：</p>
<div class="highlight"><pre><span></span><code><span class="nv">CUDA_VISIBLE_DEVICES</span><span class="o">=</span><span class="m">0</span><span class="w"> </span>python<span class="w"> </span>-c<span class="w"> </span><span class="s2">&quot;import torch; print(torch.cuda.get_device_capability())&quot;</span>
</code></pre></div>
<p>因此，如果您得到 <code>8, 6</code>，则使用 <code>TORCH_CUDA_ARCH_LIST="8.6"</code>。如果您有多个不同的卡，您可以像这样列出所有卡 <code>TORCH_CUDA_ARCH_LIST="6.1;8.6"</code>。</p>
<p>如果您需要在多台机器上使用相同的设置，请创建一个二进制 wheel：</p>
<div class="highlight"><pre><span></span><code>git<span class="w"> </span>clone<span class="w"> </span>https://github.com/microsoft/DeepSpeed/
<span class="nb">cd</span><span class="w"> </span>DeepSpeed
rm<span class="w"> </span>-rf<span class="w"> </span>build
<span class="nv">TORCH_CUDA_ARCH_LIST</span><span class="o">=</span><span class="s2">&quot;8.6&quot;</span><span class="w"> </span><span class="nv">DS_BUILD_CPU_ADAM</span><span class="o">=</span><span class="m">1</span><span class="w"> </span><span class="nv">DS_BUILD_UTILS</span><span class="o">=</span><span class="m">1</span><span class="w"> </span><span class="se">\</span>
python<span class="w"> </span>setup.py<span class="w"> </span>build_ext<span class="w"> </span>-j8<span class="w"> </span>bdist_wheel
</code></pre></div>
<p>它将生成类似于 <code>dist/deepspeed-0.3.13+8cd046f-cp38-cp38-linux_x86_64.whl</code> 的文件，现在您可以在本地或任何其他机器上安装它，如 <code>pip install deepspeed-0.3.13+8cd046f-cp38-cp38-linux_x86_64.whl</code>。</p>
<p>再次提醒确保调整 <code>TORCH_CUDA_ARCH_LIST</code> 以匹配目标架构。</p>
<p>您可以在<a href="https://developer.nvidia.com/cuda-gpus">这里</a>找到完整的 NVIDIA GPU 列表及其对应的 <strong>计算能力</strong>（与此上下文中的架构相同）。</p>
<p>您可以使用以下命令检查 PyTorch 构建时使用的架构：</p>
<div class="highlight"><pre><span></span><code>python<span class="w"> </span>-c<span class="w"> </span><span class="s2">&quot;import torch; print(torch.cuda.get_arch_list())&quot;</span>
</code></pre></div>
<p>以下是如何查找已安装 GPU 中的一张卡的架构。例如，对于 GPU 0：</p>
<div class="highlight"><pre><span></span><code><span class="nv">CUDA_VISIBLE_DEVICES</span><span class="o">=</span><span class="m">0</span><span class="w"> </span>python<span class="w"> </span>-c<span class="w"> </span><span class="s2">&quot;import torch; \</span>
<span class="s2">print(torch.cuda.get_device_properties(torch.device(&#39;cuda&#39;)))&quot;</span>
</code></pre></div>
<p>如果输出结果如下：</p>
<div class="highlight"><pre><span></span><code>_CudaDeviceProperties<span class="o">(</span><span class="nv">name</span><span class="o">=</span><span class="s1">&#39;GeForce RTX 3090&#39;</span>,<span class="w"> </span><span class="nv">major</span><span class="o">=</span><span class="m">8</span>,<span class="w"> </span><span class="nv">minor</span><span class="o">=</span><span class="m">6</span>,<span class="w"> </span><span class="nv">total_memory</span><span class="o">=</span>24268MB,<span class="w"> </span><span class="nv">multi_processor_count</span><span class="o">=</span><span class="m">82</span><span class="o">)</span>
</code></pre></div>
<p>然后您就知道这张卡的架构是 <code>8.6</code>。</p>
<p>您也可以完全省略 <code>TORCH_CUDA_ARCH_LIST</code>，然后构建程序将自动查询构建所在的 GPU 的架构。这可能与目标机器上的 GPU 不匹配，因此最好明确指定所需的架构。</p>
<p>如果尝试了所有建议的方法仍然遇到构建问题，请继续在 <a href="https://github.com/microsoft/DeepSpeed/issues">Deepspeed</a>的 GitHub Issue 上提交问题。</p>
<p><a id='deepspeed-multi-gpu'></a></p>
<h3 id="gpu">多GPU启用<a class="headerlink" href="#gpu" title="Permanent link">⚓︎</a></h3>
<p>为了启用DeepSpeed 集成，调整 [<code>Trainer</code>] 的命令行参数，添加一个新的参数 <code>--deepspeed ds_config.json</code>，其中 <code>ds_config.json</code> 是 DeepSpeed 配置文件，如文档 <a href="https://www.deepspeed.ai/docs/config-json/">这里</a> 所述。文件命名由您决定。
建议使用 DeepSpeed 的 <code>add_config_arguments</code> 程序将必要的命令行参数添加到您的代码中。
有关更多信息，请参阅 <a href="https://deepspeed.readthedocs.io/en/latest/initialize.html#argument-parsing">DeepSpeed 的参数解析</a> 文档。</p>
<p>在这里，您可以使用您喜欢的启动器。您可以继续使用 PyTorch 启动器：</p>
<div class="highlight"><pre><span></span><code>torch.distributed.run<span class="w"> </span>--nproc_per_node<span class="o">=</span><span class="m">2</span><span class="w"> </span>your_program.py<span class="w"> </span>&lt;normal<span class="w"> </span>cl<span class="w"> </span>args&gt;<span class="w"> </span>--deepspeed<span class="w"> </span>ds_config.json
</code></pre></div>
<p>或使用由 <code>deepspeed</code> 提供的启动器：</p>
<div class="highlight"><pre><span></span><code>deepspeed<span class="w"> </span>--num_gpus<span class="o">=</span><span class="m">2</span><span class="w"> </span>your_program.py<span class="w"> </span>&lt;normal<span class="w"> </span>cl<span class="w"> </span>args&gt;<span class="w"> </span>--deepspeed<span class="w"> </span>ds_config.json
</code></pre></div>
<p>正如您所见，这两个启动器的参数不同，但对于大多数需求，任何一个都可以满足工作需求。有关如何配置各个节点和 GPU 的完整详细信息，请查看 <a href="https://www.deepspeed.ai/getting-started/#resource-configuration-multi-node">此处</a>。</p>
<p>当您使用 <code>deepspeed</code> 启动器并且希望使用所有可用的 GPU 时，您可以简单地省略 <code>--num_gpus</code> 标志。</p>
<p>以下是在 DeepSpeed 中启用使用所有可用 GPU情况下， 运行 <code>run_translation.py</code> 的示例：</p>
<div class="highlight"><pre><span></span><code>deepspeed<span class="w"> </span>examples/pytorch/translation/run_translation.py<span class="w"> </span><span class="se">\</span>
--deepspeed<span class="w"> </span>tests/deepspeed/ds_config_zero3.json<span class="w"> </span><span class="se">\</span>
--model_name_or_path<span class="w"> </span>t5-small<span class="w"> </span>--per_device_train_batch_size<span class="w"> </span><span class="m">1</span><span class="w"> </span><span class="se">\</span>
--output_dir<span class="w"> </span>output_dir<span class="w"> </span>--overwrite_output_dir<span class="w"> </span>--fp16<span class="w"> </span><span class="se">\</span>
--do_train<span class="w"> </span>--max_train_samples<span class="w"> </span><span class="m">500</span><span class="w"> </span>--num_train_epochs<span class="w"> </span><span class="m">1</span><span class="w"> </span><span class="se">\</span>
--dataset_name<span class="w"> </span>wmt16<span class="w"> </span>--dataset_config<span class="w"> </span><span class="s2">&quot;ro-en&quot;</span><span class="w"> </span><span class="se">\</span>
--source_lang<span class="w"> </span>en<span class="w"> </span>--target_lang<span class="w"> </span>ro
</code></pre></div>
<p>请注意，在 DeepSpeed 文档中，您可能会看到 <code>--deepspeed --deepspeed_config ds_config.json</code> - 即两个与 DeepSpeed 相关的参数，但为简单起见，并且因为已经有很多参数要处理，我们将两者合并为一个单一参数。</p>
<p>有关一些实际使用示例，请参阅 <a href="https://github.com/huggingface/transformers/issues/8771#issuecomment-759248400">此帖</a>。</p>
<p><a id='deepspeed-one-gpu'></a></p>
<h3 id="gpu_1">单GPU启用<a class="headerlink" href="#gpu_1" title="Permanent link">⚓︎</a></h3>
<p>要使用一张 GPU 启用 DeepSpeed，调整 [<code>Trainer</code>] 的命令行参数如下：</p>
<div class="highlight"><pre><span></span><code>deepspeed<span class="w"> </span>--num_gpus<span class="o">=</span><span class="m">1</span><span class="w"> </span>examples/pytorch/translation/run_translation.py<span class="w"> </span><span class="se">\</span>
--deepspeed<span class="w"> </span>tests/deepspeed/ds_config_zero2.json<span class="w"> </span><span class="se">\</span>
--model_name_or_path<span class="w"> </span>t5-small<span class="w"> </span>--per_device_train_batch_size<span class="w"> </span><span class="m">1</span><span class="w"> </span><span class="se">\</span>
--output_dir<span class="w"> </span>output_dir<span class="w"> </span>--overwrite_output_dir<span class="w"> </span>--fp16<span class="w"> </span><span class="se">\</span>
--do_train<span class="w"> </span>--max_train_samples<span class="w"> </span><span class="m">500</span><span class="w"> </span>--num_train_epochs<span class="w"> </span><span class="m">1</span><span class="w"> </span><span class="se">\</span>
--dataset_name<span class="w"> </span>wmt16<span class="w"> </span>--dataset_config<span class="w"> </span><span class="s2">&quot;ro-en&quot;</span><span class="w"> </span><span class="se">\</span>
--source_lang<span class="w"> </span>en<span class="w"> </span>--target_lang<span class="w"> </span>ro
</code></pre></div>
<p>这与多 GPU 的情况几乎相同，但在这里我们通过 <code>--num_gpus=1</code> 明确告诉 DeepSpeed 仅使用一张 GPU。默认情况下，DeepSpeed 启用给定节点上可以看到的所有 GPU。如果您一开始只有一张 GPU，那么您不需要这个参数。以下 <a href="https://www.deepspeed.ai/getting-started/#resource-configuration-multi-node">文档</a> 讨论了启动器的选项。</p>
<p>为什么要在仅使用一张 GPU 的情况下使用 DeepSpeed 呢？</p>
<ol>
<li>它具有 ZeRO-offload 功能，可以将一些计算和内存委托给主机的 CPU 和 内存，从而为模型的需求保留更多 GPU 资源 - 例如更大的批处理大小，或启用正常情况下无法容纳的非常大模型。</li>
<li>它提供了智能的 GPU 内存管理系统，最小化内存碎片，这再次允许您容纳更大的模型和数据批次。</li>
</ol>
<p>虽然接下来我们将详细讨论配置，但在单个 GPU 上通过 DeepSpeed 实现巨大性能提升的关键是在配置文件中至少有以下配置：</p>
<div class="highlight"><pre><span></span><code><span class="p">{</span>
<span class="w">  </span><span class="nt">&quot;zero_optimization&quot;</span><span class="p">:</span><span class="w"> </span><span class="p">{</span>
<span class="w">     </span><span class="nt">&quot;stage&quot;</span><span class="p">:</span><span class="w"> </span><span class="mi">2</span><span class="p">,</span>
<span class="w">     </span><span class="nt">&quot;offload_optimizer&quot;</span><span class="p">:</span><span class="w"> </span><span class="p">{</span>
<span class="w">         </span><span class="nt">&quot;device&quot;</span><span class="p">:</span><span class="w"> </span><span class="s2">&quot;cpu&quot;</span><span class="p">,</span>
<span class="w">         </span><span class="nt">&quot;pin_memory&quot;</span><span class="p">:</span><span class="w"> </span><span class="kc">true</span>
<span class="w">     </span><span class="p">},</span>
<span class="w">     </span><span class="nt">&quot;allgather_partitions&quot;</span><span class="p">:</span><span class="w"> </span><span class="kc">true</span><span class="p">,</span>
<span class="w">     </span><span class="nt">&quot;allgather_bucket_size&quot;</span><span class="p">:</span><span class="w"> </span><span class="mf">2e8</span><span class="p">,</span>
<span class="w">     </span><span class="nt">&quot;reduce_scatter&quot;</span><span class="p">:</span><span class="w"> </span><span class="kc">true</span><span class="p">,</span>
<span class="w">     </span><span class="nt">&quot;reduce_bucket_size&quot;</span><span class="p">:</span><span class="w"> </span><span class="mf">2e8</span><span class="p">,</span>
<span class="w">     </span><span class="nt">&quot;overlap_comm&quot;</span><span class="p">:</span><span class="w"> </span><span class="kc">true</span><span class="p">,</span>
<span class="w">     </span><span class="nt">&quot;contiguous_gradients&quot;</span><span class="p">:</span><span class="w"> </span><span class="kc">true</span>
<span class="w">  </span><span class="p">}</span>
<span class="p">}</span>
</code></pre></div>
<p>这会启用<code>optimizer offload</code>和一些其他重要功能。您可以尝试不同的buffer大小，有关详细信息，请参见下面的讨论。</p>
<p>关于这种启用类型的实际使用示例，请参阅 <a href="https://github.com/huggingface/transformers/issues/8771#issuecomment-759176685">此帖</a>。</p>
<p>您还可以尝试使用本文后面进一步解释的支持<code>CPU 和 NVMe offload</code>功能的ZeRO-3 。</p>
<!--- TODO: Benchmark whether we can get better performance out of ZeRO-3 vs. ZeRO-2 on a single GPU, and then
recommend ZeRO-3 config as starting one. -->

<p>注意：</p>
<ul>
<li>如果您需要在特定的 GPU 上运行，而不是 GPU 0，则无法使用 <code>CUDA_VISIBLE_DEVICES</code> 来限制可用 GPU 的可见范围。相反，您必须使用以下语法：</li>
</ul>
<div class="highlight"><pre><span></span><code>deepspeed<span class="w"> </span>--include<span class="w"> </span>localhost:1<span class="w"> </span>examples/pytorch/translation/run_translation.py<span class="w"> </span>...
</code></pre></div>
<p>在这个例子中，我们告诉 DeepSpeed 使用 GPU 1（第二个 GPU）。</p>
<p><a id='deepspeed-multi-node'></a></p>
<h3 id="_2">多节点启用<a class="headerlink" href="#_2" title="Permanent link">⚓︎</a></h3>
<p>这一部分的信息不仅适用于 DeepSpeed 集成，也适用于任何多节点程序。但 DeepSpeed 提供了一个比其他启动器更易于使用的 <code>deepspeed</code> 启动器，除非您在 SLURM 环境中。</p>
<p>在本节，让我们假设您有两个节点，每个节点有 8 张 GPU。您可以通过 <code>ssh hostname1</code> 访问第一个节点，通过 <code>ssh hostname2</code> 访问第二个节点，两者必须能够在本地通过 ssh 无密码方式相互访问。当然，您需要将这些主机（节点）名称重命名为您实际使用的主机名称。</p>
<h4 id="torchdistributedrun">torch.distributed.run启动器<a class="headerlink" href="#torchdistributedrun" title="Permanent link">⚓︎</a></h4>
<p>例如，要使用 <code>torch.distributed.run</code>，您可以执行以下操作：</p>
<div class="highlight"><pre><span></span><code>python<span class="w"> </span>-m<span class="w"> </span>torch.distributed.run<span class="w"> </span>--nproc_per_node<span class="o">=</span><span class="m">8</span><span class="w"> </span>--nnode<span class="o">=</span><span class="m">2</span><span class="w"> </span>--node_rank<span class="o">=</span><span class="m">0</span><span class="w"> </span>--master_addr<span class="o">=</span>hostname1<span class="w"> </span><span class="se">\</span>
--master_port<span class="o">=</span><span class="m">9901</span><span class="w"> </span>your_program.py<span class="w"> </span>&lt;normal<span class="w"> </span>cl<span class="w"> </span>args&gt;<span class="w"> </span>--deepspeed<span class="w"> </span>ds_config.json
</code></pre></div>
<p>您必须 ssh 到每个节点，并在每个节点上运行相同的命令！不用担心，启动器会等待两个节点同步完成。</p>
<p>有关更多信息，请参阅 <a href="https://pytorch.org/docs/stable/elastic/run.html">torchrun</a>。顺便说一下，这也是替代了几个 PyTorch 版本前的 <code>torch.distributed.launch</code> 的启动器。</p>
<h4 id="deepspeed_1">deepspeed启动器<a class="headerlink" href="#deepspeed_1" title="Permanent link">⚓︎</a></h4>
<p>要改用 <code>deepspeed</code> 启动器，首先需要创建一个 <code>hostfile</code> 文件：</p>
<p><div class="highlight"><pre><span></span><code>hostname1 slots=8
hostname2 slots=8
</code></pre></div>
然后，您可以这样启动：</p>
<div class="highlight"><pre><span></span><code>deepspeed<span class="w"> </span>--num_gpus<span class="w"> </span><span class="m">8</span><span class="w"> </span>--num_nodes<span class="w"> </span><span class="m">2</span><span class="w"> </span>--hostfile<span class="w"> </span>hostfile<span class="w"> </span>--master_addr<span class="w"> </span>hostname1<span class="w"> </span>--master_port<span class="o">=</span><span class="m">9901</span><span class="w"> </span><span class="se">\</span>
your_program.py<span class="w"> </span>&lt;normal<span class="w"> </span>cl<span class="w"> </span>args&gt;<span class="w"> </span>--deepspeed<span class="w"> </span>ds_config.json
</code></pre></div>
<p>与 <code>torch.distributed.run</code> 启动器不同，<code>deepspeed</code> 将自动在两个节点上启动此命令！</p>
<p>更多信息，请参阅<a href="https://www.deepspeed.ai/getting-started/#resource-configuration-multi-node">资源配置（多节点）</a>。</p>
<h4 id="slurm">在 SLURM 环境中启动<a class="headerlink" href="#slurm" title="Permanent link">⚓︎</a></h4>
<p>在 SLURM 环境中，可以采用以下方法。以下是一个 SLURM 脚本 <code>launch.slurm</code>，您需要根据您的具体 SLURM 环境进行调整。</p>
<div class="highlight"><pre><span></span><code><span class="c1">#SBATCH --job-name=test-nodes        # name</span>
<span class="c1">#SBATCH --nodes=2                    # nodes</span>
<span class="c1">#SBATCH --ntasks-per-node=1          # crucial - only 1 task per dist per node!</span>
<span class="c1">#SBATCH --cpus-per-task=10           # number of cores per tasks</span>
<span class="c1">#SBATCH --gres=gpu:8                 # number of gpus</span>
<span class="c1">#SBATCH --time 20:00:00              # maximum execution time (HH:MM:SS)</span>
<span class="c1">#SBATCH --output=%x-%j.out           # output file name</span>

<span class="nb">export</span><span class="w"> </span><span class="nv">GPUS_PER_NODE</span><span class="o">=</span><span class="m">8</span>
<span class="nb">export</span><span class="w"> </span><span class="nv">MASTER_ADDR</span><span class="o">=</span><span class="k">$(</span>scontrol<span class="w"> </span>show<span class="w"> </span>hostnames<span class="w"> </span><span class="nv">$SLURM_JOB_NODELIST</span><span class="w"> </span><span class="p">|</span><span class="w"> </span>head<span class="w"> </span>-n<span class="w"> </span><span class="m">1</span><span class="k">)</span>
<span class="nb">export</span><span class="w"> </span><span class="nv">MASTER_PORT</span><span class="o">=</span><span class="m">9901</span>

srun<span class="w"> </span>--jobid<span class="w"> </span><span class="nv">$SLURM_JOBID</span><span class="w"> </span>bash<span class="w"> </span>-c<span class="w"> </span><span class="s1">&#39;python -m torch.distributed.run \</span>
<span class="s1"> --nproc_per_node $GPUS_PER_NODE --nnodes $SLURM_NNODES --node_rank $SLURM_PROCID \</span>
<span class="s1"> --master_addr $MASTER_ADDR --master_port $MASTER_PORT \</span>
<span class="s1">your_program.py &lt;normal cl args&gt; --deepspeed ds_config.json&#39;</span>
</code></pre></div>
<p>剩下的就是运行它：</p>
<div class="highlight"><pre><span></span><code>sbatch<span class="w"> </span>launch.slurm
</code></pre></div>
<p><code>srun</code> 将负责在所有节点上同时启动程序。</p>
<h4 id="_3">使用非共享文件系统<a class="headerlink" href="#_3" title="Permanent link">⚓︎</a></h4>
<p>默认情况下，DeepSpeed 假定多节点环境使用共享存储。如果不是这种情况，每个节点只能看到本地文件系统，你需要调整配置文件，包含一个 <a href="https://www.deepspeed.ai/docs/config-json/#checkpoint-options"><code>checkpoint</code> 部分</a>并设置如下选项：</p>
<div class="highlight"><pre><span></span><code><span class="p">{</span>
<span class="w">  </span><span class="nt">&quot;checkpoint&quot;</span><span class="p">:</span><span class="w"> </span><span class="p">{</span>
<span class="w">    </span><span class="nt">&quot;use_node_local_storage&quot;</span><span class="p">:</span><span class="w"> </span><span class="kc">true</span>
<span class="w">  </span><span class="p">}</span>
<span class="p">}</span>
</code></pre></div>
<p>或者，你还可以使用 [<code>Trainer</code>] 的 <code>--save_on_each_node</code> 参数，上述配置将自动添加。</p>
<p><a id='deepspeed-notebook'></a></p>
<h3 id="notebooks">在Notebooks启用<a class="headerlink" href="#notebooks" title="Permanent link">⚓︎</a></h3>
<p>在将<code>notebook cells</code>作为脚本运行的情况下，问题在于没有正常的 <code>deepspeed</code> 启动器可依赖，因此在某些设置下，我们必须仿真运行它。</p>
<p>如果您只使用一个 GPU，以下是如何调整notebook中的训练代码以使用 DeepSpeed。</p>
<div class="highlight"><pre><span></span><code><span class="c1"># DeepSpeed requires a distributed environment even when only one process is used.</span>
<span class="c1"># This emulates a launcher in the notebook</span>
<span class="kn">import</span> <span class="nn">os</span>

<span class="n">os</span><span class="o">.</span><span class="n">environ</span><span class="p">[</span><span class="s2">&quot;MASTER_ADDR&quot;</span><span class="p">]</span> <span class="o">=</span> <span class="s2">&quot;localhost&quot;</span>
<span class="n">os</span><span class="o">.</span><span class="n">environ</span><span class="p">[</span><span class="s2">&quot;MASTER_PORT&quot;</span><span class="p">]</span> <span class="o">=</span> <span class="s2">&quot;9994&quot;</span>  <span class="c1"># modify if RuntimeError: Address already in use</span>
<span class="n">os</span><span class="o">.</span><span class="n">environ</span><span class="p">[</span><span class="s2">&quot;RANK&quot;</span><span class="p">]</span> <span class="o">=</span> <span class="s2">&quot;0&quot;</span>
<span class="n">os</span><span class="o">.</span><span class="n">environ</span><span class="p">[</span><span class="s2">&quot;LOCAL_RANK&quot;</span><span class="p">]</span> <span class="o">=</span> <span class="s2">&quot;0&quot;</span>
<span class="n">os</span><span class="o">.</span><span class="n">environ</span><span class="p">[</span><span class="s2">&quot;WORLD_SIZE&quot;</span><span class="p">]</span> <span class="o">=</span> <span class="s2">&quot;1&quot;</span>

<span class="c1"># Now proceed as normal, plus pass the deepspeed config file</span>
<span class="n">training_args</span> <span class="o">=</span> <span class="n">TrainingArguments</span><span class="p">(</span><span class="o">...</span><span class="p">,</span> <span class="n">deepspeed</span><span class="o">=</span><span class="s2">&quot;ds_config_zero3.json&quot;</span><span class="p">)</span>
<span class="n">trainer</span> <span class="o">=</span> <span class="n">Trainer</span><span class="p">(</span><span class="o">...</span><span class="p">)</span>
<span class="n">trainer</span><span class="o">.</span><span class="n">train</span><span class="p">()</span>
</code></pre></div>
<p>注意：<code>...</code> 代表您传递给函数的正常参数。</p>
<p>如果要使用多于一个 GPU，您必须在 DeepSpeed 中使用多进程环境。也就是说，您必须使用专门的启动器来实现这一目的，而不能通过仿真本节开头呈现的分布式环境来完成。</p>
<p>如果想要在notebook中动态创建配置文件并保存在当前目录，您可以在一个专用的cell中使用：</p>
<p>```python no-style
%%bash
cat &lt;&lt;'EOT' &gt; ds_config_zero3.json
{
    "fp16": {
        "enabled": "auto",
        "loss_scale": 0,
        "loss_scale_window": 1000,
        "initial_scale_power": 16,
        "hysteresis": 2,
        "min_loss_scale": 1
    },</p>
<div class="highlight"><pre><span></span><code>&quot;optimizer&quot;: {
    &quot;type&quot;: &quot;AdamW&quot;,
    &quot;params&quot;: {
        &quot;lr&quot;: &quot;auto&quot;,
        &quot;betas&quot;: &quot;auto&quot;,
        &quot;eps&quot;: &quot;auto&quot;,
        &quot;weight_decay&quot;: &quot;auto&quot;
    }
},

&quot;scheduler&quot;: {
    &quot;type&quot;: &quot;WarmupLR&quot;,
    &quot;params&quot;: {
        &quot;warmup_min_lr&quot;: &quot;auto&quot;,
        &quot;warmup_max_lr&quot;: &quot;auto&quot;,
        &quot;warmup_num_steps&quot;: &quot;auto&quot;
    }
},

&quot;zero_optimization&quot;: {
    &quot;stage&quot;: 3,
    &quot;offload_optimizer&quot;: {
        &quot;device&quot;: &quot;cpu&quot;,
        &quot;pin_memory&quot;: true
    },
    &quot;offload_param&quot;: {
        &quot;device&quot;: &quot;cpu&quot;,
        &quot;pin_memory&quot;: true
    },
    &quot;overlap_comm&quot;: true,
    &quot;contiguous_gradients&quot;: true,
    &quot;sub_group_size&quot;: 1e9,
    &quot;reduce_bucket_size&quot;: &quot;auto&quot;,
    &quot;stage3_prefetch_bucket_size&quot;: &quot;auto&quot;,
    &quot;stage3_param_persistence_threshold&quot;: &quot;auto&quot;,
    &quot;stage3_max_live_parameters&quot;: 1e9,
    &quot;stage3_max_reuse_distance&quot;: 1e9,
    &quot;stage3_gather_16bit_weights_on_model_save&quot;: true
},

&quot;gradient_accumulation_steps&quot;: &quot;auto&quot;,
&quot;gradient_clipping&quot;: &quot;auto&quot;,
&quot;steps_per_print&quot;: 2000,
&quot;train_batch_size&quot;: &quot;auto&quot;,
&quot;train_micro_batch_size_per_gpu&quot;: &quot;auto&quot;,
&quot;wall_clock_breakdown&quot;: false
</code></pre></div>
<p>}
EOT
<code>如果训练脚本在一个普通文件中而不是在notebook cells中，您可以通过笔记本中的 shell 正常启动 `deepspeed`。例如，要使用 `run_translation.py`，您可以这样启动：</code>python no-style
!git clone https://github.com/huggingface/transformers
!cd transformers; deepspeed examples/pytorch/translation/run_translation.py ...
```</p>
<p>或者使用 <code>%%bash</code> 魔术命令，您可以编写多行代码，用于运行 shell 程序：</p>
<p>```python no-style
%%bash</p>
<p>git clone https://github.com/huggingface/transformers
cd transformers
deepspeed examples/pytorch/translation/run_translation.py ...
<div class="highlight"><pre><span></span><code>在这种情况下，您不需要本节开头呈现的任何代码。

注意：虽然 `%%bash` 魔术命令很方便，但目前它会缓冲输出，因此在进程完成之前您看不到日志。


&lt;a id=&#39;deepspeed-config&#39;&gt;&lt;/a&gt;

### 配置

有关可以在 DeepSpeed 配置文件中使用的完整配置选项的详细指南，请参阅[以下文档](https://www.deepspeed.ai/docs/config-json/)。

您可以在 [DeepSpeedExamples 仓库](https://github.com/microsoft/DeepSpeedExamples)中找到解决各种实际需求的数十个 DeepSpeed 配置示例。

```bash
git clone https://github.com/microsoft/DeepSpeedExamples
cd DeepSpeedExamples
find . -name &#39;*json&#39;
</code></pre></div></p>
<p>延续上面的代码，假设您要配置 Lamb 优化器。那么您可以通过以下方式在示例的 <code>.json</code> 文件中进行搜索：</p>
<div class="highlight"><pre><span></span><code>grep<span class="w"> </span>-i<span class="w"> </span>Lamb<span class="w"> </span><span class="k">$(</span>find<span class="w"> </span>.<span class="w"> </span>-name<span class="w"> </span><span class="s1">&#39;*json&#39;</span><span class="k">)</span>
</code></pre></div>
<p>还可以在<a href="https://github.com/microsoft/DeepSpeed">主仓</a>中找到更多示例。</p>
<p>在使用 DeepSpeed 时，您总是需要提供一个 DeepSpeed 配置文件，但是一些配置参数必须通过命令行进行配置。您将在本指南的剩余章节找到这些细微差别。</p>
<p>为了了解 DeepSpeed 配置文件，这里有一个激活 ZeRO stage 2 功能的示例，包括优化器状态的 CPU offload，使用 <code>AdamW</code> 优化器和 <code>WarmupLR</code>  调度器，并且如果传递了 <code>--fp16</code> 参数将启用混合精度训练：</p>
<div class="highlight"><pre><span></span><code><span class="p">{</span>
<span class="w">    </span><span class="nt">&quot;fp16&quot;</span><span class="p">:</span><span class="w"> </span><span class="p">{</span>
<span class="w">        </span><span class="nt">&quot;enabled&quot;</span><span class="p">:</span><span class="w"> </span><span class="s2">&quot;auto&quot;</span><span class="p">,</span>
<span class="w">        </span><span class="nt">&quot;loss_scale&quot;</span><span class="p">:</span><span class="w"> </span><span class="mi">0</span><span class="p">,</span>
<span class="w">        </span><span class="nt">&quot;loss_scale_window&quot;</span><span class="p">:</span><span class="w"> </span><span class="mi">1000</span><span class="p">,</span>
<span class="w">        </span><span class="nt">&quot;initial_scale_power&quot;</span><span class="p">:</span><span class="w"> </span><span class="mi">16</span><span class="p">,</span>
<span class="w">        </span><span class="nt">&quot;hysteresis&quot;</span><span class="p">:</span><span class="w"> </span><span class="mi">2</span><span class="p">,</span>
<span class="w">        </span><span class="nt">&quot;min_loss_scale&quot;</span><span class="p">:</span><span class="w"> </span><span class="mi">1</span>
<span class="w">    </span><span class="p">},</span>

<span class="w">    </span><span class="nt">&quot;optimizer&quot;</span><span class="p">:</span><span class="w"> </span><span class="p">{</span>
<span class="w">        </span><span class="nt">&quot;type&quot;</span><span class="p">:</span><span class="w"> </span><span class="s2">&quot;AdamW&quot;</span><span class="p">,</span>
<span class="w">        </span><span class="nt">&quot;params&quot;</span><span class="p">:</span><span class="w"> </span><span class="p">{</span>
<span class="w">            </span><span class="nt">&quot;lr&quot;</span><span class="p">:</span><span class="w"> </span><span class="s2">&quot;auto&quot;</span><span class="p">,</span>
<span class="w">            </span><span class="nt">&quot;betas&quot;</span><span class="p">:</span><span class="w"> </span><span class="s2">&quot;auto&quot;</span><span class="p">,</span>
<span class="w">            </span><span class="nt">&quot;eps&quot;</span><span class="p">:</span><span class="w"> </span><span class="s2">&quot;auto&quot;</span><span class="p">,</span>
<span class="w">            </span><span class="nt">&quot;weight_decay&quot;</span><span class="p">:</span><span class="w"> </span><span class="s2">&quot;auto&quot;</span>
<span class="w">        </span><span class="p">}</span>
<span class="w">    </span><span class="p">},</span>

<span class="w">    </span><span class="nt">&quot;scheduler&quot;</span><span class="p">:</span><span class="w"> </span><span class="p">{</span>
<span class="w">        </span><span class="nt">&quot;type&quot;</span><span class="p">:</span><span class="w"> </span><span class="s2">&quot;WarmupLR&quot;</span><span class="p">,</span>
<span class="w">        </span><span class="nt">&quot;params&quot;</span><span class="p">:</span><span class="w"> </span><span class="p">{</span>
<span class="w">            </span><span class="nt">&quot;warmup_min_lr&quot;</span><span class="p">:</span><span class="w"> </span><span class="s2">&quot;auto&quot;</span><span class="p">,</span>
<span class="w">            </span><span class="nt">&quot;warmup_max_lr&quot;</span><span class="p">:</span><span class="w"> </span><span class="s2">&quot;auto&quot;</span><span class="p">,</span>
<span class="w">            </span><span class="nt">&quot;warmup_num_steps&quot;</span><span class="p">:</span><span class="w"> </span><span class="s2">&quot;auto&quot;</span>
<span class="w">        </span><span class="p">}</span>
<span class="w">    </span><span class="p">},</span>

<span class="w">    </span><span class="nt">&quot;zero_optimization&quot;</span><span class="p">:</span><span class="w"> </span><span class="p">{</span>
<span class="w">        </span><span class="nt">&quot;stage&quot;</span><span class="p">:</span><span class="w"> </span><span class="mi">2</span><span class="p">,</span>
<span class="w">        </span><span class="nt">&quot;offload_optimizer&quot;</span><span class="p">:</span><span class="w"> </span><span class="p">{</span>
<span class="w">            </span><span class="nt">&quot;device&quot;</span><span class="p">:</span><span class="w"> </span><span class="s2">&quot;cpu&quot;</span><span class="p">,</span>
<span class="w">            </span><span class="nt">&quot;pin_memory&quot;</span><span class="p">:</span><span class="w"> </span><span class="kc">true</span>
<span class="w">        </span><span class="p">},</span>
<span class="w">        </span><span class="nt">&quot;allgather_partitions&quot;</span><span class="p">:</span><span class="w"> </span><span class="kc">true</span><span class="p">,</span>
<span class="w">        </span><span class="nt">&quot;allgather_bucket_size&quot;</span><span class="p">:</span><span class="w"> </span><span class="mf">2e8</span><span class="p">,</span>
<span class="w">        </span><span class="nt">&quot;overlap_comm&quot;</span><span class="p">:</span><span class="w"> </span><span class="kc">true</span><span class="p">,</span>
<span class="w">        </span><span class="nt">&quot;reduce_scatter&quot;</span><span class="p">:</span><span class="w"> </span><span class="kc">true</span><span class="p">,</span>
<span class="w">        </span><span class="nt">&quot;reduce_bucket_size&quot;</span><span class="p">:</span><span class="w"> </span><span class="mf">2e8</span><span class="p">,</span>
<span class="w">        </span><span class="nt">&quot;contiguous_gradients&quot;</span><span class="p">:</span><span class="w"> </span><span class="kc">true</span>
<span class="w">    </span><span class="p">},</span>

<span class="w">    </span><span class="nt">&quot;gradient_accumulation_steps&quot;</span><span class="p">:</span><span class="w"> </span><span class="s2">&quot;auto&quot;</span><span class="p">,</span>
<span class="w">    </span><span class="nt">&quot;gradient_clipping&quot;</span><span class="p">:</span><span class="w"> </span><span class="s2">&quot;auto&quot;</span><span class="p">,</span>
<span class="w">    </span><span class="nt">&quot;train_batch_size&quot;</span><span class="p">:</span><span class="w"> </span><span class="s2">&quot;auto&quot;</span><span class="p">,</span>
<span class="w">    </span><span class="nt">&quot;train_micro_batch_size_per_gpu&quot;</span><span class="p">:</span><span class="w"> </span><span class="s2">&quot;auto&quot;</span><span class="p">,</span>
<span class="p">}</span>
</code></pre></div>
<p>当您执行程序时，DeepSpeed 将把它从 [<code>Trainer</code>] 收到的配置日志输出到console，因此您可以看到传递给它的最终配置。</p>
<p><a id='deepspeed-config-passing'></a></p>
<h3 id="_4">传递配置<a class="headerlink" href="#_4" title="Permanent link">⚓︎</a></h3>
<p>正如本文档讨论的那样，通常将 DeepSpeed 配置作为指向 JSON 文件的路径传递，但如果您没有使用命令行界面配置训练，而是通过 [<code>TrainingArguments</code>] 实例化 [<code>Trainer</code>]，那么对于 <code>deepspeed</code> 参数，你可以传递一个嵌套的 <code>dict</code>。这使您能够即时创建配置，而无需在将其传递给 [<code>TrainingArguments</code>] 之前将其写入文件系统。</p>
<p>总结起来，您可以这样做：</p>
<div class="highlight"><pre><span></span><code><span class="n">TrainingArguments</span><span class="p">(</span><span class="o">...</span><span class="p">,</span> <span class="n">deepspeed</span><span class="o">=</span><span class="s2">&quot;/path/to/ds_config.json&quot;</span><span class="p">)</span>
</code></pre></div>
<p>或者:</p>
<div class="highlight"><pre><span></span><code><span class="n">ds_config_dict</span> <span class="o">=</span> <span class="nb">dict</span><span class="p">(</span><span class="n">scheduler</span><span class="o">=</span><span class="n">scheduler_params</span><span class="p">,</span> <span class="n">optimizer</span><span class="o">=</span><span class="n">optimizer_params</span><span class="p">)</span>
<span class="n">TrainingArguments</span><span class="p">(</span><span class="o">...</span><span class="p">,</span> <span class="n">deepspeed</span><span class="o">=</span><span class="n">ds_config_dict</span><span class="p">)</span>
</code></pre></div>
<p><a id='deepspeed-config-shared'></a></p>
<h3 id="_5">共享配置<a class="headerlink" href="#_5" title="Permanent link">⚓︎</a></h3>
<p><Tip warning={true}></p>
<p>这一部分是必读的。</p>
<p></Tip></p>
<p>一些配置值对于 [<code>Trainer</code>] 和 DeepSpeed 正常运行都是必需的，因此，为了防止定义冲突及导致的难以检测的错误，我们选择通过 [<code>Trainer</code>] 命令行参数配置这些值。</p>
<p>此外，一些配置值是基于模型的配置自动派生的，因此，与其记住手动调整多个值，最好让 [<code>Trainer</code>] 为您做大部分配置。</p>
<p>因此，在本指南的其余部分，您将找到一个特殊的配置值：<code>auto</code>，当设置时将自动将参数替换为正确或最有效的值。请随意选择忽略此建议或显式设置该值，在这种情况下，请务必确保 [<code>Trainer</code>] 参数和 DeepSpeed 配置保持一致。例如，您是否使用相同的学习率、批量大小或梯度累积设置？如果这些不匹配，训练可能以非常难以检测的方式失败。请重视该警告。</p>
<p>还有一些参数是仅适用于 DeepSpeed 的，并且这些参数必须手动设置以适应您的需求。</p>
<p>在您自己的程序中，如果您想要作为主动修改 DeepSpeed 配置并以此配置 [<code>TrainingArguments</code>]，您还可以使用以下方法。步骤如下：</p>
<ol>
<li>创建或加载要用作主配置的 DeepSpeed 配置</li>
<li>根据这些参数值创建 [<code>TrainingArguments</code>] 对象</li>
</ol>
<p>请注意，一些值，比如 <code>scheduler.params.total_num_steps</code>，是在 [<code>Trainer</code>] 的 <code>train</code> 过程中计算的，但当然您也可以自己计算这些值。</p>
<p><a id='deepspeed-zero'></a></p>
<h3 id="zero">ZeRO<a class="headerlink" href="#zero" title="Permanent link">⚓︎</a></h3>
<p><a href="https://www.deepspeed.ai/tutorials/zero/">Zero Redundancy Optimizer (ZeRO)</a> 是 DeepSpeed 的工作核心。它支持3个不同级别（stages）的优化。Stage 1 对于扩展性来说不是很有趣，因此本文档重点关注Stage 2和Stage 3。Stage 3通过最新的 ZeRO-Infinity 进一步改进。你可以在 DeepSpeed 文档中找到更详细的信息。</p>
<p>配置文件的 <code>zero_optimization</code> 部分是最重要的部分（<a href="https://www.deepspeed.ai/docs/config-json/#zero-optimizations-for-fp16-training">文档</a>），因为在这里您定义了要启用哪些 ZeRO stages 以及如何配置它们。您可以在 DeepSpeed 文档中找到每个参数的解释。</p>
<p>这一部分必须通过 DeepSpeed 配置文件单独配置 - [<code>Trainer</code>] 不提供相应的命令行参数。</p>
<p>注意：目前 DeepSpeed 不验证参数名称，因此如果您拼错了任何参数，它将使用拼写错误的参数的默认设置。您可以观察 DeepSpeed 引擎启动日志消息，看看它将使用哪些值。</p>
<p><a id='deepspeed-zero2-config'></a></p>
<h4 id="zero-2">ZeRO-2 配置<a class="headerlink" href="#zero-2" title="Permanent link">⚓︎</a></h4>
<p>以下是 ZeRO stage 2 的配置示例：</p>
<div class="highlight"><pre><span></span><code><span class="p">{</span>
<span class="w">    </span><span class="nt">&quot;zero_optimization&quot;</span><span class="p">:</span><span class="w"> </span><span class="p">{</span>
<span class="w">        </span><span class="nt">&quot;stage&quot;</span><span class="p">:</span><span class="w"> </span><span class="mi">2</span><span class="p">,</span>
<span class="w">        </span><span class="nt">&quot;offload_optimizer&quot;</span><span class="p">:</span><span class="w"> </span><span class="p">{</span>
<span class="w">            </span><span class="nt">&quot;device&quot;</span><span class="p">:</span><span class="w"> </span><span class="s2">&quot;cpu&quot;</span><span class="p">,</span>
<span class="w">            </span><span class="nt">&quot;pin_memory&quot;</span><span class="p">:</span><span class="w"> </span><span class="kc">true</span>
<span class="w">        </span><span class="p">},</span>
<span class="w">        </span><span class="nt">&quot;allgather_partitions&quot;</span><span class="p">:</span><span class="w"> </span><span class="kc">true</span><span class="p">,</span>
<span class="w">        </span><span class="nt">&quot;allgather_bucket_size&quot;</span><span class="p">:</span><span class="w"> </span><span class="mf">5e8</span><span class="p">,</span>
<span class="w">        </span><span class="nt">&quot;overlap_comm&quot;</span><span class="p">:</span><span class="w"> </span><span class="kc">true</span><span class="p">,</span>
<span class="w">        </span><span class="nt">&quot;reduce_scatter&quot;</span><span class="p">:</span><span class="w"> </span><span class="kc">true</span><span class="p">,</span>
<span class="w">        </span><span class="nt">&quot;reduce_bucket_size&quot;</span><span class="p">:</span><span class="w"> </span><span class="mf">5e8</span><span class="p">,</span>
<span class="w">        </span><span class="nt">&quot;contiguous_gradients&quot;</span><span class="p">:</span><span class="w"> </span><span class="kc">true</span>
<span class="w">    </span><span class="p">}</span>
<span class="p">}</span>
</code></pre></div>
<p><strong>性能调优：</strong></p>
<ul>
<li>启用 <code>offload_optimizer</code> 应该减少 GPU 内存使用（需要 <code>"stage": 2</code>）。</li>
<li><code>"overlap_comm": true</code> 通过增加 GPU 内存使用来降低all-reduce 的延迟。 <code>overlap_comm</code> 使用了 <code>allgather_bucket_size</code> 和 <code>reduce_bucket_size</code> 值的4.5倍。因此，如果它们设置为 <code>5e8</code>，这将需要一个9GB的内存占用（<code>5e8 x 2Bytes x 2 x 4.5</code>）。因此，如果您的 GPU 内存为8GB或更小，为了避免出现OOM错误，您需要将这些参数减小到约 <code>2e8</code>，这将需要3.6GB。如果您的 GPU 容量更大，当您开始遇到OOM时，你可能也需要这样做。</li>
<li>当减小这些buffers时，您以更慢的通信速度来换取更多的 GPU 内存。buffers大小越小，通信速度越慢，GPU 可用于其他任务的内存就越多。因此，如果更大的批处理大小很重要，那么稍微减慢训练时间可能是一个很好的权衡。</li>
</ul>
<p>此外，<code>deepspeed==0.4.4</code> 添加了一个新选项 <code>round_robin_gradients</code>，您可以通过以下方式启用：</p>
<p><div class="highlight"><pre><span></span><code><span class="p">{</span>
<span class="w">    </span><span class="nt">&quot;zero_optimization&quot;</span><span class="p">:</span><span class="w"> </span><span class="p">{</span>
<span class="w">        </span><span class="nt">&quot;round_robin_gradients&quot;</span><span class="p">:</span><span class="w"> </span><span class="kc">true</span>
<span class="w">    </span><span class="p">}</span>
<span class="p">}</span>
</code></pre></div>
这是一个用于 CPU offloading 的stage 2优化，通过细粒度梯度分区在 ranks 之间并行复制到 CPU 内存，从而实现了性能的提升。性能优势随着梯度累积步骤（在优化器步骤之间进行更多复制）或 GPU 数量（增加并行性）增加而增加。</p>
<p><a id='deepspeed-zero3-config'></a></p>
<h4 id="zero-3">ZeRO-3 配置<a class="headerlink" href="#zero-3" title="Permanent link">⚓︎</a></h4>
<p>以下是 ZeRO stage 3的配置示例：</p>
<div class="highlight"><pre><span></span><code><span class="p">{</span>
<span class="w">    </span><span class="nt">&quot;zero_optimization&quot;</span><span class="p">:</span><span class="w"> </span><span class="p">{</span>
<span class="w">        </span><span class="nt">&quot;stage&quot;</span><span class="p">:</span><span class="w"> </span><span class="mi">3</span><span class="p">,</span>
<span class="w">        </span><span class="nt">&quot;offload_optimizer&quot;</span><span class="p">:</span><span class="w"> </span><span class="p">{</span>
<span class="w">            </span><span class="nt">&quot;device&quot;</span><span class="p">:</span><span class="w"> </span><span class="s2">&quot;cpu&quot;</span><span class="p">,</span>
<span class="w">            </span><span class="nt">&quot;pin_memory&quot;</span><span class="p">:</span><span class="w"> </span><span class="kc">true</span>
<span class="w">        </span><span class="p">},</span>
<span class="w">        </span><span class="nt">&quot;offload_param&quot;</span><span class="p">:</span><span class="w"> </span><span class="p">{</span>
<span class="w">            </span><span class="nt">&quot;device&quot;</span><span class="p">:</span><span class="w"> </span><span class="s2">&quot;cpu&quot;</span><span class="p">,</span>
<span class="w">            </span><span class="nt">&quot;pin_memory&quot;</span><span class="p">:</span><span class="w"> </span><span class="kc">true</span>
<span class="w">        </span><span class="p">},</span>
<span class="w">        </span><span class="nt">&quot;overlap_comm&quot;</span><span class="p">:</span><span class="w"> </span><span class="kc">true</span><span class="p">,</span>
<span class="w">        </span><span class="nt">&quot;contiguous_gradients&quot;</span><span class="p">:</span><span class="w"> </span><span class="kc">true</span><span class="p">,</span>
<span class="w">        </span><span class="nt">&quot;sub_group_size&quot;</span><span class="p">:</span><span class="w"> </span><span class="mf">1e9</span><span class="p">,</span>
<span class="w">        </span><span class="nt">&quot;reduce_bucket_size&quot;</span><span class="p">:</span><span class="w"> </span><span class="s2">&quot;auto&quot;</span><span class="p">,</span>
<span class="w">        </span><span class="nt">&quot;stage3_prefetch_bucket_size&quot;</span><span class="p">:</span><span class="w"> </span><span class="s2">&quot;auto&quot;</span><span class="p">,</span>
<span class="w">        </span><span class="nt">&quot;stage3_param_persistence_threshold&quot;</span><span class="p">:</span><span class="w"> </span><span class="s2">&quot;auto&quot;</span><span class="p">,</span>
<span class="w">        </span><span class="nt">&quot;stage3_max_live_parameters&quot;</span><span class="p">:</span><span class="w"> </span><span class="mf">1e9</span><span class="p">,</span>
<span class="w">        </span><span class="nt">&quot;stage3_max_reuse_distance&quot;</span><span class="p">:</span><span class="w"> </span><span class="mf">1e9</span><span class="p">,</span>
<span class="w">        </span><span class="nt">&quot;stage3_gather_16bit_weights_on_model_save&quot;</span><span class="p">:</span><span class="w"> </span><span class="kc">true</span>
<span class="w">    </span><span class="p">}</span>
<span class="p">}</span>
</code></pre></div>
<p>如果您因为你的模型或激活值超过 GPU 内存而遇到OOM问题，并且您有未使用的 CPU 内存，可以通股票使用 <code>"device": "cpu"</code> 将优化器状态和参数卸载到 CPU 内存中，来解决这个限制。如果您不想卸载到 CPU 内存，可以在 <code>device</code> 条目中使用 <code>none</code> 代替 <code>cpu</code>。将优化器状态卸载到 NVMe 上会在后面进一步讨论。</p>
<p>通过将 <code>pin_memory</code> 设置为 <code>true</code> 启用固定内存。此功能会以减少可用于其他进程的内存为代价来提高吞吐量。固定内存被分配给特定请求它的进程，通常比普通 CPU 内存访问速度更快。</p>
<p><strong>性能调优：</strong></p>
<ul>
<li><code>stage3_max_live_parameters</code>: <code>1e9</code></li>
<li><code>stage3_max_reuse_distance</code>: <code>1e9</code></li>
</ul>
<p>如果遇到OOM问题，请减小 <code>stage3_max_live_parameters</code> 和 <code>stage3_max_reuse_distance</code>。它们对性能的影响应该很小，除非您正在进行激活值checkpointing。<code>1e9</code> 大约会消耗 ~2GB。内存由 <code>stage3_max_live_parameters</code> 和 <code>stage3_max_reuse_distance</code> 共享，所以它不是叠加的，而是总共2GB。</p>
<p><code>stage3_max_live_parameters</code> 是在任何给定时间要在 GPU 上保留多少个完整参数的上限。"reuse distance" 是我们用来确定参数在将来何时会再次使用的度量标准，我们使用 <code>stage3_max_reuse_distance</code> 来决定是丢弃参数还是保留参数。如果一个参数在不久的将来（小于 <code>stage3_max_reuse_distance</code>）将被再次使用，那么我们将其保留以减少通信开销。这在启用激活值checkpoing时非常有用，其中我们以单层粒度进行前向重计算和反向传播，并希望在反向传播期间保留前向重计算中的参数。</p>
<p>以下配置值取决于模型的隐藏大小：</p>
<ul>
<li><code>reduce_bucket_size</code>: <code>hidden_size*hidden_size</code></li>
<li><code>stage3_prefetch_bucket_size</code>: <code>0.9 * hidden_size * hidden_size</code></li>
<li><code>stage3_param_persistence_threshold</code>: <code>10 * hidden_size</code></li>
</ul>
<p>因此，将这些值设置为 <code>auto</code>，[<code>Trainer</code>] 将自动分配推荐的参数值。当然，如果您愿意，也可以显式设置这些值。</p>
<p><code>stage3_gather_16bit_weights_on_model_save</code> 在模型保存时启用模型的 fp16 权重整合。对于大模型和多个 GPU，无论是在内存还是速度方面，这都是一项昂贵的操作。目前如果计划恢复训练，这是必需的。请注意未来的更新可能会删除此限制并让使用更加灵活。</p>
<p>如果您从 ZeRO-2 配置迁移，请注意 <code>allgather_partitions</code>、<code>allgather_bucket_size</code> 和 <code>reduce_scatter</code> 配置参数在 ZeRO-3 中不被使用。如果保留这些配置文件，它们将被忽略。</p>
<ul>
<li><code>sub_group_size</code>: <code>1e9</code></li>
</ul>
<p><code>sub_group_size</code> 控制在优化器步骤期间更新参数的粒度。参数被分组到大小为 <code>sub_group_size</code> 的桶中，每个桶逐个更新。在 ZeRO-Infinity 中与 NVMe offload一起使用时，<code>sub_group_size</code> 控制了在优化器步骤期间在 NVMe 和 CPU 内存之间移动模型状态的粒度。这可以防止非常大的模型耗尽 CPU 内存。</p>
<p>当不使用 NVMe offload时，可以将 <code>sub_group_size</code> 保留为其默认值 <em>1e9</em>。在以下情况下，您可能需要更改其默认值：</p>
<ol>
<li>在优化器步骤中遇到OOM：减小 <code>sub_group_size</code> 以减少临时buffers的内存利用</li>
<li>优化器步骤花费很长时间：增加 <code>sub_group_size</code> 以提高由于增加的数据buffers而导致的带宽利用率。</li>
</ol>
<h4 id="zero-0">ZeRO-0 配置<a class="headerlink" href="#zero-0" title="Permanent link">⚓︎</a></h4>
<p>请注意，我们将 Stage 0 和 1 放在最后，因为它们很少使用。</p>
<p>Stage 0 禁用了所有类型的分片，只是将 DeepSpeed 作为 DDP 使用。您可以通过以下方式启用：</p>
<div class="highlight"><pre><span></span><code><span class="p">{</span>
<span class="w">    </span><span class="nt">&quot;zero_optimization&quot;</span><span class="p">:</span><span class="w"> </span><span class="p">{</span>
<span class="w">        </span><span class="nt">&quot;stage&quot;</span><span class="p">:</span><span class="w"> </span><span class="mi">0</span>
<span class="w">    </span><span class="p">}</span>
<span class="p">}</span>
</code></pre></div>
<p>这将实质上禁用 ZeRO，而无需更改其他任何内容。</p>
<h4 id="zero-1">ZeRO-1 配置<a class="headerlink" href="#zero-1" title="Permanent link">⚓︎</a></h4>
<p>Stage 1 等同于 Stage 2 减去梯度分片。您可以尝试使用以下配置，仅对优化器状态进行分片，以稍微加速：</p>
<div class="highlight"><pre><span></span><code><span class="p">{</span>
<span class="w">    </span><span class="nt">&quot;zero_optimization&quot;</span><span class="p">:</span><span class="w"> </span><span class="p">{</span>
<span class="w">        </span><span class="nt">&quot;stage&quot;</span><span class="p">:</span><span class="w"> </span><span class="mi">1</span>
<span class="w">    </span><span class="p">}</span>
<span class="p">}</span>
</code></pre></div>
<p><a id='deepspeed-nvme'></a></p>
<h3 id="nvme">NVMe 支持<a class="headerlink" href="#nvme" title="Permanent link">⚓︎</a></h3>
<p>ZeRO-Infinity 通过使用 NVMe 内存扩展 GPU 和 CPU 内存，从而允许训练非常大的模型。由于智能分区和平铺算法，在offload期间每个 GPU 需要发送和接收非常小量的数据，因此 NVMe 被证明适用于训练过程中提供更大的总内存池。ZeRO-Infinity 需要启用 ZeRO-3。</p>
<p>以下配置示例启用 NVMe 来offload优化器状态和参数：</p>
<div class="highlight"><pre><span></span><code><span class="p">{</span>
<span class="w">    </span><span class="nt">&quot;zero_optimization&quot;</span><span class="p">:</span><span class="w"> </span><span class="p">{</span>
<span class="w">        </span><span class="nt">&quot;stage&quot;</span><span class="p">:</span><span class="w"> </span><span class="mi">3</span><span class="p">,</span>
<span class="w">        </span><span class="nt">&quot;offload_optimizer&quot;</span><span class="p">:</span><span class="w"> </span><span class="p">{</span>
<span class="w">            </span><span class="nt">&quot;device&quot;</span><span class="p">:</span><span class="w"> </span><span class="s2">&quot;nvme&quot;</span><span class="p">,</span>
<span class="w">            </span><span class="nt">&quot;nvme_path&quot;</span><span class="p">:</span><span class="w"> </span><span class="s2">&quot;/local_nvme&quot;</span><span class="p">,</span>
<span class="w">            </span><span class="nt">&quot;pin_memory&quot;</span><span class="p">:</span><span class="w"> </span><span class="kc">true</span><span class="p">,</span>
<span class="w">            </span><span class="nt">&quot;buffer_count&quot;</span><span class="p">:</span><span class="w"> </span><span class="mi">4</span><span class="p">,</span>
<span class="w">            </span><span class="nt">&quot;fast_init&quot;</span><span class="p">:</span><span class="w"> </span><span class="kc">false</span>
<span class="w">        </span><span class="p">},</span>
<span class="w">        </span><span class="nt">&quot;offload_param&quot;</span><span class="p">:</span><span class="w"> </span><span class="p">{</span>
<span class="w">            </span><span class="nt">&quot;device&quot;</span><span class="p">:</span><span class="w"> </span><span class="s2">&quot;nvme&quot;</span><span class="p">,</span>
<span class="w">            </span><span class="nt">&quot;nvme_path&quot;</span><span class="p">:</span><span class="w"> </span><span class="s2">&quot;/local_nvme&quot;</span><span class="p">,</span>
<span class="w">            </span><span class="nt">&quot;pin_memory&quot;</span><span class="p">:</span><span class="w"> </span><span class="kc">true</span><span class="p">,</span>
<span class="w">            </span><span class="nt">&quot;buffer_count&quot;</span><span class="p">:</span><span class="w"> </span><span class="mi">5</span><span class="p">,</span>
<span class="w">            </span><span class="nt">&quot;buffer_size&quot;</span><span class="p">:</span><span class="w"> </span><span class="mf">1e8</span><span class="p">,</span>
<span class="w">            </span><span class="nt">&quot;max_in_cpu&quot;</span><span class="p">:</span><span class="w"> </span><span class="mf">1e9</span>
<span class="w">        </span><span class="p">},</span>
<span class="w">        </span><span class="nt">&quot;aio&quot;</span><span class="p">:</span><span class="w"> </span><span class="p">{</span>
<span class="w">            </span><span class="nt">&quot;block_size&quot;</span><span class="p">:</span><span class="w"> </span><span class="mi">262144</span><span class="p">,</span>
<span class="w">            </span><span class="nt">&quot;queue_depth&quot;</span><span class="p">:</span><span class="w"> </span><span class="mi">32</span><span class="p">,</span>
<span class="w">            </span><span class="nt">&quot;thread_count&quot;</span><span class="p">:</span><span class="w"> </span><span class="mi">1</span><span class="p">,</span>
<span class="w">            </span><span class="nt">&quot;single_submit&quot;</span><span class="p">:</span><span class="w"> </span><span class="kc">false</span><span class="p">,</span>
<span class="w">            </span><span class="nt">&quot;overlap_events&quot;</span><span class="p">:</span><span class="w"> </span><span class="kc">true</span>
<span class="w">        </span><span class="p">},</span>
<span class="w">        </span><span class="nt">&quot;overlap_comm&quot;</span><span class="p">:</span><span class="w"> </span><span class="kc">true</span><span class="p">,</span>
<span class="w">        </span><span class="nt">&quot;contiguous_gradients&quot;</span><span class="p">:</span><span class="w"> </span><span class="kc">true</span><span class="p">,</span>
<span class="w">        </span><span class="nt">&quot;sub_group_size&quot;</span><span class="p">:</span><span class="w"> </span><span class="mf">1e9</span><span class="p">,</span>
<span class="w">        </span><span class="nt">&quot;reduce_bucket_size&quot;</span><span class="p">:</span><span class="w"> </span><span class="s2">&quot;auto&quot;</span><span class="p">,</span>
<span class="w">        </span><span class="nt">&quot;stage3_prefetch_bucket_size&quot;</span><span class="p">:</span><span class="w"> </span><span class="s2">&quot;auto&quot;</span><span class="p">,</span>
<span class="w">        </span><span class="nt">&quot;stage3_param_persistence_threshold&quot;</span><span class="p">:</span><span class="w"> </span><span class="s2">&quot;auto&quot;</span><span class="p">,</span>
<span class="w">        </span><span class="nt">&quot;stage3_max_live_parameters&quot;</span><span class="p">:</span><span class="w"> </span><span class="mf">1e9</span><span class="p">,</span>
<span class="w">        </span><span class="nt">&quot;stage3_max_reuse_distance&quot;</span><span class="p">:</span><span class="w"> </span><span class="mf">1e9</span><span class="p">,</span>
<span class="w">        </span><span class="nt">&quot;stage3_gather_16bit_weights_on_model_save&quot;</span><span class="p">:</span><span class="w"> </span><span class="kc">true</span>
<span class="w">    </span><span class="p">},</span>
<span class="p">}</span>
</code></pre></div>
<p>您可以选择将优化器状态和参数都卸载到 NVMe，也可以只选择其中一个，或者都不选择。例如，如果您有大量的 CPU 内存可用，只卸载到 CPU 内存训练速度会更快（提示："device": "cpu"）。</p>
<p>这是有关卸载 <a href="https://www.deepspeed.ai/docs/config-json/#optimizer-offloading">优化器状态</a> 和 <a href="https://www.deepspeed.ai/docs/config-json/#parameter-offloading">参数</a> 的完整文档。</p>
<p>确保您的 <code>nvme_path</code> 实际上是一个 NVMe，因为它与普通硬盘或 SSD 一起工作，但速度会慢得多。快速可扩展的训练是根据现代 NVMe 传输速度设计的（截至本文撰写时，可以达到 ~3.5GB/s 读取，~3GB/s 写入的峰值速度）。</p>
<p>为了找出最佳的 <code>aio</code> 配置块，您必须在目标设置上运行一个基准测试，具体操作请参见<a href="https://github.com/microsoft/DeepSpeed/issues/998">说明</a>。</p>
<p><a id='deepspeed-zero2-zero3-performance'></a></p>
<h4 id="zero-2-zero-3">ZeRO-2 和 ZeRO-3 性能对比<a class="headerlink" href="#zero-2-zero-3" title="Permanent link">⚓︎</a></h4>
<p>如果其他一切都配置相同，ZeRO-3 可能比 ZeRO-2 慢，因为前者除了 ZeRO-2 的操作外，还必须收集模型权重。如果 ZeRO-2 满足您的需求，而且您不需要扩展到几个 GPU 以上，那么您可以选择继续使用它。重要的是要理解，ZeRO-3 以速度为代价实现了更高的可扩展性。</p>
<p>可以调整 ZeRO-3 配置使其性能接近 ZeRO-2：</p>
<ul>
<li>将 <code>stage3_param_persistence_threshold</code> 设置为一个非常大的数字 - 大于最大的参数，例如 <code>6 * hidden_size * hidden_size</code>。这将保留参数在 GPU 上。</li>
<li>关闭 <code>offload_params</code>，因为 ZeRO-2 没有这个选项。</li>
</ul>
<p>即使不更改 <code>stage3_param_persistence_threshold</code>，仅将 <code>offload_params</code> 关闭，性能可能会显著提高。当然，这些更改将影响您可以训练的模型的大小。因此，这些更改可根据需求帮助您在可扩展性和速度之间进行权衡。</p>
<p><a id='deepspeed-zero2-example'></a></p>
<h4 id="zero-2_1">ZeRO-2 示例<a class="headerlink" href="#zero-2_1" title="Permanent link">⚓︎</a></h4>
<p>这是一个完整的 ZeRO-2 自动配置文件 <code>ds_config_zero2.json</code>：</p>
<div class="highlight"><pre><span></span><code><span class="p">{</span>
<span class="w">    </span><span class="nt">&quot;fp16&quot;</span><span class="p">:</span><span class="w"> </span><span class="p">{</span>
<span class="w">        </span><span class="nt">&quot;enabled&quot;</span><span class="p">:</span><span class="w"> </span><span class="s2">&quot;auto&quot;</span><span class="p">,</span>
<span class="w">        </span><span class="nt">&quot;loss_scale&quot;</span><span class="p">:</span><span class="w"> </span><span class="mi">0</span><span class="p">,</span>
<span class="w">        </span><span class="nt">&quot;loss_scale_window&quot;</span><span class="p">:</span><span class="w"> </span><span class="mi">1000</span><span class="p">,</span>
<span class="w">        </span><span class="nt">&quot;initial_scale_power&quot;</span><span class="p">:</span><span class="w"> </span><span class="mi">16</span><span class="p">,</span>
<span class="w">        </span><span class="nt">&quot;hysteresis&quot;</span><span class="p">:</span><span class="w"> </span><span class="mi">2</span><span class="p">,</span>
<span class="w">        </span><span class="nt">&quot;min_loss_scale&quot;</span><span class="p">:</span><span class="w"> </span><span class="mi">1</span>
<span class="w">    </span><span class="p">},</span>

<span class="w">    </span><span class="nt">&quot;optimizer&quot;</span><span class="p">:</span><span class="w"> </span><span class="p">{</span>
<span class="w">        </span><span class="nt">&quot;type&quot;</span><span class="p">:</span><span class="w"> </span><span class="s2">&quot;AdamW&quot;</span><span class="p">,</span>
<span class="w">        </span><span class="nt">&quot;params&quot;</span><span class="p">:</span><span class="w"> </span><span class="p">{</span>
<span class="w">            </span><span class="nt">&quot;lr&quot;</span><span class="p">:</span><span class="w"> </span><span class="s2">&quot;auto&quot;</span><span class="p">,</span>
<span class="w">            </span><span class="nt">&quot;betas&quot;</span><span class="p">:</span><span class="w"> </span><span class="s2">&quot;auto&quot;</span><span class="p">,</span>
<span class="w">            </span><span class="nt">&quot;eps&quot;</span><span class="p">:</span><span class="w"> </span><span class="s2">&quot;auto&quot;</span><span class="p">,</span>
<span class="w">            </span><span class="nt">&quot;weight_decay&quot;</span><span class="p">:</span><span class="w"> </span><span class="s2">&quot;auto&quot;</span>
<span class="w">        </span><span class="p">}</span>
<span class="w">    </span><span class="p">},</span>

<span class="w">    </span><span class="nt">&quot;scheduler&quot;</span><span class="p">:</span><span class="w"> </span><span class="p">{</span>
<span class="w">        </span><span class="nt">&quot;type&quot;</span><span class="p">:</span><span class="w"> </span><span class="s2">&quot;WarmupLR&quot;</span><span class="p">,</span>
<span class="w">        </span><span class="nt">&quot;params&quot;</span><span class="p">:</span><span class="w"> </span><span class="p">{</span>
<span class="w">            </span><span class="nt">&quot;warmup_min_lr&quot;</span><span class="p">:</span><span class="w"> </span><span class="s2">&quot;auto&quot;</span><span class="p">,</span>
<span class="w">            </span><span class="nt">&quot;warmup_max_lr&quot;</span><span class="p">:</span><span class="w"> </span><span class="s2">&quot;auto&quot;</span><span class="p">,</span>
<span class="w">            </span><span class="nt">&quot;warmup_num_steps&quot;</span><span class="p">:</span><span class="w"> </span><span class="s2">&quot;auto&quot;</span>
<span class="w">        </span><span class="p">}</span>
<span class="w">    </span><span class="p">},</span>

<span class="w">    </span><span class="nt">&quot;zero_optimization&quot;</span><span class="p">:</span><span class="w"> </span><span class="p">{</span>
<span class="w">        </span><span class="nt">&quot;stage&quot;</span><span class="p">:</span><span class="w"> </span><span class="mi">2</span><span class="p">,</span>
<span class="w">        </span><span class="nt">&quot;offload_optimizer&quot;</span><span class="p">:</span><span class="w"> </span><span class="p">{</span>
<span class="w">            </span><span class="nt">&quot;device&quot;</span><span class="p">:</span><span class="w"> </span><span class="s2">&quot;cpu&quot;</span><span class="p">,</span>
<span class="w">            </span><span class="nt">&quot;pin_memory&quot;</span><span class="p">:</span><span class="w"> </span><span class="kc">true</span>
<span class="w">        </span><span class="p">},</span>
<span class="w">        </span><span class="nt">&quot;allgather_partitions&quot;</span><span class="p">:</span><span class="w"> </span><span class="kc">true</span><span class="p">,</span>
<span class="w">        </span><span class="nt">&quot;allgather_bucket_size&quot;</span><span class="p">:</span><span class="w"> </span><span class="mf">2e8</span><span class="p">,</span>
<span class="w">        </span><span class="nt">&quot;overlap_comm&quot;</span><span class="p">:</span><span class="w"> </span><span class="kc">true</span><span class="p">,</span>
<span class="w">        </span><span class="nt">&quot;reduce_scatter&quot;</span><span class="p">:</span><span class="w"> </span><span class="kc">true</span><span class="p">,</span>
<span class="w">        </span><span class="nt">&quot;reduce_bucket_size&quot;</span><span class="p">:</span><span class="w"> </span><span class="mf">2e8</span><span class="p">,</span>
<span class="w">        </span><span class="nt">&quot;contiguous_gradients&quot;</span><span class="p">:</span><span class="w"> </span><span class="kc">true</span>
<span class="w">    </span><span class="p">},</span>

<span class="w">    </span><span class="nt">&quot;gradient_accumulation_steps&quot;</span><span class="p">:</span><span class="w"> </span><span class="s2">&quot;auto&quot;</span><span class="p">,</span>
<span class="w">    </span><span class="nt">&quot;gradient_clipping&quot;</span><span class="p">:</span><span class="w"> </span><span class="s2">&quot;auto&quot;</span><span class="p">,</span>
<span class="w">    </span><span class="nt">&quot;steps_per_print&quot;</span><span class="p">:</span><span class="w"> </span><span class="mi">2000</span><span class="p">,</span>
<span class="w">    </span><span class="nt">&quot;train_batch_size&quot;</span><span class="p">:</span><span class="w"> </span><span class="s2">&quot;auto&quot;</span><span class="p">,</span>
<span class="w">    </span><span class="nt">&quot;train_micro_batch_size_per_gpu&quot;</span><span class="p">:</span><span class="w"> </span><span class="s2">&quot;auto&quot;</span><span class="p">,</span>
<span class="w">    </span><span class="nt">&quot;wall_clock_breakdown&quot;</span><span class="p">:</span><span class="w"> </span><span class="kc">false</span>
<span class="p">}</span>
</code></pre></div>
<p>这是一个完整的手动设置的启用所有功能的 ZeRO-2 配置文件。主要是为了让您看到典型的参数值是什么样的，但我们强烈建议使用其中包含多个 <code>auto</code> 设置的配置文件。</p>
<div class="highlight"><pre><span></span><code><span class="p">{</span>
<span class="w">    </span><span class="nt">&quot;fp16&quot;</span><span class="p">:</span><span class="w"> </span><span class="p">{</span>
<span class="w">        </span><span class="nt">&quot;enabled&quot;</span><span class="p">:</span><span class="w"> </span><span class="kc">true</span><span class="p">,</span>
<span class="w">        </span><span class="nt">&quot;loss_scale&quot;</span><span class="p">:</span><span class="w"> </span><span class="mi">0</span><span class="p">,</span>
<span class="w">        </span><span class="nt">&quot;loss_scale_window&quot;</span><span class="p">:</span><span class="w"> </span><span class="mi">1000</span><span class="p">,</span>
<span class="w">        </span><span class="nt">&quot;initial_scale_power&quot;</span><span class="p">:</span><span class="w"> </span><span class="mi">16</span><span class="p">,</span>
<span class="w">        </span><span class="nt">&quot;hysteresis&quot;</span><span class="p">:</span><span class="w"> </span><span class="mi">2</span><span class="p">,</span>
<span class="w">        </span><span class="nt">&quot;min_loss_scale&quot;</span><span class="p">:</span><span class="w"> </span><span class="mi">1</span>
<span class="w">    </span><span class="p">},</span>

<span class="w">    </span><span class="nt">&quot;optimizer&quot;</span><span class="p">:</span><span class="w"> </span><span class="p">{</span>
<span class="w">        </span><span class="nt">&quot;type&quot;</span><span class="p">:</span><span class="w"> </span><span class="s2">&quot;AdamW&quot;</span><span class="p">,</span>
<span class="w">        </span><span class="nt">&quot;params&quot;</span><span class="p">:</span><span class="w"> </span><span class="p">{</span>
<span class="w">            </span><span class="nt">&quot;lr&quot;</span><span class="p">:</span><span class="w"> </span><span class="mf">3e-5</span><span class="p">,</span>
<span class="w">            </span><span class="nt">&quot;betas&quot;</span><span class="p">:</span><span class="w"> </span><span class="p">[</span><span class="mf">0.8</span><span class="p">,</span><span class="w"> </span><span class="mf">0.999</span><span class="p">],</span>
<span class="w">            </span><span class="nt">&quot;eps&quot;</span><span class="p">:</span><span class="w"> </span><span class="mf">1e-8</span><span class="p">,</span>
<span class="w">            </span><span class="nt">&quot;weight_decay&quot;</span><span class="p">:</span><span class="w"> </span><span class="mf">3e-7</span>
<span class="w">        </span><span class="p">}</span>
<span class="w">    </span><span class="p">},</span>

<span class="w">    </span><span class="nt">&quot;scheduler&quot;</span><span class="p">:</span><span class="w"> </span><span class="p">{</span>
<span class="w">        </span><span class="nt">&quot;type&quot;</span><span class="p">:</span><span class="w"> </span><span class="s2">&quot;WarmupLR&quot;</span><span class="p">,</span>
<span class="w">        </span><span class="nt">&quot;params&quot;</span><span class="p">:</span><span class="w"> </span><span class="p">{</span>
<span class="w">            </span><span class="nt">&quot;warmup_min_lr&quot;</span><span class="p">:</span><span class="w"> </span><span class="mi">0</span><span class="p">,</span>
<span class="w">            </span><span class="nt">&quot;warmup_max_lr&quot;</span><span class="p">:</span><span class="w"> </span><span class="mf">3e-5</span><span class="p">,</span>
<span class="w">            </span><span class="nt">&quot;warmup_num_steps&quot;</span><span class="p">:</span><span class="w"> </span><span class="mi">500</span>
<span class="w">        </span><span class="p">}</span>
<span class="w">    </span><span class="p">},</span>

<span class="w">    </span><span class="nt">&quot;zero_optimization&quot;</span><span class="p">:</span><span class="w"> </span><span class="p">{</span>
<span class="w">        </span><span class="nt">&quot;stage&quot;</span><span class="p">:</span><span class="w"> </span><span class="mi">2</span><span class="p">,</span>
<span class="w">        </span><span class="nt">&quot;offload_optimizer&quot;</span><span class="p">:</span><span class="w"> </span><span class="p">{</span>
<span class="w">            </span><span class="nt">&quot;device&quot;</span><span class="p">:</span><span class="w"> </span><span class="s2">&quot;cpu&quot;</span><span class="p">,</span>
<span class="w">            </span><span class="nt">&quot;pin_memory&quot;</span><span class="p">:</span><span class="w"> </span><span class="kc">true</span>
<span class="w">        </span><span class="p">},</span>
<span class="w">        </span><span class="nt">&quot;allgather_partitions&quot;</span><span class="p">:</span><span class="w"> </span><span class="kc">true</span><span class="p">,</span>
<span class="w">        </span><span class="nt">&quot;allgather_bucket_size&quot;</span><span class="p">:</span><span class="w"> </span><span class="mf">2e8</span><span class="p">,</span>
<span class="w">        </span><span class="nt">&quot;overlap_comm&quot;</span><span class="p">:</span><span class="w"> </span><span class="kc">true</span><span class="p">,</span>
<span class="w">        </span><span class="nt">&quot;reduce_scatter&quot;</span><span class="p">:</span><span class="w"> </span><span class="kc">true</span><span class="p">,</span>
<span class="w">        </span><span class="nt">&quot;reduce_bucket_size&quot;</span><span class="p">:</span><span class="w"> </span><span class="mf">2e8</span><span class="p">,</span>
<span class="w">        </span><span class="nt">&quot;contiguous_gradients&quot;</span><span class="p">:</span><span class="w"> </span><span class="kc">true</span>
<span class="w">    </span><span class="p">},</span>

<span class="w">    </span><span class="nt">&quot;steps_per_print&quot;</span><span class="p">:</span><span class="w"> </span><span class="mi">2000</span><span class="p">,</span>
<span class="w">    </span><span class="nt">&quot;wall_clock_breakdown&quot;</span><span class="p">:</span><span class="w"> </span><span class="kc">false</span>
<span class="p">}</span>
</code></pre></div>
<p><a id='deepspeed-zero3-example'></a></p>
<h4 id="zero-3_1">ZeRO-3 示例<a class="headerlink" href="#zero-3_1" title="Permanent link">⚓︎</a></h4>
<p>这是一个完整的 ZeRO-3 自动配置文件 <code>ds_config_zero3.json</code>：</p>
<div class="highlight"><pre><span></span><code><span class="p">{</span>
<span class="w">    </span><span class="nt">&quot;fp16&quot;</span><span class="p">:</span><span class="w"> </span><span class="p">{</span>
<span class="w">        </span><span class="nt">&quot;enabled&quot;</span><span class="p">:</span><span class="w"> </span><span class="s2">&quot;auto&quot;</span><span class="p">,</span>
<span class="w">        </span><span class="nt">&quot;loss_scale&quot;</span><span class="p">:</span><span class="w"> </span><span class="mi">0</span><span class="p">,</span>
<span class="w">        </span><span class="nt">&quot;loss_scale_window&quot;</span><span class="p">:</span><span class="w"> </span><span class="mi">1000</span><span class="p">,</span>
<span class="w">        </span><span class="nt">&quot;initial_scale_power&quot;</span><span class="p">:</span><span class="w"> </span><span class="mi">16</span><span class="p">,</span>
<span class="w">        </span><span class="nt">&quot;hysteresis&quot;</span><span class="p">:</span><span class="w"> </span><span class="mi">2</span><span class="p">,</span>
<span class="w">        </span><span class="nt">&quot;min_loss_scale&quot;</span><span class="p">:</span><span class="w"> </span><span class="mi">1</span>
<span class="w">    </span><span class="p">},</span>

<span class="w">    </span><span class="nt">&quot;optimizer&quot;</span><span class="p">:</span><span class="w"> </span><span class="p">{</span>
<span class="w">        </span><span class="nt">&quot;type&quot;</span><span class="p">:</span><span class="w"> </span><span class="s2">&quot;AdamW&quot;</span><span class="p">,</span>
<span class="w">        </span><span class="nt">&quot;params&quot;</span><span class="p">:</span><span class="w"> </span><span class="p">{</span>
<span class="w">            </span><span class="nt">&quot;lr&quot;</span><span class="p">:</span><span class="w"> </span><span class="s2">&quot;auto&quot;</span><span class="p">,</span>
<span class="w">            </span><span class="nt">&quot;betas&quot;</span><span class="p">:</span><span class="w"> </span><span class="s2">&quot;auto&quot;</span><span class="p">,</span>
<span class="w">            </span><span class="nt">&quot;eps&quot;</span><span class="p">:</span><span class="w"> </span><span class="s2">&quot;auto&quot;</span><span class="p">,</span>
<span class="w">            </span><span class="nt">&quot;weight_decay&quot;</span><span class="p">:</span><span class="w"> </span><span class="s2">&quot;auto&quot;</span>
<span class="w">        </span><span class="p">}</span>
<span class="w">    </span><span class="p">},</span>

<span class="w">    </span><span class="nt">&quot;scheduler&quot;</span><span class="p">:</span><span class="w"> </span><span class="p">{</span>
<span class="w">        </span><span class="nt">&quot;type&quot;</span><span class="p">:</span><span class="w"> </span><span class="s2">&quot;WarmupLR&quot;</span><span class="p">,</span>
<span class="w">        </span><span class="nt">&quot;params&quot;</span><span class="p">:</span><span class="w"> </span><span class="p">{</span>
<span class="w">            </span><span class="nt">&quot;warmup_min_lr&quot;</span><span class="p">:</span><span class="w"> </span><span class="s2">&quot;auto&quot;</span><span class="p">,</span>
<span class="w">            </span><span class="nt">&quot;warmup_max_lr&quot;</span><span class="p">:</span><span class="w"> </span><span class="s2">&quot;auto&quot;</span><span class="p">,</span>
<span class="w">            </span><span class="nt">&quot;warmup_num_steps&quot;</span><span class="p">:</span><span class="w"> </span><span class="s2">&quot;auto&quot;</span>
<span class="w">        </span><span class="p">}</span>
<span class="w">    </span><span class="p">},</span>

<span class="w">    </span><span class="nt">&quot;zero_optimization&quot;</span><span class="p">:</span><span class="w"> </span><span class="p">{</span>
<span class="w">        </span><span class="nt">&quot;stage&quot;</span><span class="p">:</span><span class="w"> </span><span class="mi">3</span><span class="p">,</span>
<span class="w">        </span><span class="nt">&quot;offload_optimizer&quot;</span><span class="p">:</span><span class="w"> </span><span class="p">{</span>
<span class="w">            </span><span class="nt">&quot;device&quot;</span><span class="p">:</span><span class="w"> </span><span class="s2">&quot;cpu&quot;</span><span class="p">,</span>
<span class="w">            </span><span class="nt">&quot;pin_memory&quot;</span><span class="p">:</span><span class="w"> </span><span class="kc">true</span>
<span class="w">        </span><span class="p">},</span>
<span class="w">        </span><span class="nt">&quot;offload_param&quot;</span><span class="p">:</span><span class="w"> </span><span class="p">{</span>
<span class="w">            </span><span class="nt">&quot;device&quot;</span><span class="p">:</span><span class="w"> </span><span class="s2">&quot;cpu&quot;</span><span class="p">,</span>
<span class="w">            </span><span class="nt">&quot;pin_memory&quot;</span><span class="p">:</span><span class="w"> </span><span class="kc">true</span>
<span class="w">        </span><span class="p">},</span>
<span class="w">        </span><span class="nt">&quot;overlap_comm&quot;</span><span class="p">:</span><span class="w"> </span><span class="kc">true</span><span class="p">,</span>
<span class="w">        </span><span class="nt">&quot;contiguous_gradients&quot;</span><span class="p">:</span><span class="w"> </span><span class="kc">true</span><span class="p">,</span>
<span class="w">        </span><span class="nt">&quot;sub_group_size&quot;</span><span class="p">:</span><span class="w"> </span><span class="mf">1e9</span><span class="p">,</span>
<span class="w">        </span><span class="nt">&quot;reduce_bucket_size&quot;</span><span class="p">:</span><span class="w"> </span><span class="s2">&quot;auto&quot;</span><span class="p">,</span>
<span class="w">        </span><span class="nt">&quot;stage3_prefetch_bucket_size&quot;</span><span class="p">:</span><span class="w"> </span><span class="s2">&quot;auto&quot;</span><span class="p">,</span>
<span class="w">        </span><span class="nt">&quot;stage3_param_persistence_threshold&quot;</span><span class="p">:</span><span class="w"> </span><span class="s2">&quot;auto&quot;</span><span class="p">,</span>
<span class="w">        </span><span class="nt">&quot;stage3_max_live_parameters&quot;</span><span class="p">:</span><span class="w"> </span><span class="mf">1e9</span><span class="p">,</span>
<span class="w">        </span><span class="nt">&quot;stage3_max_reuse_distance&quot;</span><span class="p">:</span><span class="w"> </span><span class="mf">1e9</span><span class="p">,</span>
<span class="w">        </span><span class="nt">&quot;stage3_gather_16bit_weights_on_model_save&quot;</span><span class="p">:</span><span class="w"> </span><span class="kc">true</span>
<span class="w">    </span><span class="p">},</span>

<span class="w">    </span><span class="nt">&quot;gradient_accumulation_steps&quot;</span><span class="p">:</span><span class="w"> </span><span class="s2">&quot;auto&quot;</span><span class="p">,</span>
<span class="w">    </span><span class="nt">&quot;gradient_clipping&quot;</span><span class="p">:</span><span class="w"> </span><span class="s2">&quot;auto&quot;</span><span class="p">,</span>
<span class="w">    </span><span class="nt">&quot;steps_per_print&quot;</span><span class="p">:</span><span class="w"> </span><span class="mi">2000</span><span class="p">,</span>
<span class="w">    </span><span class="nt">&quot;train_batch_size&quot;</span><span class="p">:</span><span class="w"> </span><span class="s2">&quot;auto&quot;</span><span class="p">,</span>
<span class="w">    </span><span class="nt">&quot;train_micro_batch_size_per_gpu&quot;</span><span class="p">:</span><span class="w"> </span><span class="s2">&quot;auto&quot;</span><span class="p">,</span>
<span class="w">    </span><span class="nt">&quot;wall_clock_breakdown&quot;</span><span class="p">:</span><span class="w"> </span><span class="kc">false</span>
<span class="p">}</span>
</code></pre></div>
<p>这是一个完整的 手动设置的启用所有功能的ZeRO-3 配置文件。主要是为了让您看到典型的参数值是什么样的，但我们强烈建议使用其中包含多个 <code>auto</code> 设置的配置文件。</p>
<div class="highlight"><pre><span></span><code><span class="p">{</span>
<span class="w">    </span><span class="nt">&quot;fp16&quot;</span><span class="p">:</span><span class="w"> </span><span class="p">{</span>
<span class="w">        </span><span class="nt">&quot;enabled&quot;</span><span class="p">:</span><span class="w"> </span><span class="kc">true</span><span class="p">,</span>
<span class="w">        </span><span class="nt">&quot;loss_scale&quot;</span><span class="p">:</span><span class="w"> </span><span class="mi">0</span><span class="p">,</span>
<span class="w">        </span><span class="nt">&quot;loss_scale_window&quot;</span><span class="p">:</span><span class="w"> </span><span class="mi">1000</span><span class="p">,</span>
<span class="w">        </span><span class="nt">&quot;initial_scale_power&quot;</span><span class="p">:</span><span class="w"> </span><span class="mi">16</span><span class="p">,</span>
<span class="w">        </span><span class="nt">&quot;hysteresis&quot;</span><span class="p">:</span><span class="w"> </span><span class="mi">2</span><span class="p">,</span>
<span class="w">        </span><span class="nt">&quot;min_loss_scale&quot;</span><span class="p">:</span><span class="w"> </span><span class="mi">1</span>
<span class="w">    </span><span class="p">},</span>

<span class="w">    </span><span class="nt">&quot;optimizer&quot;</span><span class="p">:</span><span class="w"> </span><span class="p">{</span>
<span class="w">        </span><span class="nt">&quot;type&quot;</span><span class="p">:</span><span class="w"> </span><span class="s2">&quot;AdamW&quot;</span><span class="p">,</span>
<span class="w">        </span><span class="nt">&quot;params&quot;</span><span class="p">:</span><span class="w"> </span><span class="p">{</span>
<span class="w">            </span><span class="nt">&quot;lr&quot;</span><span class="p">:</span><span class="w"> </span><span class="mf">3e-5</span><span class="p">,</span>
<span class="w">            </span><span class="nt">&quot;betas&quot;</span><span class="p">:</span><span class="w"> </span><span class="p">[</span><span class="mf">0.8</span><span class="p">,</span><span class="w"> </span><span class="mf">0.999</span><span class="p">],</span>
<span class="w">            </span><span class="nt">&quot;eps&quot;</span><span class="p">:</span><span class="w"> </span><span class="mf">1e-8</span><span class="p">,</span>
<span class="w">            </span><span class="nt">&quot;weight_decay&quot;</span><span class="p">:</span><span class="w"> </span><span class="mf">3e-7</span>
<span class="w">        </span><span class="p">}</span>
<span class="w">    </span><span class="p">},</span>

<span class="w">    </span><span class="nt">&quot;scheduler&quot;</span><span class="p">:</span><span class="w"> </span><span class="p">{</span>
<span class="w">        </span><span class="nt">&quot;type&quot;</span><span class="p">:</span><span class="w"> </span><span class="s2">&quot;WarmupLR&quot;</span><span class="p">,</span>
<span class="w">        </span><span class="nt">&quot;params&quot;</span><span class="p">:</span><span class="w"> </span><span class="p">{</span>
<span class="w">            </span><span class="nt">&quot;warmup_min_lr&quot;</span><span class="p">:</span><span class="w"> </span><span class="mi">0</span><span class="p">,</span>
<span class="w">            </span><span class="nt">&quot;warmup_max_lr&quot;</span><span class="p">:</span><span class="w"> </span><span class="mf">3e-5</span><span class="p">,</span>
<span class="w">            </span><span class="nt">&quot;warmup_num_steps&quot;</span><span class="p">:</span><span class="w"> </span><span class="mi">500</span>
<span class="w">        </span><span class="p">}</span>
<span class="w">    </span><span class="p">},</span>

<span class="w">    </span><span class="nt">&quot;zero_optimization&quot;</span><span class="p">:</span><span class="w"> </span><span class="p">{</span>
<span class="w">        </span><span class="nt">&quot;stage&quot;</span><span class="p">:</span><span class="w"> </span><span class="mi">3</span><span class="p">,</span>
<span class="w">        </span><span class="nt">&quot;offload_optimizer&quot;</span><span class="p">:</span><span class="w"> </span><span class="p">{</span>
<span class="w">            </span><span class="nt">&quot;device&quot;</span><span class="p">:</span><span class="w"> </span><span class="s2">&quot;cpu&quot;</span><span class="p">,</span>
<span class="w">            </span><span class="nt">&quot;pin_memory&quot;</span><span class="p">:</span><span class="w"> </span><span class="kc">true</span>
<span class="w">        </span><span class="p">},</span>
<span class="w">        </span><span class="nt">&quot;offload_param&quot;</span><span class="p">:</span><span class="w"> </span><span class="p">{</span>
<span class="w">            </span><span class="nt">&quot;device&quot;</span><span class="p">:</span><span class="w"> </span><span class="s2">&quot;cpu&quot;</span><span class="p">,</span>
<span class="w">            </span><span class="nt">&quot;pin_memory&quot;</span><span class="p">:</span><span class="w"> </span><span class="kc">true</span>
<span class="w">        </span><span class="p">},</span>
<span class="w">        </span><span class="nt">&quot;overlap_comm&quot;</span><span class="p">:</span><span class="w"> </span><span class="kc">true</span><span class="p">,</span>
<span class="w">        </span><span class="nt">&quot;contiguous_gradients&quot;</span><span class="p">:</span><span class="w"> </span><span class="kc">true</span><span class="p">,</span>
<span class="w">        </span><span class="nt">&quot;sub_group_size&quot;</span><span class="p">:</span><span class="w"> </span><span class="mf">1e9</span><span class="p">,</span>
<span class="w">        </span><span class="nt">&quot;reduce_bucket_size&quot;</span><span class="p">:</span><span class="w"> </span><span class="mf">1e6</span><span class="p">,</span>
<span class="w">        </span><span class="nt">&quot;stage3_prefetch_bucket_size&quot;</span><span class="p">:</span><span class="w"> </span><span class="mf">0.94e6</span><span class="p">,</span>
<span class="w">        </span><span class="nt">&quot;stage3_param_persistence_threshold&quot;</span><span class="p">:</span><span class="w"> </span><span class="mf">1e4</span><span class="p">,</span>
<span class="w">        </span><span class="nt">&quot;stage3_max_live_parameters&quot;</span><span class="p">:</span><span class="w"> </span><span class="mf">1e9</span><span class="p">,</span>
<span class="w">        </span><span class="nt">&quot;stage3_max_reuse_distance&quot;</span><span class="p">:</span><span class="w"> </span><span class="mf">1e9</span><span class="p">,</span>
<span class="w">        </span><span class="nt">&quot;stage3_gather_16bit_weights_on_model_save&quot;</span><span class="p">:</span><span class="w"> </span><span class="kc">true</span>
<span class="w">    </span><span class="p">},</span>

<span class="w">    </span><span class="nt">&quot;steps_per_print&quot;</span><span class="p">:</span><span class="w"> </span><span class="mi">2000</span><span class="p">,</span>
<span class="w">    </span><span class="nt">&quot;wall_clock_breakdown&quot;</span><span class="p">:</span><span class="w"> </span><span class="kc">false</span>
<span class="p">}</span>
</code></pre></div>
<h4 id="zero-stage-offloads">如何选择最佳性能的ZeRO Stage和 offloads<a class="headerlink" href="#zero-stage-offloads" title="Permanent link">⚓︎</a></h4>
<p>了解了这些不同stages后，现在您需要决定使用哪个stage。本节将尝试回答这个问题。</p>
<p>通常，以下规则适用：</p>
<ul>
<li>速度方面（左边比右边快）</li>
</ul>
<p>stage 0（DDP） &gt; stage 1 &gt; stage 2 &gt; stage 2 + offload  &gt; stage 3 &gt; stage3 + offload</p>
<ul>
<li>GPU内存使用方面（右边比左边更节省GPU内存）</li>
</ul>
<p>stage 0（DDP） &lt; stage 1 &lt; stage 2 &lt; stage 2 + offload &lt; stage 3 &lt; stage 3 + offload</p>
<p>所以，当您希望在尽量使用较少数量的GPU的同时获得最快的执行速度时，可以按照以下步骤进行。我们从最快的方法开始，如果遇到GPU内存溢出，然后切换到下一个速度较慢但使用的GPU内存更少的方法。以此类推。</p>
<p>首先，将批量大小设置为1（您始终可以使用梯度累积来获得任何所需的有效批量大小）。</p>
<ol>
<li>启用 <code>--gradient_checkpointing 1</code>（HF Trainer）或直接 <code>model.gradient_checkpointing_enable()</code> - 如果发生OOM（Out of Memory），则执行以下步骤。</li>
<li>首先尝试 ZeRO stage 2。如果发生OOM，则执行以下步骤。</li>
<li>尝试 ZeRO stage 2 + <code>offload_optimizer</code> - 如果发生OOM，则执行以下步骤。</li>
<li>切换到 ZeRO stage 3 - 如果发生OOM，则执行以下步骤。</li>
<li>启用 <code>offload_param</code> 到 <code>cpu</code> - 如果发生OOM，则执行以下步骤。</li>
<li>启用 <code>offload_optimizer</code> 到 <code>cpu</code> - 如果发生OOM，则执行以下步骤。</li>
<li>如果仍然无法适应批量大小为1，请首先检查各种默认值并尽可能降低它们。例如，如果使用 <code>generate</code> 并且不使用宽搜索束，将其缩小，因为它会占用大量内存。</li>
<li>绝对要使用混合半精度而非fp32 - 在Ampere及更高的GPU上使用bf16，在旧的GPU体系结构上使用fp16。</li>
<li>如果仍然发生OOM，可以添加更多硬件或启用ZeRO-Infinity - 即切换 <code>offload_param</code> 和 <code>offload_optimizer</code> 到 <code>nvme</code>。您需要确保它是非常快的NVMe。作为趣闻，我曾经能够在一个小型GPU上使用BLOOM-176B进行推理，使用了ZeRO-Infinity，尽管速度非常慢。但它奏效了！</li>
</ol>
<p>当然，您也可以按相反的顺序进行这些步骤，从最节省GPU内存的配置开始，然后逐步反向进行，或者尝试进行二分法。</p>
<p>一旦您的批量大小为1不会导致OOM，就测量您的有效吞吐量。</p>
<p>接下来尝试将批量大小增加到尽可能大，因为批量大小越大，GPU的效率越高，特别是在它们乘法运算的矩阵很大时。</p>
<p>现在性能优化游戏开始了。您可以关闭一些offload特性，或者降低ZeRO stage，并增加/减少批量大小，再次测量有效吞吐量。反复尝试，直到满意为止。</p>
<p>不要花费太多时间，但如果您即将开始一个为期3个月的训练 - 请花几天时间找到吞吐量方面最有效的设置。这样您的训练成本将最低，而且您会更快地完成训练。在当前快节奏的机器学习世界中，如果您花费一个额外的月份来训练某样东西，你很可能会错过一个黄金机会。当然，这只是我分享的一种观察，我并不是在催促你。在开始训练BLOOM-176B之前，我花了2天时间进行这个过程，成功将吞吐量从90 TFLOPs提高到150 TFLOPs！这一努力为我们节省了一个多月的训练时间。</p>
<p>这些注释主要是为训练模式编写的，但它们在推理中也应该大部分适用。例如，在推理中，Gradient Checkpointing 是无用的，因为它只在训练过程中有用。此外，我们发现，如果你正在进行多GPU推理并且不使用 <a href="https://www.deepspeed.ai/tutorials/inference-tutorial/">DeepSpeed-Inference</a>，<a href="https://huggingface.co/blog/bloom-inference-pytorch-scripts">Accelerate</a> 应该提供更优越的性能。</p>
<p>其他与性能相关的快速注释：
- 如果您从头开始训练某个模型，请尽量确保张量的形状可以被16整除（例如隐藏层大小）。对于批量大小，至少尝试可被2整除。如果您想从GPU中挤取更高性能，还有一些硬件特定的<a href="https://developer.nvidia.com/blog/optimizing-gpu-performance-tensor-cores/">wave和tile量化</a>的可整除性。</p>
<h3 id="activation-checkpointing-gradient-checkpointing">Activation Checkpointing 或 Gradient Checkpointing<a class="headerlink" href="#activation-checkpointing-gradient-checkpointing" title="Permanent link">⚓︎</a></h3>
<p>Activation Checkpointing和Gradient Checkpointing是指相同方法的两个不同术语。这确实让人感到困惑，但事实就是这样。</p>
<p>Gradient Checkpointing允许通过牺牲速度来换取GPU内存，这要么使您能够克服GPU内存溢出，要么增加批量大小来获得更好的性能。</p>
<p>HF Transformers 模型对DeepSpeed的Activation Checkpointing一无所知，因此如果尝试在DeepSpeed配置文件中启用该功能，什么都不会发生。</p>
<p>因此，您有两种方法可以利用这个非常有益的功能：</p>
<ol>
<li>如果您想使用 HF Transformers 模型，你可以使用 <code>model.gradient_checkpointing_enable()</code> 或在 HF Trainer 中使用 <code>--gradient_checkpointing</code>，它会自动为您启用这个功能。在这里使用了 <code>torch.utils.checkpoint</code>。</li>
<li>如果您编写自己的模型并希望使用DeepSpeed的Activation Checkpointing，可以使用<a href="https://deepspeed.readthedocs.io/en/latest/activation-checkpointing.html">规定的API</a>。您还可以使用 HF Transformers 的模型代码，将 <code>torch.utils.checkpoint</code> 替换为 DeepSpeed 的API。后者更灵活，因为它允许您将前向激活值卸载到CPU内存，而不是重新计算它们。</li>
</ol>
<h3 id="optimizer-scheduler">Optimizer 和 Scheduler<a class="headerlink" href="#optimizer-scheduler" title="Permanent link">⚓︎</a></h3>
<p>只要你不启用 <code>offload_optimizer</code>，您可以混合使用DeepSpeed和HuggingFace的调度器和优化器，但有一个例外，即不要使用HuggingFace调度器和DeepSpeed优化器的组合：</p>
<table>
<thead>
<tr>
<th style="text-align: left;">Combos</th>
<th style="text-align: left;">HF Scheduler</th>
<th style="text-align: left;">DS Scheduler</th>
</tr>
</thead>
<tbody>
<tr>
<td style="text-align: left;">HF Optimizer</td>
<td style="text-align: left;">Yes</td>
<td style="text-align: left;">Yes</td>
</tr>
<tr>
<td style="text-align: left;">DS Optimizer</td>
<td style="text-align: left;">No</td>
<td style="text-align: left;">Yes</td>
</tr>
</tbody>
</table>
<p>在启用 <code>offload_optimizer</code> 的情况下，可以使用非DeepSpeed优化器，只要该优化器具有CPU和GPU的实现（除了LAMB）。</p>
<p><a id='deepspeed-optimizer'></a></p>
<h4 id="optimizer">Optimizer<a class="headerlink" href="#optimizer" title="Permanent link">⚓︎</a></h4>
<p>DeepSpeed的主要优化器包括Adam、AdamW、OneBitAdam和Lamb。这些优化器已经与ZeRO进行了彻底的测试，因此建议使用它们。然而，也可以导入<code>torch</code>中的其他优化器。完整的文档在<a href="https://www.deepspeed.ai/docs/config-json/#optimizer-parameters">这里</a>。</p>
<p>如果在配置文件中不配置<code>optimizer</code>条目，[<code>Trainer</code>] 将自动将其设置为 <code>AdamW</code>，并使用提供的值或以下命令行参数的默认值：<code>--learning_rate</code>、<code>--adam_beta1</code>、<code>--adam_beta2</code>、<code>--adam_epsilon</code> 和 <code>--weight_decay</code>。</p>
<p>以下是<code>AdamW</code> 的自动配置示例：</p>
<div class="highlight"><pre><span></span><code><span class="p">{</span>
<span class="w">   </span><span class="nt">&quot;optimizer&quot;</span><span class="p">:</span><span class="w"> </span><span class="p">{</span>
<span class="w">       </span><span class="nt">&quot;type&quot;</span><span class="p">:</span><span class="w"> </span><span class="s2">&quot;AdamW&quot;</span><span class="p">,</span>
<span class="w">       </span><span class="nt">&quot;params&quot;</span><span class="p">:</span><span class="w"> </span><span class="p">{</span>
<span class="w">         </span><span class="nt">&quot;lr&quot;</span><span class="p">:</span><span class="w"> </span><span class="s2">&quot;auto&quot;</span><span class="p">,</span>
<span class="w">         </span><span class="nt">&quot;betas&quot;</span><span class="p">:</span><span class="w"> </span><span class="s2">&quot;auto&quot;</span><span class="p">,</span>
<span class="w">         </span><span class="nt">&quot;eps&quot;</span><span class="p">:</span><span class="w"> </span><span class="s2">&quot;auto&quot;</span><span class="p">,</span>
<span class="w">         </span><span class="nt">&quot;weight_decay&quot;</span><span class="p">:</span><span class="w"> </span><span class="s2">&quot;auto&quot;</span>
<span class="w">       </span><span class="p">}</span>
<span class="w">   </span><span class="p">}</span>
<span class="p">}</span>
</code></pre></div>
<p>请注意，命令行参数将设置配置文件中的值。这是为了有一个明确的值来源，并避免在不同地方设置学习率等值时难以找到的错误。命令行参数配置高于其他。被覆盖的值包括：</p>
<ul>
<li><code>lr</code> 的值为 <code>--learning_rate</code></li>
<li><code>betas</code> 的值为 <code>--adam_beta1 --adam_beta2</code></li>
<li><code>eps</code> 的值为 <code>--adam_epsilon</code></li>
<li><code>weight_decay</code> 的值为 <code>--weight_decay</code></li>
</ul>
<p>因此，请记住在命令行上调整共享的超参数。</p>
<p>您也可以显式地设置这些值：</p>
<div class="highlight"><pre><span></span><code><span class="p">{</span>
<span class="w">   </span><span class="nt">&quot;optimizer&quot;</span><span class="p">:</span><span class="w"> </span><span class="p">{</span>
<span class="w">       </span><span class="nt">&quot;type&quot;</span><span class="p">:</span><span class="w"> </span><span class="s2">&quot;AdamW&quot;</span><span class="p">,</span>
<span class="w">       </span><span class="nt">&quot;params&quot;</span><span class="p">:</span><span class="w"> </span><span class="p">{</span>
<span class="w">         </span><span class="nt">&quot;lr&quot;</span><span class="p">:</span><span class="w"> </span><span class="mf">0.001</span><span class="p">,</span>
<span class="w">         </span><span class="nt">&quot;betas&quot;</span><span class="p">:</span><span class="w"> </span><span class="p">[</span><span class="mf">0.8</span><span class="p">,</span><span class="w"> </span><span class="mf">0.999</span><span class="p">],</span>
<span class="w">         </span><span class="nt">&quot;eps&quot;</span><span class="p">:</span><span class="w"> </span><span class="mf">1e-8</span><span class="p">,</span>
<span class="w">         </span><span class="nt">&quot;weight_decay&quot;</span><span class="p">:</span><span class="w"> </span><span class="mf">3e-7</span>
<span class="w">       </span><span class="p">}</span>
<span class="w">   </span><span class="p">}</span>
<span class="p">}</span>
</code></pre></div>
<p>但在这种情况下，您需要自己同步[<code>Trainer</code>]命令行参数和DeepSpeed配置。</p>
<p>如果您想使用上面未列出的其他优化器，您将不得不将其添加到顶层配置中。</p>
<div class="highlight"><pre><span></span><code><span class="p">{</span>
<span class="w">   </span><span class="nt">&quot;zero_allow_untested_optimizer&quot;</span><span class="p">:</span><span class="w"> </span><span class="kc">true</span>
<span class="p">}</span>
</code></pre></div>
<p>类似于 <code>AdamW</code>，您可以配置其他官方支持的优化器。只是记住这些可能有不同的配置值。例如，对于Adam，您可能需要将 <code>weight_decay</code> 设置在 <code>0.01</code> 左右。</p>
<p>此外，当与DeepSpeed的CPU Adam优化器一起使用时，offload的效果最好。如果您想在offload时使用不同的优化器，自 <code>deepspeed==0.8.3</code> 起，您还需要添加：</p>
<p><div class="highlight"><pre><span></span><code><span class="p">{</span>
<span class="w">   </span><span class="nt">&quot;zero_force_ds_cpu_optimizer&quot;</span><span class="p">:</span><span class="w"> </span><span class="kc">false</span>
<span class="p">}</span>
</code></pre></div>
到顶层配置中。</p>
<p><a id='deepspeed-scheduler'></a></p>
<h4 id="scheduler">Scheduler<a class="headerlink" href="#scheduler" title="Permanent link">⚓︎</a></h4>
<p>DeepSpeed支持<code>LRRangeTest</code>、<code>OneCycle</code>、<code>WarmupLR</code>和<code>WarmupDecayLR</code>学习率调度器。完整文档在<a href="https://www.deepspeed.ai/docs/config-json/#scheduler-parameters">这里</a>。</p>
<p>以下是🤗 Transformers 和 DeepSpeed 之间的调度器重叠部分：</p>
<ul>
<li>通过 <code>--lr_scheduler_type constant_with_warmup</code> 实现 <code>WarmupLR</code></li>
<li>通过 <code>--lr_scheduler_type linear</code> 实现 <code>WarmupDecayLR</code>。这也是 <code>--lr_scheduler_type</code> 的默认值，因此，如果不配置调度器，这将是默认配置的调度器。</li>
</ul>
<p>如果在配置文件中不配置 <code>scheduler</code> 条目，[<code>Trainer</code>] 将使用 <code>--lr_scheduler_type</code>、<code>--learning_rate</code> 和 <code>--warmup_steps</code> 或 <code>--warmup_ratio</code> 的值来配置其🤗 Transformers 版本。</p>
<p>以下是 <code>WarmupLR</code> 的自动配置示例：</p>
<div class="highlight"><pre><span></span><code><span class="p">{</span>
<span class="w">   </span><span class="nt">&quot;scheduler&quot;</span><span class="p">:</span><span class="w"> </span><span class="p">{</span>
<span class="w">         </span><span class="nt">&quot;type&quot;</span><span class="p">:</span><span class="w"> </span><span class="s2">&quot;WarmupLR&quot;</span><span class="p">,</span>
<span class="w">         </span><span class="nt">&quot;params&quot;</span><span class="p">:</span><span class="w"> </span><span class="p">{</span>
<span class="w">             </span><span class="nt">&quot;warmup_min_lr&quot;</span><span class="p">:</span><span class="w"> </span><span class="s2">&quot;auto&quot;</span><span class="p">,</span>
<span class="w">             </span><span class="nt">&quot;warmup_max_lr&quot;</span><span class="p">:</span><span class="w"> </span><span class="s2">&quot;auto&quot;</span><span class="p">,</span>
<span class="w">             </span><span class="nt">&quot;warmup_num_steps&quot;</span><span class="p">:</span><span class="w"> </span><span class="s2">&quot;auto&quot;</span>
<span class="w">         </span><span class="p">}</span>
<span class="w">     </span><span class="p">}</span>
<span class="p">}</span>
</code></pre></div>
<p>由于使用了 <em>"auto"</em>，[<code>Trainer</code>] 的参数将在配置文件中设置正确的值。这是为了有一个明确的值来源，并避免在不同地方设置学习率等值时难以找到的错误。命令行配置高于其他。被设置的值包括：</p>
<ul>
<li><code>warmup_min_lr</code> 的值为 <code>0</code>。</li>
<li><code>warmup_max_lr</code> 的值为 <code>--learning_rate</code>。</li>
<li><code>warmup_num_steps</code> 的值为 <code>--warmup_steps</code>（如果提供）。否则，将使用 <code>--warmup_ratio</code> 乘以训练步骤的数量，并四舍五入。</li>
<li><code>total_num_steps</code> 的值为 <code>--max_steps</code> 或者如果没有提供，将在运行时根据环境、数据集的大小和其他命令行参数（对于 <code>WarmupDecayLR</code> 来说需要）自动推导。</li>
</ul>
<p>当然，您可以接管任何或所有的配置值，并自行设置这些值：</p>
<div class="highlight"><pre><span></span><code><span class="p">{</span>
<span class="w">   </span><span class="nt">&quot;scheduler&quot;</span><span class="p">:</span><span class="w"> </span><span class="p">{</span>
<span class="w">         </span><span class="nt">&quot;type&quot;</span><span class="p">:</span><span class="w"> </span><span class="s2">&quot;WarmupLR&quot;</span><span class="p">,</span>
<span class="w">         </span><span class="nt">&quot;params&quot;</span><span class="p">:</span><span class="w"> </span><span class="p">{</span>
<span class="w">             </span><span class="nt">&quot;warmup_min_lr&quot;</span><span class="p">:</span><span class="w"> </span><span class="mi">0</span><span class="p">,</span>
<span class="w">             </span><span class="nt">&quot;warmup_max_lr&quot;</span><span class="p">:</span><span class="w"> </span><span class="mf">0.001</span><span class="p">,</span>
<span class="w">             </span><span class="nt">&quot;warmup_num_steps&quot;</span><span class="p">:</span><span class="w"> </span><span class="mi">1000</span>
<span class="w">         </span><span class="p">}</span>
<span class="w">     </span><span class="p">}</span>
<span class="p">}</span>
</code></pre></div>
<p>但在这种情况下，您需要自己同步[<code>Trainer</code>]命令行参数和DeepSpeed配置。</p>
<p>例如，对于 <code>WarmupDecayLR</code>，您可以使用以下条目：</p>
<div class="highlight"><pre><span></span><code><span class="p">{</span>
<span class="w">   </span><span class="nt">&quot;scheduler&quot;</span><span class="p">:</span><span class="w"> </span><span class="p">{</span>
<span class="w">         </span><span class="nt">&quot;type&quot;</span><span class="p">:</span><span class="w"> </span><span class="s2">&quot;WarmupDecayLR&quot;</span><span class="p">,</span>
<span class="w">         </span><span class="nt">&quot;params&quot;</span><span class="p">:</span><span class="w"> </span><span class="p">{</span>
<span class="w">             </span><span class="nt">&quot;last_batch_iteration&quot;</span><span class="p">:</span><span class="w"> </span><span class="mi">-1</span><span class="p">,</span>
<span class="w">             </span><span class="nt">&quot;total_num_steps&quot;</span><span class="p">:</span><span class="w"> </span><span class="s2">&quot;auto&quot;</span><span class="p">,</span>
<span class="w">             </span><span class="nt">&quot;warmup_min_lr&quot;</span><span class="p">:</span><span class="w"> </span><span class="s2">&quot;auto&quot;</span><span class="p">,</span>
<span class="w">             </span><span class="nt">&quot;warmup_max_lr&quot;</span><span class="p">:</span><span class="w"> </span><span class="s2">&quot;auto&quot;</span><span class="p">,</span>
<span class="w">             </span><span class="nt">&quot;warmup_num_steps&quot;</span><span class="p">:</span><span class="w"> </span><span class="s2">&quot;auto&quot;</span>
<span class="w">         </span><span class="p">}</span>
<span class="w">     </span><span class="p">}</span>
<span class="p">}</span>
</code></pre></div>
<p>然后，<code>total_num_steps</code>、<code>warmup_max_lr</code>、<code>warmup_num_steps</code> 和 <code>total_num_steps</code> 将在加载时设置。</p>
<p><a id='deepspeed-fp32'></a></p>
<h3 id="fp32">fp32精度<a class="headerlink" href="#fp32" title="Permanent link">⚓︎</a></h3>
<p>DeepSpeed支持完整的fp32和fp16混合精度。</p>
<p>由于fp16混合精度具有更小的内存需求和更快的速度，唯一不使用它的时候是当您使用的模型在这种训练模式下表现不佳时。通常，当模型没有在fp16混合精度下进行预训练时（例如，bf16预训练模型经常出现这种情况），会出现这种情况。这样的模型可能会发生溢出或下溢，导致 <code>NaN</code> 损失。如果是这种情况，那么您将希望使用完整的fp32模式，通过显式禁用默认启用的fp16混合精度模式：</p>
<div class="highlight"><pre><span></span><code><span class="p">{</span>
<span class="w">    </span><span class="nt">&quot;fp16&quot;</span><span class="p">:</span><span class="w"> </span><span class="p">{</span>
<span class="w">        </span><span class="nt">&quot;enabled&quot;</span><span class="p">:</span><span class="w"> </span><span class="kc">false</span><span class="p">,</span>
<span class="w">    </span><span class="p">}</span>
<span class="p">}</span>
</code></pre></div>
<p>如果您使用基于Ampere架构的GPU，PyTorch版本1.7及更高版本将自动切换到使用更高效的tf32格式进行一些操作，但结果仍将以fp32格式呈现。有关详细信息和基准测试，请参见<a href="https://pytorch.org/docs/stable/notes/cuda.html#tensorfloat-32-tf32-on-ampere-devices">TensorFloat-32(TF32) on Ampere devices</a>。如果出于某种原因您不希望使用它，该文档包括有关如何禁用此自动转换的说明。</p>
<p>在🤗 Trainer中，你可以使用 <code>--tf32</code> 来启用它，或使用 <code>--tf32 0</code> 或 <code>--no_tf32</code> 来禁用它。默认情况下，使用PyTorch的默认设置。</p>
<p><a id='deepspeed-amp'></a></p>
<h3 id="_6">自动混合精度<a class="headerlink" href="#_6" title="Permanent link">⚓︎</a></h3>
<p>您可以使用自动混合精度，可以选择使用类似 PyTorch AMP 的方式，也可以选择使用类似 Apex 的方式：</p>
<h3 id="fp16">fp16<a class="headerlink" href="#fp16" title="Permanent link">⚓︎</a></h3>
<p>要配置PyTorch AMP-like 的 fp16（float16） 模式，请设置：</p>
<div class="highlight"><pre><span></span><code><span class="p">{</span>
<span class="w">    </span><span class="nt">&quot;fp16&quot;</span><span class="p">:</span><span class="w"> </span><span class="p">{</span>
<span class="w">        </span><span class="nt">&quot;enabled&quot;</span><span class="p">:</span><span class="w"> </span><span class="s2">&quot;auto&quot;</span><span class="p">,</span>
<span class="w">        </span><span class="nt">&quot;loss_scale&quot;</span><span class="p">:</span><span class="w"> </span><span class="mi">0</span><span class="p">,</span>
<span class="w">        </span><span class="nt">&quot;loss_scale_window&quot;</span><span class="p">:</span><span class="w"> </span><span class="mi">1000</span><span class="p">,</span>
<span class="w">        </span><span class="nt">&quot;initial_scale_power&quot;</span><span class="p">:</span><span class="w"> </span><span class="mi">16</span><span class="p">,</span>
<span class="w">        </span><span class="nt">&quot;hysteresis&quot;</span><span class="p">:</span><span class="w"> </span><span class="mi">2</span><span class="p">,</span>
<span class="w">        </span><span class="nt">&quot;min_loss_scale&quot;</span><span class="p">:</span><span class="w"> </span><span class="mi">1</span>
<span class="w">    </span><span class="p">}</span>
<span class="p">}</span>
</code></pre></div>
<p>并且，[<code>Trainer</code>]将根据<code>args.fp16_backend</code>的值自动启用或禁用它。其余的配置值由您决定。</p>
<p>当传递<code>--fp16 --fp16_backend amp</code>或<code>--fp16_full_eval</code>命令行参数时，此模式将被启用。</p>
<p>您也可以显式地启用/禁用此模式：</p>
<div class="highlight"><pre><span></span><code><span class="p">{</span>
<span class="w">    </span><span class="nt">&quot;fp16&quot;</span><span class="p">:</span><span class="w"> </span><span class="p">{</span>
<span class="w">        </span><span class="nt">&quot;enabled&quot;</span><span class="p">:</span><span class="w"> </span><span class="kc">true</span><span class="p">,</span>
<span class="w">        </span><span class="nt">&quot;loss_scale&quot;</span><span class="p">:</span><span class="w"> </span><span class="mi">0</span><span class="p">,</span>
<span class="w">        </span><span class="nt">&quot;loss_scale_window&quot;</span><span class="p">:</span><span class="w"> </span><span class="mi">1000</span><span class="p">,</span>
<span class="w">        </span><span class="nt">&quot;initial_scale_power&quot;</span><span class="p">:</span><span class="w"> </span><span class="mi">16</span><span class="p">,</span>
<span class="w">        </span><span class="nt">&quot;hysteresis&quot;</span><span class="p">:</span><span class="w"> </span><span class="mi">2</span><span class="p">,</span>
<span class="w">        </span><span class="nt">&quot;min_loss_scale&quot;</span><span class="p">:</span><span class="w"> </span><span class="mi">1</span>
<span class="w">    </span><span class="p">}</span>
<span class="p">}</span>
</code></pre></div>
<p>但是之后您需要自己同步[<code>Trainer</code>]命令行参数和DeepSpeed配置。</p>
<p>以下是<a href="https://www.deepspeed.ai/docs/config-json/#fp16-training-options">相关文档</a></p>
<h3 id="bf16">bf16<a class="headerlink" href="#bf16" title="Permanent link">⚓︎</a></h3>
<p>如果需要使用bfloat16而不是fp16，那么可以使用以下配置部分：</p>
<div class="highlight"><pre><span></span><code><span class="p">{</span>
<span class="w">    </span><span class="nt">&quot;bf16&quot;</span><span class="p">:</span><span class="w"> </span><span class="p">{</span>
<span class="w">        </span><span class="nt">&quot;enabled&quot;</span><span class="p">:</span><span class="w"> </span><span class="s2">&quot;auto&quot;</span>
<span class="w">    </span><span class="p">}</span>
<span class="p">}</span>
</code></pre></div>
<p>bf16具有与fp32相同的动态范围，因此不需要损失缩放。</p>
<p>当传递<code>--bf16</code>或<code>--bf16_full_eval</code>命令行参数时，启用此模式。</p>
<p>您还可以显式地启用/禁用此模式：</p>
<div class="highlight"><pre><span></span><code><span class="p">{</span>
<span class="w">    </span><span class="nt">&quot;bf16&quot;</span><span class="p">:</span><span class="w"> </span><span class="p">{</span>
<span class="w">        </span><span class="nt">&quot;enabled&quot;</span><span class="p">:</span><span class="w"> </span><span class="kc">true</span>
<span class="w">    </span><span class="p">}</span>
<span class="p">}</span>
</code></pre></div>
<p><Tip></p>
<p>在<code>deepspeed==0.6.0</code>版本中，bf16支持是新的实验性功能。</p>
<p>如果您启用了bf16来进行<a href="#gradient-accumulation">梯度累积</a>，您需要意识到它会以bf16累积梯度，这可能不是您想要的，因为这种格式的低精度可能会导致lossy accumulation。</p>
<p>修复这个问题的工作正在努力进行，同时提供了使用更高精度的<code>dtype</code>（fp16或fp32）的选项。</p>
<p></Tip></p>
<h3 id="nccl">NCCL集合<a class="headerlink" href="#nccl" title="Permanent link">⚓︎</a></h3>
<p>在训练过程中，有两种数据类型：<code>dtype</code>和用于通信收集操作的<code>dtype</code>，如各种归约和收集/分散操作。</p>
<p>所有的gather/scatter操作都是在数据相同的<code>dtype</code>中执行的，所以如果您正在使用bf16的训练模式，那么它将在bf16中进行gather操作 - gather操作是非损失性的。</p>
<p>各种reduce操作可能会是非常损失性的，例如当梯度在多个gpu上平均时，如果通信是在fp16或bf16中进行的，那么结果可能是有损失性的 - 因为当在一个低精度中添加多个数字时，结果可能不是精确的。更糟糕的是，bf16比fp16具有更低的精度。通常，当平均梯度时，损失最小，这些梯度通常非常小。因此，对于半精度训练，默认情况下，fp16被用作reduction操作的默认值。但是，您可以完全控制这个功能，如果你选择的话，您可以添加一个小的开销，并确保reductions将使用fp32作为累积数据类型，只有当结果准备好时，它才会降级到您在训练中使用的半精度<code>dtype</code>。</p>
<p>要覆盖默认设置，您只需添加一个新的配置条目：</p>
<div class="highlight"><pre><span></span><code><span class="p">{</span>
<span class="w">    </span><span class="nt">&quot;communication_data_type&quot;</span><span class="p">:</span><span class="w"> </span><span class="s2">&quot;fp32&quot;</span>
<span class="p">}</span>
</code></pre></div>
<p>根据这个信息，有效的值包括"fp16"、"bfp16"和"fp32"。</p>
<p>注意：在stage zero 3中，bf16通信数据类型存在一个bug，该问题已在<code>deepspeed==0.8.1</code>版本中得到修复。</p>
<h3 id="apex">apex<a class="headerlink" href="#apex" title="Permanent link">⚓︎</a></h3>
<p>配置apex AMP-like模式：</p>
<div class="highlight"><pre><span></span><code><span class="nt">&quot;amp&quot;</span><span class="p">:</span><span class="w"> </span><span class="p">{</span>
<span class="w">    </span><span class="nt">&quot;enabled&quot;</span><span class="p">:</span><span class="w"> </span><span class="s2">&quot;auto&quot;</span><span class="p">,</span>
<span class="w">    </span><span class="nt">&quot;opt_level&quot;</span><span class="p">:</span><span class="w"> </span><span class="s2">&quot;auto&quot;</span>
<span class="p">}</span>
</code></pre></div>
<p>并且，[<code>Trainer</code>]将根据<code>args.fp16_backend</code>和<code>args.fp16_opt_level</code>的值自动配置它。</p>
<p>当传递<code>--fp16 --fp16_backend apex --fp16_opt_level 01</code>命令行参数时，此模式将被启用。</p>
<p>您还可以显式配置此模式：</p>
<div class="highlight"><pre><span></span><code><span class="p">{</span>
<span class="w">    </span><span class="nt">&quot;amp&quot;</span><span class="p">:</span><span class="w"> </span><span class="p">{</span>
<span class="w">        </span><span class="nt">&quot;enabled&quot;</span><span class="p">:</span><span class="w"> </span><span class="kc">true</span><span class="p">,</span>
<span class="w">        </span><span class="nt">&quot;opt_level&quot;</span><span class="p">:</span><span class="w"> </span><span class="s2">&quot;O1&quot;</span>
<span class="w">    </span><span class="p">}</span>
<span class="p">}</span>
</code></pre></div>
<p>但是，您需要自己同步[<code>Trainer</code>]命令行参数和DeepSpeed配置。</p>
<p>这里是<a href="https://www.deepspeed.ai/docs/config-json/#automatic-mixed-precision-amp-training-options">文档</a></p>
<p><a id='deepspeed-bs'></a></p>
<h3 id="batch-size">Batch Size<a class="headerlink" href="#batch-size" title="Permanent link">⚓︎</a></h3>
<p>配置batch size可以使用如下参数:</p>
<div class="highlight"><pre><span></span><code><span class="p">{</span>
<span class="w">    </span><span class="nt">&quot;train_batch_size&quot;</span><span class="p">:</span><span class="w"> </span><span class="s2">&quot;auto&quot;</span><span class="p">,</span>
<span class="w">    </span><span class="nt">&quot;train_micro_batch_size_per_gpu&quot;</span><span class="p">:</span><span class="w"> </span><span class="s2">&quot;auto&quot;</span>
<span class="p">}</span>
</code></pre></div>
<p>并且，[<code>Trainer</code>]将自动将<code>train_micro_batch_size_per_gpu</code>设置为<code>args.per_device_train_batch_size</code>的值，并将<code>train_batch_size</code>设置为<code>args.world_size * args.per_device_train_batch_size * args.gradient_accumulation_steps</code>。</p>
<p>您也可以显式设置这些值：</p>
<div class="highlight"><pre><span></span><code><span class="p">{</span>
<span class="w">    </span><span class="nt">&quot;train_batch_size&quot;</span><span class="p">:</span><span class="w"> </span><span class="mi">12</span><span class="p">,</span>
<span class="w">    </span><span class="nt">&quot;train_micro_batch_size_per_gpu&quot;</span><span class="p">:</span><span class="w"> </span><span class="mi">4</span>
<span class="p">}</span>
</code></pre></div>
<p>但是，您需要自己同步[<code>Trainer</code>]命令行参数和DeepSpeed配置。</p>
<p><a id='deepspeed-grad-acc'></a></p>
<h3 id="gradient-accumulation">Gradient Accumulation<a class="headerlink" href="#gradient-accumulation" title="Permanent link">⚓︎</a></h3>
<p>配置gradient accumulation设置如下:</p>
<div class="highlight"><pre><span></span><code><span class="p">{</span>
<span class="w">    </span><span class="nt">&quot;gradient_accumulation_steps&quot;</span><span class="p">:</span><span class="w"> </span><span class="s2">&quot;auto&quot;</span>
<span class="p">}</span>
</code></pre></div>
<p>并且，[<code>Trainer</code>]将自动将其设置为<code>args.gradient_accumulation_steps</code>的值。</p>
<p>您也可以显式设置这个值：</p>
<div class="highlight"><pre><span></span><code><span class="p">{</span>
<span class="w">    </span><span class="nt">&quot;gradient_accumulation_steps&quot;</span><span class="p">:</span><span class="w"> </span><span class="mi">3</span>
<span class="p">}</span>
</code></pre></div>
<p>但是，您需要自己同步[<code>Trainer</code>]命令行参数和DeepSpeed配置。</p>
<p><a id='deepspeed-grad-clip'></a></p>
<h3 id="gradient-clipping">Gradient Clipping<a class="headerlink" href="#gradient-clipping" title="Permanent link">⚓︎</a></h3>
<p>配置gradient clipping如下:</p>
<div class="highlight"><pre><span></span><code><span class="p">{</span>
<span class="w">    </span><span class="nt">&quot;gradient_clipping&quot;</span><span class="p">:</span><span class="w"> </span><span class="s2">&quot;auto&quot;</span>
<span class="p">}</span>
</code></pre></div>
<p>并且，[<code>Trainer</code>]将自动将其设置为<code>args.max_grad_norm</code>的值。</p>
<p>您也可以显式设置这个值：</p>
<div class="highlight"><pre><span></span><code><span class="p">{</span>
<span class="w">    </span><span class="nt">&quot;gradient_clipping&quot;</span><span class="p">:</span><span class="w"> </span><span class="mf">1.0</span>
<span class="p">}</span>
</code></pre></div>
<p>但是，您需要自己同步[<code>Trainer</code>]命令行参数和DeepSpeed配置。</p>
<p><a id='deepspeed-weight-extraction'></a></p>
<h3 id="_7">获取模型权重<a class="headerlink" href="#_7" title="Permanent link">⚓︎</a></h3>
<p>只要您继续使用DeepSpeed进行训练和恢复，您就不需要担心任何事情。DeepSpeed在其自定义检查点优化器文件中存储fp32主权重，这些文件是<code>global_step*/*optim_states.pt</code>（这是glob模式），并保存在正常的checkpoint下。</p>
<p><strong>FP16权重：</strong></p>
<p>当模型保存在ZeRO-2下时，您最终会得到一个包含模型权重的普通<code>pytorch_model.bin</code>文件，但它们只是权重的fp16版本。</p>
<p>在ZeRO-3下，事情要复杂得多，因为模型权重分布在多个GPU上，因此需要<code>"stage3_gather_16bit_weights_on_model_save": true</code>才能让<code>Trainer</code>保存fp16版本的权重。如果这个设置是<code>False</code>，<code>pytorch_model.bin</code>将不会被创建。这是因为默认情况下，DeepSpeed的<code>state_dict</code>包含一个占位符而不是实际的权重。如果我们保存这个<code>state_dict</code>，就无法再加载它了。</p>
<div class="highlight"><pre><span></span><code><span class="p">{</span>
<span class="w">    </span><span class="nt">&quot;zero_optimization&quot;</span><span class="p">:</span><span class="w"> </span><span class="p">{</span>
<span class="w">        </span><span class="nt">&quot;stage3_gather_16bit_weights_on_model_save&quot;</span><span class="p">:</span><span class="w"> </span><span class="kc">true</span>
<span class="w">    </span><span class="p">}</span>
<span class="p">}</span>
</code></pre></div>
<p><strong>FP32权重：</strong></p>
<p>虽然fp16权重适合恢复训练，但如果您完成了模型的微调并希望将其上传到<a href="https://huggingface.co/models">models hub</a>或传递给其他人，您很可能想要获取fp32权重。这最好不要在训练期间完成，因为这需要大量内存，因此最好在训练完成后离线进行。但是，如果需要并且有充足的空闲CPU内存，可以在相同的训练脚本中完成。以下部分将讨论这两种方法。</p>
<p><strong>实时FP32权重恢复：</strong></p>
<p>如果您的模型很大，并且在训练结束时几乎没有剩余的空闲CPU内存，这种方法可能不起作用。</p>
<p>如果您至少保存了一个检查点，并且想要使用最新的一个，可以按照以下步骤操作：</p>
<div class="highlight"><pre><span></span><code><span class="kn">from</span> <span class="nn">transformers.trainer_utils</span> <span class="kn">import</span> <span class="n">get_last_checkpoint</span>
<span class="kn">from</span> <span class="nn">deepspeed.utils.zero_to_fp32</span> <span class="kn">import</span> <span class="n">load_state_dict_from_zero_checkpoint</span>

<span class="n">checkpoint_dir</span> <span class="o">=</span> <span class="n">get_last_checkpoint</span><span class="p">(</span><span class="n">trainer</span><span class="o">.</span><span class="n">args</span><span class="o">.</span><span class="n">output_dir</span><span class="p">)</span>
<span class="n">fp32_model</span> <span class="o">=</span> <span class="n">load_state_dict_from_zero_checkpoint</span><span class="p">(</span><span class="n">trainer</span><span class="o">.</span><span class="n">model</span><span class="p">,</span> <span class="n">checkpoint_dir</span><span class="p">)</span>
</code></pre></div>
<p>如果您在使用<code>--load_best_model_at_end</code>类：<em>~transformers.TrainingArguments</em>参数（用于跟踪最佳
检查点），那么你可以首先显式地保存最终模型，然后再执行相同的操作：</p>
<div class="highlight"><pre><span></span><code><span class="kn">from</span> <span class="nn">deepspeed.utils.zero_to_fp32</span> <span class="kn">import</span> <span class="n">load_state_dict_from_zero_checkpoint</span>

<span class="n">checkpoint_dir</span> <span class="o">=</span> <span class="n">os</span><span class="o">.</span><span class="n">path</span><span class="o">.</span><span class="n">join</span><span class="p">(</span><span class="n">trainer</span><span class="o">.</span><span class="n">args</span><span class="o">.</span><span class="n">output_dir</span><span class="p">,</span> <span class="s2">&quot;checkpoint-final&quot;</span><span class="p">)</span>
<span class="n">trainer</span><span class="o">.</span><span class="n">deepspeed</span><span class="o">.</span><span class="n">save_checkpoint</span><span class="p">(</span><span class="n">checkpoint_dir</span><span class="p">)</span>
<span class="n">fp32_model</span> <span class="o">=</span> <span class="n">load_state_dict_from_zero_checkpoint</span><span class="p">(</span><span class="n">trainer</span><span class="o">.</span><span class="n">model</span><span class="p">,</span> <span class="n">checkpoint_dir</span><span class="p">)</span>
</code></pre></div>
<p><Tip></p>
<p>注意，一旦运行了<code>load_state_dict_from_zero_checkpoint</code>，该模型将不再可以在相同的应用程序的DeepSpeed上下文中使用。也就是说，您需要重新初始化deepspeed引擎，因为<code>model.load_state_dict(state_dict)</code>会从其中移除所有的DeepSpeed相关点。所以您只能训练结束时这样做。</p>
<p></Tip></p>
<p>当然，您不必使用类：<em>~transformers.Trainer</em>，您可以根据你的需求调整上面的示例。</p>
<p>如果您出于某种原因想要更多的优化，您也可以提取权重的fp32 <code>state_dict</code>并按照以下示例进行操作：</p>
<div class="highlight"><pre><span></span><code><span class="kn">from</span> <span class="nn">deepspeed.utils.zero_to_fp32</span> <span class="kn">import</span> <span class="n">get_fp32_state_dict_from_zero_checkpoint</span>

<span class="n">state_dict</span> <span class="o">=</span> <span class="n">get_fp32_state_dict_from_zero_checkpoint</span><span class="p">(</span><span class="n">checkpoint_dir</span><span class="p">)</span>  <span class="c1"># already on cpu</span>
<span class="n">model</span> <span class="o">=</span> <span class="n">model</span><span class="o">.</span><span class="n">cpu</span><span class="p">()</span>
<span class="n">model</span><span class="o">.</span><span class="n">load_state_dict</span><span class="p">(</span><span class="n">state_dict</span><span class="p">)</span>
</code></pre></div>
<p><strong>离线FP32权重恢复：</strong></p>
<p>DeepSpeed会创建一个特殊的转换脚本<code>zero_to_fp32.py</code>，并将其放置在checkpoint文件夹的顶层。使用此脚本，您可以在任何时候提取权重。该脚本是独立的，您不再需要配置文件或<code>Trainer</code>来执行提取操作。</p>
<p>假设您的checkpoint文件夹如下所示：</p>
<div class="highlight"><pre><span></span><code>$<span class="w"> </span>ls<span class="w"> </span>-l<span class="w"> </span>output_dir/checkpoint-1/
-rw-rw-r--<span class="w"> </span><span class="m">1</span><span class="w"> </span>stas<span class="w"> </span>stas<span class="w"> </span><span class="m">1</span>.4K<span class="w"> </span>Mar<span class="w"> </span><span class="m">27</span><span class="w"> </span><span class="m">20</span>:42<span class="w"> </span>config.json
drwxrwxr-x<span class="w"> </span><span class="m">2</span><span class="w"> </span>stas<span class="w"> </span>stas<span class="w"> </span><span class="m">4</span>.0K<span class="w"> </span>Mar<span class="w"> </span><span class="m">25</span><span class="w"> </span><span class="m">19</span>:52<span class="w"> </span>global_step1/
-rw-rw-r--<span class="w"> </span><span class="m">1</span><span class="w"> </span>stas<span class="w"> </span>stas<span class="w">   </span><span class="m">12</span><span class="w"> </span>Mar<span class="w"> </span><span class="m">27</span><span class="w"> </span><span class="m">13</span>:16<span class="w"> </span>latest
-rw-rw-r--<span class="w"> </span><span class="m">1</span><span class="w"> </span>stas<span class="w"> </span>stas<span class="w"> </span>827K<span class="w"> </span>Mar<span class="w"> </span><span class="m">27</span><span class="w"> </span><span class="m">20</span>:42<span class="w"> </span>optimizer.pt
-rw-rw-r--<span class="w"> </span><span class="m">1</span><span class="w"> </span>stas<span class="w"> </span>stas<span class="w"> </span>231M<span class="w"> </span>Mar<span class="w"> </span><span class="m">27</span><span class="w"> </span><span class="m">20</span>:42<span class="w"> </span>pytorch_model.bin
-rw-rw-r--<span class="w"> </span><span class="m">1</span><span class="w"> </span>stas<span class="w"> </span>stas<span class="w">  </span><span class="m">623</span><span class="w"> </span>Mar<span class="w"> </span><span class="m">27</span><span class="w"> </span><span class="m">20</span>:42<span class="w"> </span>scheduler.pt
-rw-rw-r--<span class="w"> </span><span class="m">1</span><span class="w"> </span>stas<span class="w"> </span>stas<span class="w"> </span><span class="m">1</span>.8K<span class="w"> </span>Mar<span class="w"> </span><span class="m">27</span><span class="w"> </span><span class="m">20</span>:42<span class="w"> </span>special_tokens_map.json
-rw-rw-r--<span class="w"> </span><span class="m">1</span><span class="w"> </span>stas<span class="w"> </span>stas<span class="w"> </span>774K<span class="w"> </span>Mar<span class="w"> </span><span class="m">27</span><span class="w"> </span><span class="m">20</span>:42<span class="w"> </span>spiece.model
-rw-rw-r--<span class="w"> </span><span class="m">1</span><span class="w"> </span>stas<span class="w"> </span>stas<span class="w"> </span><span class="m">1</span>.9K<span class="w"> </span>Mar<span class="w"> </span><span class="m">27</span><span class="w"> </span><span class="m">20</span>:42<span class="w"> </span>tokenizer_config.json
-rw-rw-r--<span class="w"> </span><span class="m">1</span><span class="w"> </span>stas<span class="w"> </span>stas<span class="w">  </span><span class="m">339</span><span class="w"> </span>Mar<span class="w"> </span><span class="m">27</span><span class="w"> </span><span class="m">20</span>:42<span class="w"> </span>trainer_state.json
-rw-rw-r--<span class="w"> </span><span class="m">1</span><span class="w"> </span>stas<span class="w"> </span>stas<span class="w"> </span><span class="m">2</span>.3K<span class="w"> </span>Mar<span class="w"> </span><span class="m">27</span><span class="w"> </span><span class="m">20</span>:42<span class="w"> </span>training_args.bin
-rwxrw-r--<span class="w"> </span><span class="m">1</span><span class="w"> </span>stas<span class="w"> </span>stas<span class="w"> </span><span class="m">5</span>.5K<span class="w"> </span>Mar<span class="w"> </span><span class="m">27</span><span class="w"> </span><span class="m">13</span>:16<span class="w"> </span>zero_to_fp32.py*
</code></pre></div>
<p>在这个例子中，只有一个DeepSpeed检查点子文件夹<em>global_step1</em>。因此，要重构fp32权重，只需运行：</p>
<div class="highlight"><pre><span></span><code>python<span class="w"> </span>zero_to_fp32.py<span class="w"> </span>.<span class="w"> </span>pytorch_model.bin
</code></pre></div>
<p>这就是它。<code>pytorch_model.bin</code>现在将包含从多个GPUs合并的完整的fp32模型权重。</p>
<p>该脚本将自动能够处理ZeRO-2或ZeRO-3 checkpoint。</p>
<p><code>python zero_to_fp32.py -h</code>将为您提供使用细节。</p>
<p>该脚本将通过文件<code>latest</code>的内容自动发现deepspeed子文件夹，在当前示例中，它将包含<code>global_step1</code>。</p>
<p>注意：目前该脚本需要2倍于最终fp32模型权重的通用内存。</p>
<h3 id="zero-3-infinity-nuances">ZeRO-3 和 Infinity Nuances<a class="headerlink" href="#zero-3-infinity-nuances" title="Permanent link">⚓︎</a></h3>
<p>ZeRO-3与ZeRO-2有很大的不同，主要是因为它的参数分片功能。</p>
<p>ZeRO-Infinity进一步扩展了ZeRO-3，以支持NVMe内存和其他速度和可扩展性改进。</p>
<p>尽管所有努力都是为了在不需要对模型进行任何特殊更改的情况下就能正常运行，但在某些情况下，您可能需要以下信息。</p>
<h4 id="_8">构建大模型<a class="headerlink" href="#_8" title="Permanent link">⚓︎</a></h4>
<p>DeepSpeed/ZeRO-3可以处理参数量达到数万亿的模型，这些模型可能无法适应现有的内存。在这种情况下，如果您还是希望初始化更快地发生，可以使用<em>deepspeed.zero.Init()</em>上下文管理器（也是一个函数装饰器）来初始化模型，如下所示：</p>
<div class="highlight"><pre><span></span><code><span class="kn">from</span> <span class="nn">transformers</span> <span class="kn">import</span> <span class="n">T5ForConditionalGeneration</span><span class="p">,</span> <span class="n">T5Config</span>
<span class="kn">import</span> <span class="nn">deepspeed</span>

<span class="k">with</span> <span class="n">deepspeed</span><span class="o">.</span><span class="n">zero</span><span class="o">.</span><span class="n">Init</span><span class="p">():</span>
    <span class="n">config</span> <span class="o">=</span> <span class="n">T5Config</span><span class="o">.</span><span class="n">from_pretrained</span><span class="p">(</span><span class="s2">&quot;t5-small&quot;</span><span class="p">)</span>
    <span class="n">model</span> <span class="o">=</span> <span class="n">T5ForConditionalGeneration</span><span class="p">(</span><span class="n">config</span><span class="p">)</span>
</code></pre></div>
<p>如您所见，这会为您随机初始化一个模型。</p>
<p>如果您想使用预训练模型，<code>model_class.from_pretrained</code>将在<code>is_deepspeed_zero3_enabled()</code>返回<code>True</code>的情况下激活此功能，目前这是通过传递的DeepSpeed配置文件中的ZeRO-3配置部分设置的。因此，在调用<code>from_pretrained</code>之前，您必须创建<strong>TrainingArguments</strong>对象。以下是可能的顺序示例：</p>
<div class="highlight"><pre><span></span><code><span class="kn">from</span> <span class="nn">transformers</span> <span class="kn">import</span> <span class="n">AutoModel</span><span class="p">,</span> <span class="n">Trainer</span><span class="p">,</span> <span class="n">TrainingArguments</span>

<span class="n">training_args</span> <span class="o">=</span> <span class="n">TrainingArguments</span><span class="p">(</span><span class="o">...</span><span class="p">,</span> <span class="n">deepspeed</span><span class="o">=</span><span class="n">ds_config</span><span class="p">)</span>
<span class="n">model</span> <span class="o">=</span> <span class="n">AutoModel</span><span class="o">.</span><span class="n">from_pretrained</span><span class="p">(</span><span class="s2">&quot;t5-small&quot;</span><span class="p">)</span>
<span class="n">trainer</span> <span class="o">=</span> <span class="n">Trainer</span><span class="p">(</span><span class="n">model</span><span class="o">=</span><span class="n">model</span><span class="p">,</span> <span class="n">args</span><span class="o">=</span><span class="n">training_args</span><span class="p">,</span> <span class="o">...</span><span class="p">)</span>
</code></pre></div>
<p>如果您使用的是官方示例脚本，并且命令行参数中包含<code>--deepspeed ds_config.json</code>且启用了ZeRO-3配置，那么一切都已经为您准备好了，因为这是示例脚本的编写方式。</p>
<p>注意：如果模型的fp16权重无法适应单个GPU的内存，则必须使用此功能。</p>
<p>有关此方法和其他相关功能的完整详细信息，请参阅<a href="https://deepspeed.readthedocs.io/en/latest/zero3.html#constructing-massive-models">构建大模型</a>。</p>
<p>此外，在加载fp16预训练模型时，您希望<code>from_pretrained</code>使用<code>torch_dtype=torch.float16</code>。详情请参见<a href="#from_pretrained-torch-dtype">from_pretrained-torch-dtype</a>。</p>
<h4 id="_9">参数收集<a class="headerlink" href="#_9" title="Permanent link">⚓︎</a></h4>
<p>在多个GPU上使用ZeRO-3时，没有一个GPU拥有所有参数，除非它是当前执行层的参数。因此，如果您需要一次访问所有层的所有参数，有一个特定的方法可以实现。
您可能不需要它，但如果您需要，请参考<a href="https://deepspeed.readthedocs.io/en/latest/zero3.html#manual-parameter-coordination">参数收集</a>。</p>
<p>然而，我们在多个地方确实使用了它，其中一个例子是在<code>from_pretrained</code>中加载预训练模型权重。我们一次加载一层，然后立即将其分区到所有参与的GPU上，因为对于非常大的模型，无法在一个GPU上一次性加载并将其分布到多个GPU上，因为内存限制。</p>
<p>此外，在ZeRO-3下，如果您编写自己的代码并遇到看起来像这样的模型参数权重：</p>
<div class="highlight"><pre><span></span><code><span class="n">tensor</span><span class="p">([</span><span class="mf">1.0</span><span class="p">],</span> <span class="n">device</span><span class="o">=</span><span class="s2">&quot;cuda:0&quot;</span><span class="p">,</span> <span class="n">dtype</span><span class="o">=</span><span class="n">torch</span><span class="o">.</span><span class="n">float16</span><span class="p">,</span> <span class="n">requires_grad</span><span class="o">=</span><span class="kc">True</span><span class="p">)</span>
</code></pre></div>
<p>强调<code>tensor([1.])</code>，或者如果您遇到一个错误，它说参数的大小是<code>1</code>，而不是某个更大的多维形状，这意味着参数被划分了，你看到的是一个ZeRO-3占位符。</p>
<p><a id='deepspeed-zero-inference'></a></p>
<h3 id="zero_1">ZeRO 推理<a class="headerlink" href="#zero_1" title="Permanent link">⚓︎</a></h3>
<p>"ZeRO 推断" 使用与 "ZeRO-3 训练" 相同的配置。您只需要去掉优化器和调度器部分。实际上，如果您希望与训练共享相同的配置文件，您可以将它们保留在配置文件中，它们只会被忽略。</p>
<p>您只需要传递通常的[<code>TrainingArguments</code>]参数。例如：</p>
<div class="highlight"><pre><span></span><code>deepspeed<span class="w"> </span>--num_gpus<span class="o">=</span><span class="m">2</span><span class="w"> </span>your_program.py<span class="w"> </span>&lt;normal<span class="w"> </span>cl<span class="w"> </span>args&gt;<span class="w"> </span>--do_eval<span class="w"> </span>--deepspeed<span class="w"> </span>ds_config.json
</code></pre></div>
<p>唯一的重要事情是您需要使用ZeRO-3配置，因为ZeRO-2对于推理没有任何优势，因为只有ZeRO-3才对参数进行分片，而ZeRO-1则对梯度和优化器状态进行分片。</p>
<p>以下是在DeepSpeed下运行<code>run_translation.py</code>启用所有可用GPU的示例：</p>
<div class="highlight"><pre><span></span><code>deepspeed<span class="w"> </span>examples/pytorch/translation/run_translation.py<span class="w"> </span><span class="se">\</span>
--deepspeed<span class="w"> </span>tests/deepspeed/ds_config_zero3.json<span class="w"> </span><span class="se">\</span>
--model_name_or_path<span class="w"> </span>t5-small<span class="w"> </span>--output_dir<span class="w"> </span>output_dir<span class="w"> </span><span class="se">\</span>
--do_eval<span class="w"> </span>--max_eval_samples<span class="w"> </span><span class="m">50</span><span class="w"> </span>--warmup_steps<span class="w"> </span><span class="m">50</span><span class="w">  </span><span class="se">\</span>
--max_source_length<span class="w"> </span><span class="m">128</span><span class="w"> </span>--val_max_target_length<span class="w"> </span><span class="m">128</span><span class="w"> </span><span class="se">\</span>
--overwrite_output_dir<span class="w"> </span>--per_device_eval_batch_size<span class="w"> </span><span class="m">4</span><span class="w"> </span><span class="se">\</span>
--predict_with_generate<span class="w"> </span>--dataset_config<span class="w"> </span><span class="s2">&quot;ro-en&quot;</span><span class="w"> </span>--fp16<span class="w"> </span><span class="se">\</span>
--source_lang<span class="w"> </span>en<span class="w"> </span>--target_lang<span class="w"> </span>ro<span class="w"> </span>--dataset_name<span class="w"> </span>wmt16<span class="w"> </span><span class="se">\</span>
--source_prefix<span class="w"> </span><span class="s2">&quot;translate English to Romanian: &quot;</span>
</code></pre></div>
<p>由于在推理阶段，优化器状态和梯度不需要额外的大量内存，您应该能够将更大的批次和/或序列长度放到相同的硬件上。</p>
<p>此外，DeepSpeed目前正在开发一个名为Deepspeed-Inference的相关产品，它与ZeRO技术无关，而是使用张量并行来扩展无法适应单个GPU的模型。这是一个正在进行的工作，一旦该产品完成，我们将提供集成。</p>
<h3 id="_10">内存要求<a class="headerlink" href="#_10" title="Permanent link">⚓︎</a></h3>
<p>由于 DeepSpeed ZeRO 可以将内存卸载到 CPU（和 NVMe），该框架提供了一些工具，允许根据使用的 GPU 数量告知将需要多少 CPU 和 GPU 内存。</p>
<p>让我们估计在单个GPU上微调"bigscience/T0_3B"所需的内存：</p>
<div class="highlight"><pre><span></span><code>$<span class="w"> </span>python<span class="w"> </span>-c<span class="w"> </span><span class="s1">&#39;from transformers import AutoModel; \</span>
<span class="s1">from deepspeed.runtime.zero.stage3 import estimate_zero3_model_states_mem_needs_all_live; \</span>
<span class="s1">model = AutoModel.from_pretrained(&quot;bigscience/T0_3B&quot;); \</span>
<span class="s1">estimate_zero3_model_states_mem_needs_all_live(model, num_gpus_per_node=1, num_nodes=1)&#39;</span>
<span class="o">[</span>...<span class="o">]</span>
Estimated<span class="w"> </span>memory<span class="w"> </span>needed<span class="w"> </span><span class="k">for</span><span class="w"> </span>params,<span class="w"> </span>optim<span class="w"> </span>states<span class="w"> </span>and<span class="w"> </span>gradients<span class="w"> </span><span class="k">for</span><span class="w"> </span>a:
HW:<span class="w"> </span>Setup<span class="w"> </span>with<span class="w"> </span><span class="m">1</span><span class="w"> </span>node,<span class="w"> </span><span class="m">1</span><span class="w"> </span>GPU<span class="w"> </span>per<span class="w"> </span>node.
SW:<span class="w"> </span>Model<span class="w"> </span>with<span class="w"> </span>2783M<span class="w"> </span>total<span class="w"> </span>params,<span class="w"> </span>65M<span class="w"> </span>largest<span class="w"> </span>layer<span class="w"> </span>params.
<span class="w">  </span>per<span class="w"> </span>CPU<span class="w">  </span><span class="p">|</span><span class="w">  </span>per<span class="w"> </span>GPU<span class="w"> </span><span class="p">|</span><span class="w">   </span>Options
<span class="w">   </span><span class="m">70</span>.00GB<span class="w"> </span><span class="p">|</span><span class="w">   </span><span class="m">0</span>.25GB<span class="w"> </span><span class="p">|</span><span class="w"> </span><span class="nv">offload_param</span><span class="o">=</span>cpu<span class="w"> </span>,<span class="w"> </span><span class="nv">offload_optimizer</span><span class="o">=</span>cpu<span class="w"> </span>,<span class="w"> </span><span class="nv">zero_init</span><span class="o">=</span><span class="m">1</span>
<span class="w">   </span><span class="m">70</span>.00GB<span class="w"> </span><span class="p">|</span><span class="w">   </span><span class="m">0</span>.25GB<span class="w"> </span><span class="p">|</span><span class="w"> </span><span class="nv">offload_param</span><span class="o">=</span>cpu<span class="w"> </span>,<span class="w"> </span><span class="nv">offload_optimizer</span><span class="o">=</span>cpu<span class="w"> </span>,<span class="w"> </span><span class="nv">zero_init</span><span class="o">=</span><span class="m">0</span>
<span class="w">   </span><span class="m">62</span>.23GB<span class="w"> </span><span class="p">|</span><span class="w">   </span><span class="m">5</span>.43GB<span class="w"> </span><span class="p">|</span><span class="w"> </span><span class="nv">offload_param</span><span class="o">=</span>none,<span class="w"> </span><span class="nv">offload_optimizer</span><span class="o">=</span>cpu<span class="w"> </span>,<span class="w"> </span><span class="nv">zero_init</span><span class="o">=</span><span class="m">1</span>
<span class="w">   </span><span class="m">62</span>.23GB<span class="w"> </span><span class="p">|</span><span class="w">   </span><span class="m">5</span>.43GB<span class="w"> </span><span class="p">|</span><span class="w"> </span><span class="nv">offload_param</span><span class="o">=</span>none,<span class="w"> </span><span class="nv">offload_optimizer</span><span class="o">=</span>cpu<span class="w"> </span>,<span class="w"> </span><span class="nv">zero_init</span><span class="o">=</span><span class="m">0</span>
<span class="w">    </span><span class="m">0</span>.37GB<span class="w"> </span><span class="p">|</span><span class="w">  </span><span class="m">46</span>.91GB<span class="w"> </span><span class="p">|</span><span class="w"> </span><span class="nv">offload_param</span><span class="o">=</span>none,<span class="w"> </span><span class="nv">offload_optimizer</span><span class="o">=</span>none,<span class="w"> </span><span class="nv">zero_init</span><span class="o">=</span><span class="m">1</span>
<span class="w">   </span><span class="m">15</span>.56GB<span class="w"> </span><span class="p">|</span><span class="w">  </span><span class="m">46</span>.91GB<span class="w"> </span><span class="p">|</span><span class="w"> </span><span class="nv">offload_param</span><span class="o">=</span>none,<span class="w"> </span><span class="nv">offload_optimizer</span><span class="o">=</span>none,<span class="w"> </span><span class="nv">zero_init</span><span class="o">=</span><span class="m">0</span>
</code></pre></div>
<p>因此，您可以将模型拟合在单个80GB的GPU上，不进行CPU offload，或者使用微小的8GB GPU，但需要约60GB的CPU内存。（请注意，这仅是参数、优化器状态和梯度所需的内存 - 您还需要为CUDA内核、激活值和临时变量分配更多的内存。）</p>
<p>然后，这是成本与速度的权衡。购买/租用较小的 GPU（或较少的 GPU，因为您可以使用多个 GPU 进行 Deepspeed ZeRO）。但这样会更慢，因此即使您不关心完成某项任务的速度，减速也直接影响 GPU 使用的持续时间，从而导致更大的成本。因此，请进行实验并比较哪种方法效果最好。</p>
<p>如果您有足够的GPU内存，请确保禁用CPU/NVMe卸载，因为这会使所有操作更快。</p>
<p>例如，让我们重复相同的操作，使用2个GPU：</p>
<div class="highlight"><pre><span></span><code>$<span class="w"> </span>python<span class="w"> </span>-c<span class="w"> </span><span class="s1">&#39;from transformers import AutoModel; \</span>
<span class="s1">from deepspeed.runtime.zero.stage3 import estimate_zero3_model_states_mem_needs_all_live; \</span>
<span class="s1">model = AutoModel.from_pretrained(&quot;bigscience/T0_3B&quot;); \</span>
<span class="s1">estimate_zero3_model_states_mem_needs_all_live(model, num_gpus_per_node=2, num_nodes=1)&#39;</span>
<span class="o">[</span>...<span class="o">]</span>
Estimated<span class="w"> </span>memory<span class="w"> </span>needed<span class="w"> </span><span class="k">for</span><span class="w"> </span>params,<span class="w"> </span>optim<span class="w"> </span>states<span class="w"> </span>and<span class="w"> </span>gradients<span class="w"> </span><span class="k">for</span><span class="w"> </span>a:
HW:<span class="w"> </span>Setup<span class="w"> </span>with<span class="w"> </span><span class="m">1</span><span class="w"> </span>node,<span class="w"> </span><span class="m">2</span><span class="w"> </span>GPUs<span class="w"> </span>per<span class="w"> </span>node.
SW:<span class="w"> </span>Model<span class="w"> </span>with<span class="w"> </span>2783M<span class="w"> </span>total<span class="w"> </span>params,<span class="w"> </span>65M<span class="w"> </span>largest<span class="w"> </span>layer<span class="w"> </span>params.
<span class="w">  </span>per<span class="w"> </span>CPU<span class="w">  </span><span class="p">|</span><span class="w">  </span>per<span class="w"> </span>GPU<span class="w"> </span><span class="p">|</span><span class="w">   </span>Options
<span class="w">   </span><span class="m">70</span>.00GB<span class="w"> </span><span class="p">|</span><span class="w">   </span><span class="m">0</span>.25GB<span class="w"> </span><span class="p">|</span><span class="w"> </span><span class="nv">offload_param</span><span class="o">=</span>cpu<span class="w"> </span>,<span class="w"> </span><span class="nv">offload_optimizer</span><span class="o">=</span>cpu<span class="w"> </span>,<span class="w"> </span><span class="nv">zero_init</span><span class="o">=</span><span class="m">1</span>
<span class="w">   </span><span class="m">70</span>.00GB<span class="w"> </span><span class="p">|</span><span class="w">   </span><span class="m">0</span>.25GB<span class="w"> </span><span class="p">|</span><span class="w"> </span><span class="nv">offload_param</span><span class="o">=</span>cpu<span class="w"> </span>,<span class="w"> </span><span class="nv">offload_optimizer</span><span class="o">=</span>cpu<span class="w"> </span>,<span class="w"> </span><span class="nv">zero_init</span><span class="o">=</span><span class="m">0</span>
<span class="w">   </span><span class="m">62</span>.23GB<span class="w"> </span><span class="p">|</span><span class="w">   </span><span class="m">2</span>.84GB<span class="w"> </span><span class="p">|</span><span class="w"> </span><span class="nv">offload_param</span><span class="o">=</span>none,<span class="w"> </span><span class="nv">offload_optimizer</span><span class="o">=</span>cpu<span class="w"> </span>,<span class="w"> </span><span class="nv">zero_init</span><span class="o">=</span><span class="m">1</span>
<span class="w">   </span><span class="m">62</span>.23GB<span class="w"> </span><span class="p">|</span><span class="w">   </span><span class="m">2</span>.84GB<span class="w"> </span><span class="p">|</span><span class="w"> </span><span class="nv">offload_param</span><span class="o">=</span>none,<span class="w"> </span><span class="nv">offload_optimizer</span><span class="o">=</span>cpu<span class="w"> </span>,<span class="w"> </span><span class="nv">zero_init</span><span class="o">=</span><span class="m">0</span>
<span class="w">    </span><span class="m">0</span>.74GB<span class="w"> </span><span class="p">|</span><span class="w">  </span><span class="m">23</span>.58GB<span class="w"> </span><span class="p">|</span><span class="w"> </span><span class="nv">offload_param</span><span class="o">=</span>none,<span class="w"> </span><span class="nv">offload_optimizer</span><span class="o">=</span>none,<span class="w"> </span><span class="nv">zero_init</span><span class="o">=</span><span class="m">1</span>
<span class="w">   </span><span class="m">31</span>.11GB<span class="w"> </span><span class="p">|</span><span class="w">  </span><span class="m">23</span>.58GB<span class="w"> </span><span class="p">|</span><span class="w"> </span><span class="nv">offload_param</span><span class="o">=</span>none,<span class="w"> </span><span class="nv">offload_optimizer</span><span class="o">=</span>none,<span class="w"> </span><span class="nv">zero_init</span><span class="o">=</span><span class="m">0</span>
</code></pre></div>
<p>所以，您需要2个32GB或更高的GPU，且不进行CPU卸载。</p>
<p>如需了解更多信息，请参阅<a href="https://deepspeed.readthedocs.io/en/latest/memory.html">内存估算器</a>。</p>
<h3 id="issues">归档Issues<a class="headerlink" href="#issues" title="Permanent link">⚓︎</a></h3>
<p>请按照以下步骤提交问题，以便我们能够迅速找到问题并帮助您解除工作阻塞。</p>
<p>在您的报告中，请始终包括以下内容：</p>
<ol>
<li>完整的Deepspeed配置文件</li>
<li>如果使用了[<code>Trainer</code>]，则包括命令行参数；如果自己编写了Trainer设置，则包括[<code>TrainingArguments</code>]参数。请不要导出[<code>TrainingArguments</code>]，因为它有几十个与问题无关的条目。</li>
<li>
<p>输出：</p>
<div class="highlight"><pre><span></span><code>python<span class="w"> </span>-c<span class="w"> </span><span class="s1">&#39;import torch; print(f&quot;torch: {torch.__version__}&quot;)&#39;</span>
python<span class="w"> </span>-c<span class="w"> </span><span class="s1">&#39;import transformers; print(f&quot;transformers: {transformers.__version__}&quot;)&#39;</span>
python<span class="w"> </span>-c<span class="w"> </span><span class="s1">&#39;import deepspeed; print(f&quot;deepspeed: {deepspeed.__version__}&quot;)&#39;</span>
</code></pre></div>
</li>
<li>
<p>如果可能，请包含一个Google Colab notebook链接，我们可以使用它来重现问题。您可以使用这个<a href="https://github.com/stas00/porting/blob/master/transformers/deepspeed/DeepSpeed_on_colab_CLI.ipynb">notebook</a>作为起点。</p>
</li>
<li>除非不可能，否则请始终使用标准数据集，而不是自定义数据集。</li>
<li>如果可能，尝试使用现有<a href="https://github.com/huggingface/transformers/tree/main/examples/pytorch">示例</a>之一来重现问题。</li>
</ol>
<p>需要考虑的因素：</p>
<ul>
<li>Deepspeed通常不是问题的原因。</li>
</ul>
<p>一些已提交的问题被证明与Deepspeed无关。也就是说，一旦将Deepspeed从设置中移除，问题仍然存在。</p>
<p>因此，如果问题明显与DeepSpeed相关，例如您可以看到有一个异常并且可以看到DeepSpeed模块涉及其中，请先重新测试没有DeepSpeed的设置。只有当问题仍然存在时，才向Deepspeed提供所有必需的细节。</p>
<ul>
<li>如果您明确问题是在Deepspeed核心中而不是集成部分，请直接向<a href="https://github.com/microsoft/DeepSpeed/">Deepspeed</a>提交问题。如果您不确定，请不要担心，无论使用哪个issue跟踪问题都可以，一旦您发布问题，我们会弄清楚并将其重定向到另一个issue跟踪（如果需要的话）。</li>
</ul>
<h3 id="troubleshooting">Troubleshooting<a class="headerlink" href="#troubleshooting" title="Permanent link">⚓︎</a></h3>
<h4 id="deepspeed_2">启动时<code>deepspeed</code>进程被终止，没有回溯<a class="headerlink" href="#deepspeed_2" title="Permanent link">⚓︎</a></h4>
<p>如果启动时<code>deepspeed</code>进程被终止，没有回溯，这通常意味着程序尝试分配的CPU内存超过了系统的限制或进程被允许分配的内存，操作系统内核杀死了该进程。这是因为您的配置文件很可能将<code>offload_optimizer</code>或<code>offload_param</code>或两者都配置为卸载到<code>cpu</code>。如果您有NVMe，可以尝试在ZeRO-3下卸载到NVMe。这里是如何<a href="https://deepspeed.readthedocs.io/en/latest/memory.html">估计特定模型所需的内存</a>。</p>
<h4 id="lossnan">训练和/或评估/预测loss为<code>NaN</code><a class="headerlink" href="#lossnan" title="Permanent link">⚓︎</a></h4>
<p>这种情况通常发生在使用bf16混合精度模式预训练的模型试图在fp16（带或不带混合精度）下使用时。大多数在TPU上训练的模型以及由谷歌发布的模型都属于这个类别（例如，几乎所有基于t5的模型）。在这种情况下，解决方案是要么使用fp32，要么在支持的情况下使用bf16（如TPU、Ampere GPU或更新的版本）。</p>
<p>另一个问题可能与使用fp16有关。当您配置此部分时：</p>
<div class="highlight"><pre><span></span><code><span class="p">{</span>
<span class="w">    </span><span class="nt">&quot;fp16&quot;</span><span class="p">:</span><span class="w"> </span><span class="p">{</span>
<span class="w">        </span><span class="nt">&quot;enabled&quot;</span><span class="p">:</span><span class="w"> </span><span class="s2">&quot;auto&quot;</span><span class="p">,</span>
<span class="w">        </span><span class="nt">&quot;loss_scale&quot;</span><span class="p">:</span><span class="w"> </span><span class="mi">0</span><span class="p">,</span>
<span class="w">        </span><span class="nt">&quot;loss_scale_window&quot;</span><span class="p">:</span><span class="w"> </span><span class="mi">1000</span><span class="p">,</span>
<span class="w">        </span><span class="nt">&quot;initial_scale_power&quot;</span><span class="p">:</span><span class="w"> </span><span class="mi">16</span><span class="p">,</span>
<span class="w">        </span><span class="nt">&quot;hysteresis&quot;</span><span class="p">:</span><span class="w"> </span><span class="mi">2</span><span class="p">,</span>
<span class="w">        </span><span class="nt">&quot;min_loss_scale&quot;</span><span class="p">:</span><span class="w"> </span><span class="mi">1</span>
<span class="w">    </span><span class="p">}</span>
<span class="p">}</span>
</code></pre></div>
<p>并且您在日志中看到Deepspeed报告<code>OVERFLOW</code>如下</p>
<div class="highlight"><pre><span></span><code>0%|                                                                                                                             | 0/189 [00:00&lt;?, ?it/s]
 [deepscale] OVERFLOW! Rank 0 Skipping step. Attempted loss scale: 262144, reducing to 262144
  1%|▌                                                                                                                    | 1/189 [00:00&lt;01:26,  2.17it/s]
 [deepscale] OVERFLOW! Rank 0 Skipping step. Attempted loss scale: 262144, reducing to 131072.0
  1%|█▏
 [...]
 [deepscale] OVERFLOW! Rank 0 Skipping step. Attempted loss scale: 1, reducing to 1
 14%|████████████████▌                                                                                                   | 27/189 [00:14&lt;01:13,  2.21it/s]
 [deepscale] OVERFLOW! Rank 0 Skipping step. Attempted loss scale: 1, reducing to 1
 15%|█████████████████▏                                                                                                  | 28/189 [00:14&lt;01:13,  2.18it/s]
 [deepscale] OVERFLOW! Rank 0 Skipping step. Attempted loss scale: 1, reducing to 1
 15%|█████████████████▊                                                                                                  | 29/189 [00:15&lt;01:13,  2.18it/s]
 [deepscale] OVERFLOW! Rank 0 Skipping step. Attempted loss scale: 1, reducing to 1
[...]
</code></pre></div>
<p>这意味着Deepspeed损失缩放器无法找到一个克服损失溢出的缩放系数。</p>
<p>在这种情况下，通常需要提高<code>initial_scale_power</code>的值。将其设置为<code>"initial_scale_power": 32</code>通常会解决问题。</p>
<h3 id="_11">注意事项<a class="headerlink" href="#_11" title="Permanent link">⚓︎</a></h3>
<ul>
<li>DeepSpeed 与 PyTorch [<code>Trainer</code>] 一起工作，但不与 TF [<code>TFTrainer</code>] 一起工作。</li>
<li>尽管 DeepSpeed 有一个可安装的 PyPI 包，但强烈建议从源代码安装它，以最好地匹配您的硬件，如果您需要启用某些功能，如 1-bit Adam，这些功能在 pypi 发行版中不可用。</li>
<li>您不必使用🤗  Transformers的 [<code>Trainer</code>] 来使用 DeepSpeed   - 您可以使用任何模型与自己的训练器，您还需要根据 <a href="https://www.deepspeed.ai/getting-started/#writing-deepspeed-models">DeepSpeed 集成说明</a> 调整后者。</li>
</ul>
<h2 id="non-trainer-deepspeed">Non-Trainer Deepspeed集成<a class="headerlink" href="#non-trainer-deepspeed" title="Permanent link">⚓︎</a></h2>
<p>当<code>Trainer</code>没有被使用时，<code>~integrations.HfDeepSpeedConfig</code>被用来将Deepspeed集成到huggingface的Transformers核心功能中。它唯一做的事情就是在<code>from_pretrained</code>调用期间处理Deepspeed ZeRO-3参数收集和将模型自动分割到多个GPU上。除此之外，您需要自己完成其他所有工作。</p>
<p>当使用<code>Trainer</code>时，所有事情都自动得到了处理。</p>
<p>当不使用<code>Trainer</code>时，为了高效地部署Deepspeed ZeRO-3，您必须在实例化模型之前实例化<code>~integrations.HfDeepSpeedConfig</code>对象并保持该对象活跃。</p>
<p>如果您正在使用Deepspeed ZeRO-1或ZeRO-2，您根本不需要使用<code>HfDeepSpeedConfig</code>。</p>
<p>以预训练模型为例:</p>
<div class="highlight"><pre><span></span><code><span class="kn">from</span> <span class="nn">transformers.integrations</span> <span class="kn">import</span> <span class="n">HfDeepSpeedConfig</span>
<span class="kn">from</span> <span class="nn">transformers</span> <span class="kn">import</span> <span class="n">AutoModel</span>
<span class="kn">import</span> <span class="nn">deepspeed</span>

<span class="n">ds_config</span> <span class="o">=</span> <span class="p">{</span><span class="o">...</span><span class="p">}</span>  <span class="c1"># deepspeed config object or path to the file</span>
<span class="c1"># must run before instantiating the model to detect zero 3</span>
<span class="n">dschf</span> <span class="o">=</span> <span class="n">HfDeepSpeedConfig</span><span class="p">(</span><span class="n">ds_config</span><span class="p">)</span>  <span class="c1"># keep this object alive</span>
<span class="n">model</span> <span class="o">=</span> <span class="n">AutoModel</span><span class="o">.</span><span class="n">from_pretrained</span><span class="p">(</span><span class="s2">&quot;gpt2&quot;</span><span class="p">)</span>
<span class="n">engine</span> <span class="o">=</span> <span class="n">deepspeed</span><span class="o">.</span><span class="n">initialize</span><span class="p">(</span><span class="n">model</span><span class="o">=</span><span class="n">model</span><span class="p">,</span> <span class="n">config_params</span><span class="o">=</span><span class="n">ds_config</span><span class="p">,</span> <span class="o">...</span><span class="p">)</span>
</code></pre></div>
<p>或者以非预训练模型为例：</p>
<div class="highlight"><pre><span></span><code><span class="kn">from</span> <span class="nn">transformers.integrations</span> <span class="kn">import</span> <span class="n">HfDeepSpeedConfig</span>
<span class="kn">from</span> <span class="nn">transformers</span> <span class="kn">import</span> <span class="n">AutoModel</span><span class="p">,</span> <span class="n">AutoConfig</span>
<span class="kn">import</span> <span class="nn">deepspeed</span>

<span class="n">ds_config</span> <span class="o">=</span> <span class="p">{</span><span class="o">...</span><span class="p">}</span>  <span class="c1"># deepspeed config object or path to the file</span>
<span class="c1"># must run before instantiating the model to detect zero 3</span>
<span class="n">dschf</span> <span class="o">=</span> <span class="n">HfDeepSpeedConfig</span><span class="p">(</span><span class="n">ds_config</span><span class="p">)</span>  <span class="c1"># keep this object alive</span>
<span class="n">config</span> <span class="o">=</span> <span class="n">AutoConfig</span><span class="o">.</span><span class="n">from_pretrained</span><span class="p">(</span><span class="s2">&quot;gpt2&quot;</span><span class="p">)</span>
<span class="n">model</span> <span class="o">=</span> <span class="n">AutoModel</span><span class="o">.</span><span class="n">from_config</span><span class="p">(</span><span class="n">config</span><span class="p">)</span>
<span class="n">engine</span> <span class="o">=</span> <span class="n">deepspeed</span><span class="o">.</span><span class="n">initialize</span><span class="p">(</span><span class="n">model</span><span class="o">=</span><span class="n">model</span><span class="p">,</span> <span class="n">config_params</span><span class="o">=</span><span class="n">ds_config</span><span class="p">,</span> <span class="o">...</span><span class="p">)</span>
</code></pre></div>
<p>请注意，如果您没有使用[<code>Trainer</code>]集成，您完全需要自己动手。基本上遵循<a href="https://www.deepspeed.ai/">Deepspeed</a>网站上的文档。同时，您必须显式配置配置文件 - 不能使用<code>"auto"</code>值，而必须放入实际值。</p>
<h2 id="hfdeepspeedconfig">HfDeepSpeedConfig<a class="headerlink" href="#hfdeepspeedconfig" title="Permanent link">⚓︎</a></h2>
<p>[[autodoc]] integrations.HfDeepSpeedConfig
    - all</p>
<h3 id="deepspeed-zero">自定义DeepSpeed ZeRO推理<a class="headerlink" href="#deepspeed-zero" title="Permanent link">⚓︎</a></h3>
<p>以下是一个示例，演示了在无法将模型放入单个 GPU 时如果不使用[Trainer]进行 DeepSpeed ZeRO 推理 。该解决方案包括使用额外的 GPU 或/和将 GPU 内存卸载到 CPU 内存。</p>
<p>这里要理解的重要细微差别是，ZeRO的设计方式可以让您在不同的GPU上并行处理不同的输入。</p>
<p>这个例子有很多注释，并且是自文档化的。</p>
<p>请确保：</p>
<ol>
<li>如果您有足够的GPU内存（因为这会减慢速度），禁用CPU offload。</li>
<li>如果您拥有Ampere架构或更新的GPU，启用bf16以加快速度。如果您没有这种硬件，只要不使用任何在bf16混合精度下预训练的模型（如大多数t5模型），就可以启用fp16。否则这些模型通常在fp16中溢出，您会看到输出无效结果。</li>
</ol>
<div class="highlight"><pre><span></span><code><span class="ch">#!/usr/bin/env python</span>

<span class="c1"># This script demonstrates how to use Deepspeed ZeRO in an inference mode when one can&#39;t fit a model</span>
<span class="c1"># into a single GPU</span>
<span class="c1">#</span>
<span class="c1"># 1. Use 1 GPU with CPU offload</span>
<span class="c1"># 2. Or use multiple GPUs instead</span>
<span class="c1">#</span>
<span class="c1"># First you need to install deepspeed: pip install deepspeed</span>
<span class="c1">#</span>
<span class="c1"># Here we use a 3B &quot;bigscience/T0_3B&quot; model which needs about 15GB GPU RAM - so 1 largish or 2</span>
<span class="c1"># small GPUs can handle it. or 1 small GPU and a lot of CPU memory.</span>
<span class="c1">#</span>
<span class="c1"># To use a larger model like &quot;bigscience/T0&quot; which needs about 50GB, unless you have an 80GB GPU -</span>
<span class="c1"># you will need 2-4 gpus. And then you can adapt the script to handle more gpus if you want to</span>
<span class="c1"># process multiple inputs at once.</span>
<span class="c1">#</span>
<span class="c1"># The provided deepspeed config also activates CPU memory offloading, so chances are that if you</span>
<span class="c1"># have a lot of available CPU memory and you don&#39;t mind a slowdown you should be able to load a</span>
<span class="c1"># model that doesn&#39;t normally fit into a single GPU. If you have enough GPU memory the program will</span>
<span class="c1"># run faster if you don&#39;t want offload to CPU - so disable that section then.</span>
<span class="c1">#</span>
<span class="c1"># To deploy on 1 gpu:</span>
<span class="c1">#</span>
<span class="c1"># deepspeed --num_gpus 1 t0.py</span>
<span class="c1"># or:</span>
<span class="c1"># python -m torch.distributed.run --nproc_per_node=1 t0.py</span>
<span class="c1">#</span>
<span class="c1"># To deploy on 2 gpus:</span>
<span class="c1">#</span>
<span class="c1"># deepspeed --num_gpus 2 t0.py</span>
<span class="c1"># or:</span>
<span class="c1"># python -m torch.distributed.run --nproc_per_node=2 t0.py</span>


<span class="kn">from</span> <span class="nn">transformers</span> <span class="kn">import</span> <span class="n">AutoTokenizer</span><span class="p">,</span> <span class="n">AutoConfig</span><span class="p">,</span> <span class="n">AutoModelForSeq2SeqLM</span>
<span class="kn">from</span> <span class="nn">transformers.integrations</span> <span class="kn">import</span> <span class="n">HfDeepSpeedConfig</span>
<span class="kn">import</span> <span class="nn">deepspeed</span>
<span class="kn">import</span> <span class="nn">os</span>
<span class="kn">import</span> <span class="nn">torch</span>

<span class="n">os</span><span class="o">.</span><span class="n">environ</span><span class="p">[</span><span class="s2">&quot;TOKENIZERS_PARALLELISM&quot;</span><span class="p">]</span> <span class="o">=</span> <span class="s2">&quot;false&quot;</span>  <span class="c1"># To avoid warnings about parallelism in tokenizers</span>

<span class="c1"># distributed setup</span>
<span class="n">local_rank</span> <span class="o">=</span> <span class="nb">int</span><span class="p">(</span><span class="n">os</span><span class="o">.</span><span class="n">getenv</span><span class="p">(</span><span class="s2">&quot;LOCAL_RANK&quot;</span><span class="p">,</span> <span class="s2">&quot;0&quot;</span><span class="p">))</span>
<span class="n">world_size</span> <span class="o">=</span> <span class="nb">int</span><span class="p">(</span><span class="n">os</span><span class="o">.</span><span class="n">getenv</span><span class="p">(</span><span class="s2">&quot;WORLD_SIZE&quot;</span><span class="p">,</span> <span class="s2">&quot;1&quot;</span><span class="p">))</span>
<span class="n">torch</span><span class="o">.</span><span class="n">cuda</span><span class="o">.</span><span class="n">set_device</span><span class="p">(</span><span class="n">local_rank</span><span class="p">)</span>
<span class="n">deepspeed</span><span class="o">.</span><span class="n">init_distributed</span><span class="p">()</span>

<span class="n">model_name</span> <span class="o">=</span> <span class="s2">&quot;bigscience/T0_3B&quot;</span>

<span class="n">config</span> <span class="o">=</span> <span class="n">AutoConfig</span><span class="o">.</span><span class="n">from_pretrained</span><span class="p">(</span><span class="n">model_name</span><span class="p">)</span>
<span class="n">model_hidden_size</span> <span class="o">=</span> <span class="n">config</span><span class="o">.</span><span class="n">d_model</span>

<span class="c1"># batch size has to be divisible by world_size, but can be bigger than world_size</span>
<span class="n">train_batch_size</span> <span class="o">=</span> <span class="mi">1</span> <span class="o">*</span> <span class="n">world_size</span>

<span class="c1"># ds_config notes</span>
<span class="c1">#</span>
<span class="c1"># - enable bf16 if you use Ampere or higher GPU - this will run in mixed precision and will be</span>
<span class="c1"># faster.</span>
<span class="c1">#</span>
<span class="c1"># - for older GPUs you can enable fp16, but it&#39;ll only work for non-bf16 pretrained models - e.g.</span>
<span class="c1"># all official t5 models are bf16-pretrained</span>
<span class="c1">#</span>
<span class="c1"># - set offload_param.device to &quot;none&quot; or completely remove the `offload_param` section if you don&#39;t</span>
<span class="c1"># - want CPU offload</span>
<span class="c1">#</span>
<span class="c1"># - if using `offload_param` you can manually finetune stage3_param_persistence_threshold to control</span>
<span class="c1"># - which params should remain on gpus - the larger the value the smaller the offload size</span>
<span class="c1">#</span>
<span class="c1"># For indepth info on Deepspeed config see</span>
<span class="c1"># https://huggingface.co/docs/transformers/main/main_classes/deepspeed</span>

<span class="c1"># keeping the same format as json for consistency, except it uses lower case for true/false</span>
<span class="c1"># fmt: off</span>
<span class="n">ds_config</span> <span class="o">=</span> <span class="p">{</span>
    <span class="s2">&quot;fp16&quot;</span><span class="p">:</span> <span class="p">{</span>
        <span class="s2">&quot;enabled&quot;</span><span class="p">:</span> <span class="kc">False</span>
    <span class="p">},</span>
    <span class="s2">&quot;bf16&quot;</span><span class="p">:</span> <span class="p">{</span>
        <span class="s2">&quot;enabled&quot;</span><span class="p">:</span> <span class="kc">False</span>
    <span class="p">},</span>
    <span class="s2">&quot;zero_optimization&quot;</span><span class="p">:</span> <span class="p">{</span>
        <span class="s2">&quot;stage&quot;</span><span class="p">:</span> <span class="mi">3</span><span class="p">,</span>
        <span class="s2">&quot;offload_param&quot;</span><span class="p">:</span> <span class="p">{</span>
            <span class="s2">&quot;device&quot;</span><span class="p">:</span> <span class="s2">&quot;cpu&quot;</span><span class="p">,</span>
            <span class="s2">&quot;pin_memory&quot;</span><span class="p">:</span> <span class="kc">True</span>
        <span class="p">},</span>
        <span class="s2">&quot;overlap_comm&quot;</span><span class="p">:</span> <span class="kc">True</span><span class="p">,</span>
        <span class="s2">&quot;contiguous_gradients&quot;</span><span class="p">:</span> <span class="kc">True</span><span class="p">,</span>
        <span class="s2">&quot;reduce_bucket_size&quot;</span><span class="p">:</span> <span class="n">model_hidden_size</span> <span class="o">*</span> <span class="n">model_hidden_size</span><span class="p">,</span>
        <span class="s2">&quot;stage3_prefetch_bucket_size&quot;</span><span class="p">:</span> <span class="mf">0.9</span> <span class="o">*</span> <span class="n">model_hidden_size</span> <span class="o">*</span> <span class="n">model_hidden_size</span><span class="p">,</span>
        <span class="s2">&quot;stage3_param_persistence_threshold&quot;</span><span class="p">:</span> <span class="mi">10</span> <span class="o">*</span> <span class="n">model_hidden_size</span>
    <span class="p">},</span>
    <span class="s2">&quot;steps_per_print&quot;</span><span class="p">:</span> <span class="mi">2000</span><span class="p">,</span>
    <span class="s2">&quot;train_batch_size&quot;</span><span class="p">:</span> <span class="n">train_batch_size</span><span class="p">,</span>
    <span class="s2">&quot;train_micro_batch_size_per_gpu&quot;</span><span class="p">:</span> <span class="mi">1</span><span class="p">,</span>
    <span class="s2">&quot;wall_clock_breakdown&quot;</span><span class="p">:</span> <span class="kc">False</span>
<span class="p">}</span>
<span class="c1"># fmt: on</span>

<span class="c1"># next line instructs transformers to partition the model directly over multiple gpus using</span>
<span class="c1"># deepspeed.zero.Init when model&#39;s `from_pretrained` method is called.</span>
<span class="c1">#</span>
<span class="c1"># **it has to be run before loading the model AutoModelForSeq2SeqLM.from_pretrained(model_name)**</span>
<span class="c1">#</span>
<span class="c1"># otherwise the model will first be loaded normally and only partitioned at forward time which is</span>
<span class="c1"># less efficient and when there is little CPU RAM may fail</span>
<span class="n">dschf</span> <span class="o">=</span> <span class="n">HfDeepSpeedConfig</span><span class="p">(</span><span class="n">ds_config</span><span class="p">)</span>  <span class="c1"># keep this object alive</span>

<span class="c1"># now a model can be loaded.</span>
<span class="n">model</span> <span class="o">=</span> <span class="n">AutoModelForSeq2SeqLM</span><span class="o">.</span><span class="n">from_pretrained</span><span class="p">(</span><span class="n">model_name</span><span class="p">)</span>

<span class="c1"># initialise Deepspeed ZeRO and store only the engine object</span>
<span class="n">ds_engine</span> <span class="o">=</span> <span class="n">deepspeed</span><span class="o">.</span><span class="n">initialize</span><span class="p">(</span><span class="n">model</span><span class="o">=</span><span class="n">model</span><span class="p">,</span> <span class="n">config_params</span><span class="o">=</span><span class="n">ds_config</span><span class="p">)[</span><span class="mi">0</span><span class="p">]</span>
<span class="n">ds_engine</span><span class="o">.</span><span class="n">module</span><span class="o">.</span><span class="n">eval</span><span class="p">()</span>  <span class="c1"># inference</span>

<span class="c1"># Deepspeed ZeRO can process unrelated inputs on each GPU. So for 2 gpus you process 2 inputs at once.</span>
<span class="c1"># If you use more GPUs adjust for more.</span>
<span class="c1"># And of course if you have just one input to process you then need to pass the same string to both gpus</span>
<span class="c1"># If you use only one GPU, then you will have only rank 0.</span>
<span class="n">rank</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">distributed</span><span class="o">.</span><span class="n">get_rank</span><span class="p">()</span>
<span class="k">if</span> <span class="n">rank</span> <span class="o">==</span> <span class="mi">0</span><span class="p">:</span>
    <span class="n">text_in</span> <span class="o">=</span> <span class="s2">&quot;Is this review positive or negative? Review: this is the best cast iron skillet you will ever buy&quot;</span>
<span class="k">elif</span> <span class="n">rank</span> <span class="o">==</span> <span class="mi">1</span><span class="p">:</span>
    <span class="n">text_in</span> <span class="o">=</span> <span class="s2">&quot;Is this review positive or negative? Review: this is the worst restaurant ever&quot;</span>

<span class="n">tokenizer</span> <span class="o">=</span> <span class="n">AutoTokenizer</span><span class="o">.</span><span class="n">from_pretrained</span><span class="p">(</span><span class="n">model_name</span><span class="p">)</span>
<span class="n">inputs</span> <span class="o">=</span> <span class="n">tokenizer</span><span class="o">.</span><span class="n">encode</span><span class="p">(</span><span class="n">text_in</span><span class="p">,</span> <span class="n">return_tensors</span><span class="o">=</span><span class="s2">&quot;pt&quot;</span><span class="p">)</span><span class="o">.</span><span class="n">to</span><span class="p">(</span><span class="n">device</span><span class="o">=</span><span class="n">local_rank</span><span class="p">)</span>
<span class="k">with</span> <span class="n">torch</span><span class="o">.</span><span class="n">no_grad</span><span class="p">():</span>
    <span class="n">outputs</span> <span class="o">=</span> <span class="n">ds_engine</span><span class="o">.</span><span class="n">module</span><span class="o">.</span><span class="n">generate</span><span class="p">(</span><span class="n">inputs</span><span class="p">,</span> <span class="n">synced_gpus</span><span class="o">=</span><span class="kc">True</span><span class="p">)</span>
<span class="n">text_out</span> <span class="o">=</span> <span class="n">tokenizer</span><span class="o">.</span><span class="n">decode</span><span class="p">(</span><span class="n">outputs</span><span class="p">[</span><span class="mi">0</span><span class="p">],</span> <span class="n">skip_special_tokens</span><span class="o">=</span><span class="kc">True</span><span class="p">)</span>
<span class="nb">print</span><span class="p">(</span><span class="sa">f</span><span class="s2">&quot;rank</span><span class="si">{</span><span class="n">rank</span><span class="si">}</span><span class="s2">:</span><span class="se">\n</span><span class="s2">   in=</span><span class="si">{</span><span class="n">text_in</span><span class="si">}</span><span class="se">\n</span><span class="s2">  out=</span><span class="si">{</span><span class="n">text_out</span><span class="si">}</span><span class="s2">&quot;</span><span class="p">)</span>
</code></pre></div>
<p>让我们保存它为 <code>t0.py</code>并运行：
<div class="highlight"><pre><span></span><code>$ deepspeed --num_gpus 2 t0.py
rank0:
   in=Is this review positive or negative? Review: this is the best cast iron skillet you will ever buy
  out=Positive
rank1:
   in=Is this review positive or negative? Review: this is the worst restaurant ever
  out=negative
</code></pre></div></p>
<p>这是一个非常基本的例子，您需要根据自己的需求进行修改。</p>
<h3 id="generate"><code>generate</code> 的差异<a class="headerlink" href="#generate" title="Permanent link">⚓︎</a></h3>
<p>在使用ZeRO stage 3的多GPU时，需要通过调用<code>generate(..., synced_gpus=True)</code>来同步GPU。如果一个GPU在其它GPU之前完成生成，整个系统将挂起，因为其他GPU无法从停止生成的GPU接收权重分片。</p>
<p>从<code>transformers&gt;=4.28</code>开始，如果没有明确指定<code>synced_gpus</code>，检测到这些条件后它将自动设置为<code>True</code>。但如果您需要覆盖<code>synced_gpus</code>的值，仍然可以这样做。</p>
<h2 id="deepspeed_3">测试 DeepSpeed 集成<a class="headerlink" href="#deepspeed_3" title="Permanent link">⚓︎</a></h2>
<p>如果您提交了一个涉及DeepSpeed集成的PR，请注意我们的CircleCI PR CI设置没有GPU，因此我们只在另一个CI夜间运行需要GPU的测试。因此，如果您在PR中获得绿色的CI报告，并不意味着DeepSpeed测试通过。</p>
<p>要运行DeepSpeed测试，请至少运行以下命令：</p>
<div class="highlight"><pre><span></span><code>RUN_SLOW=1 pytest tests/deepspeed/test_deepspeed.py
</code></pre></div>
<p>如果你更改了任何模型或PyTorch示例代码，请同时运行多模型测试。以下将运行所有DeepSpeed测试：</p>
<div class="highlight"><pre><span></span><code>RUN_SLOW=1 pytest tests/deepspeed
</code></pre></div>
<h2 id="deepspeed_4">主要的DeepSpeed资源<a class="headerlink" href="#deepspeed_4" title="Permanent link">⚓︎</a></h2>
<ul>
<li><a href="https://github.com/microsoft/deepspeed">项目GitHub</a></li>
<li><a href="https://www.deepspeed.ai/getting-started/">使用文档</a></li>
<li><a href="https://deepspeed.readthedocs.io/en/latest/index.html">API文档</a></li>
<li><a href="https://www.microsoft.com/en-us/research/search/?q=deepspeed">博客文章</a></li>
</ul>
<p>论文:</p>
<ul>
<li><a href="https://arxiv.org/abs/1910.02054">ZeRO: Memory Optimizations Toward Training Trillion Parameter Models</a></li>
<li><a href="https://arxiv.org/abs/2101.06840">ZeRO-Offload: Democratizing Billion-Scale Model Training</a></li>
<li><a href="https://arxiv.org/abs/2104.07857">ZeRO-Infinity: Breaking the GPU Memory Wall for Extreme Scale Deep Learning</a></li>
</ul>
<p>最后，请记住，HuggingFace [<code>Trainer</code>]仅集成了DeepSpeed，因此如果您在使用DeepSpeed时遇到任何问题或疑问，请在<a href="https://github.com/microsoft/DeepSpeed/issues">DeepSpeed GitHub</a>上提交一个issue。</p>

  <hr>
<div class="md-source-file">
  <small>
    
      最后更新:
      <span class="git-revision-date-localized-plugin git-revision-date-localized-plugin-date">November 25, 2023</span>
      
        <br>
        创建日期:
        <span class="git-revision-date-localized-plugin git-revision-date-localized-plugin-date">November 25, 2023</span>
      
    
  </small>
</div>





                
              </article>
            </div>
          
          
        </div>
        
          <button type="button" class="md-top md-icon" data-md-component="top" hidden>
  
  <svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 24 24"><path d="M13 20h-2V8l-5.5 5.5-1.42-1.42L12 4.16l7.92 7.92-1.42 1.42L13 8v12Z"/></svg>
  回到页面顶部
</button>
        
      </main>
      
        <footer class="md-footer">
  
    
      
      <nav class="md-footer__inner md-grid" aria-label="页脚" >
        
          
          <a href="../../4-%E6%A6%82%E5%BF%B5%E6%8C%87%E5%8D%97/task_summary/" class="md-footer__link md-footer__link--prev" aria-label="上一页: Task summary">
            <div class="md-footer__button md-icon">
              
              <svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 24 24"><path d="M20 11v2H8l5.5 5.5-1.42 1.42L4.16 12l7.92-7.92L13.5 5.5 8 11h12Z"/></svg>
            </div>
            <div class="md-footer__title">
              <span class="md-footer__direction">
                上一页
              </span>
              <div class="md-ellipsis">
                Task summary
              </div>
            </div>
          </a>
        
        
          
          <a href="../model/" class="md-footer__link md-footer__link--next" aria-label="下一页: Model">
            <div class="md-footer__title">
              <span class="md-footer__direction">
                下一页
              </span>
              <div class="md-ellipsis">
                Model
              </div>
            </div>
            <div class="md-footer__button md-icon">
              
              <svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 24 24"><path d="M4 11v2h12l-5.5 5.5 1.42 1.42L19.84 12l-7.92-7.92L10.5 5.5 16 11H4Z"/></svg>
            </div>
          </a>
        
      </nav>
    
  
  <div class="md-footer-meta md-typeset">
    <div class="md-footer-meta__inner md-grid">
      <div class="md-copyright">
  
    <div class="md-copyright__highlight">
      Copyright &copy; 2023 Ean Yang
    </div>
  
  
</div>
      
        <div class="md-social">
  
    
    
    
    
    <a href="https://github.com/YQisme" target="_blank" rel="noopener" title="github主页" class="md-social__link">
      <svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 496 512"><!--! Font Awesome Free 6.4.2 by @fontawesome - https://fontawesome.com License - https://fontawesome.com/license/free (Icons: CC BY 4.0, Fonts: SIL OFL 1.1, Code: MIT License) Copyright 2023 Fonticons, Inc.--><path d="M165.9 397.4c0 2-2.3 3.6-5.2 3.6-3.3.3-5.6-1.3-5.6-3.6 0-2 2.3-3.6 5.2-3.6 3-.3 5.6 1.3 5.6 3.6zm-31.1-4.5c-.7 2 1.3 4.3 4.3 4.9 2.6 1 5.6 0 6.2-2s-1.3-4.3-4.3-5.2c-2.6-.7-5.5.3-6.2 2.3zm44.2-1.7c-2.9.7-4.9 2.6-4.6 4.9.3 2 2.9 3.3 5.9 2.6 2.9-.7 4.9-2.6 4.6-4.6-.3-1.9-3-3.2-5.9-2.9zM244.8 8C106.1 8 0 113.3 0 252c0 110.9 69.8 205.8 169.5 239.2 12.8 2.3 17.3-5.6 17.3-12.1 0-6.2-.3-40.4-.3-61.4 0 0-70 15-84.7-29.8 0 0-11.4-29.1-27.8-36.6 0 0-22.9-15.7 1.6-15.4 0 0 24.9 2 38.6 25.8 21.9 38.6 58.6 27.5 72.9 20.9 2.3-16 8.8-27.1 16-33.7-55.9-6.2-112.3-14.3-112.3-110.5 0-27.5 7.6-41.3 23.6-58.9-2.6-6.5-11.1-33.3 2.6-67.9 20.9-6.5 69 27 69 27 20-5.6 41.5-8.5 62.8-8.5s42.8 2.9 62.8 8.5c0 0 48.1-33.6 69-27 13.7 34.7 5.2 61.4 2.6 67.9 16 17.7 25.8 31.5 25.8 58.9 0 96.5-58.9 104.2-114.8 110.5 9.2 7.9 17 22.9 17 46.4 0 33.7-.3 75.4-.3 83.6 0 6.5 4.6 14.4 17.3 12.1C428.2 457.8 496 362.9 496 252 496 113.3 383.5 8 244.8 8zM97.2 352.9c-1.3 1-1 3.3.7 5.2 1.6 1.6 3.9 2.3 5.2 1 1.3-1 1-3.3-.7-5.2-1.6-1.6-3.9-2.3-5.2-1zm-10.8-8.1c-.7 1.3.3 2.9 2.3 3.9 1.6 1 3.6.7 4.3-.7.7-1.3-.3-2.9-2.3-3.9-2-.6-3.6-.3-4.3.7zm32.4 35.6c-1.6 1.3-1 4.3 1.3 6.2 2.3 2.3 5.2 2.6 6.5 1 1.3-1.3.7-4.3-1.3-6.2-2.2-2.3-5.2-2.6-6.5-1zm-11.4-14.7c-1.6 1-1.6 3.6 0 5.9 1.6 2.3 4.3 3.3 5.6 2.3 1.6-1.3 1.6-3.9 0-6.2-1.4-2.3-4-3.3-5.6-2z"/></svg>
    </a>
  
    
    
    
    
    <a href="https://space.bilibili.com/244185393?spm_id_from=333.788.0.0" target="_blank" rel="noopener" title="b站主页" class="md-social__link">
      <svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 512 512"><!--! Font Awesome Free 6.4.2 by @fontawesome - https://fontawesome.com License - https://fontawesome.com/license/free (Icons: CC BY 4.0, Fonts: SIL OFL 1.1, Code: MIT License) Copyright 2023 Fonticons, Inc.--><path d="M488.6 104.1c16.7 18.1 24.4 39.7 23.3 65.7v202.4c-.4 26.4-9.2 48.1-26.5 65.1-17.2 17-39.1 25.9-65.5 26.7H92.02c-26.45-.8-48.21-9.8-65.28-27.2C9.682 419.4.767 396.5 0 368.2V169.8c.767-26 9.682-47.6 26.74-65.7C43.81 87.75 65.57 78.77 92.02 78h29.38L96.05 52.19c-5.75-5.73-8.63-13-8.63-21.79 0-8.8 2.88-16.06 8.63-21.797C101.8 2.868 109.1 0 117.9 0s16.1 2.868 21.9 8.603L213.1 78h88l74.5-69.397C381.7 2.868 389.2 0 398 0c8.8 0 16.1 2.868 21.9 8.603 5.7 5.737 8.6 12.997 8.6 21.797 0 8.79-2.9 16.06-8.6 21.79L394.6 78h29.3c26.4.77 48 9.75 64.7 26.1zm-38.8 69.7c-.4-9.6-3.7-17.4-10.7-23.5-5.2-6.1-14-9.4-22.7-9.8H96.05c-9.59.4-17.45 3.7-23.58 9.8-6.14 6.1-9.4 13.9-9.78 23.5v194.4c0 9.2 3.26 17 9.78 23.5s14.38 9.8 23.58 9.8H416.4c9.2 0 17-3.3 23.3-9.8 6.3-6.5 9.7-14.3 10.1-23.5V173.8zm-264.3 42.7c6.3 6.3 9.7 14.1 10.1 23.2V273c-.4 9.2-3.7 16.9-9.8 23.2-6.2 6.3-14 9.5-23.6 9.5-9.6 0-17.5-3.2-23.6-9.5-6.1-6.3-9.4-14-9.8-23.2v-33.3c.4-9.1 3.8-16.9 10.1-23.2 6.3-6.3 13.2-9.6 23.3-10 9.2.4 17 3.7 23.3 10zm191.5 0c6.3 6.3 9.7 14.1 10.1 23.2V273c-.4 9.2-3.7 16.9-9.8 23.2-6.1 6.3-14 9.5-23.6 9.5-9.6 0-17.4-3.2-23.6-9.5-7-6.3-9.4-14-9.7-23.2v-33.3c.3-9.1 3.7-16.9 10-23.2 6.3-6.3 14.1-9.6 23.3-10 9.2.4 17 3.7 23.3 10z"/></svg>
    </a>
  
    
    
    
    
    <a href="https://eanyang7.com" target="_blank" rel="noopener" title="个人主页" class="md-social__link">
      <svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 320 512"><!--! Font Awesome Free 6.4.2 by @fontawesome - https://fontawesome.com License - https://fontawesome.com/license/free (Icons: CC BY 4.0, Fonts: SIL OFL 1.1, Code: MIT License) Copyright 2023 Fonticons, Inc.--><path d="M112 48a48 48 0 1 1 96 0 48 48 0 1 1-96 0zm40 304v128c0 17.7-14.3 32-32 32s-32-14.3-32-32V256.9l-28.6 47.6c-9.1 15.1-28.8 20-43.9 10.9s-20-28.8-10.9-43.9l58.3-97c17.4-28.9 48.6-46.6 82.3-46.6h29.7c33.7 0 64.9 17.7 82.3 46.6l58.3 97c9.1 15.1 4.2 34.8-10.9 43.9s-34.8 4.2-43.9-10.9L232 256.9V480c0 17.7-14.3 32-32 32s-32-14.3-32-32V352h-16z"/></svg>
    </a>
  
</div>
      
    </div>
  </div>
</footer>
      
    </div>
    <div class="md-dialog" data-md-component="dialog">
      <div class="md-dialog__inner md-typeset"></div>
    </div>
    
      <div class="md-progress" data-md-component="progress" role="progressbar"></div>
    
    
    <script id="__config" type="application/json">{"base": "../..", "features": ["navigation.instant", "navigation.instant.progress", "navigation.tracking", "navigation.prune", "navigation.top", "toc.follow", "header.autohide", "navigation.footer", "search.suggest", "search.highlight", "search.share", "content.action.edit", "content.action.view", "content.code.copy"], "search": "../../assets/javascripts/workers/search.f886a092.min.js", "translations": {"clipboard.copied": "\u5df2\u590d\u5236", "clipboard.copy": "\u590d\u5236", "search.result.more.one": "\u5728\u8be5\u9875\u4e0a\u8fd8\u6709 1 \u4e2a\u7b26\u5408\u6761\u4ef6\u7684\u7ed3\u679c", "search.result.more.other": "\u5728\u8be5\u9875\u4e0a\u8fd8\u6709 # \u4e2a\u7b26\u5408\u6761\u4ef6\u7684\u7ed3\u679c", "search.result.none": "\u6ca1\u6709\u627e\u5230\u7b26\u5408\u6761\u4ef6\u7684\u7ed3\u679c", "search.result.one": "\u627e\u5230 1 \u4e2a\u7b26\u5408\u6761\u4ef6\u7684\u7ed3\u679c", "search.result.other": "# \u4e2a\u7b26\u5408\u6761\u4ef6\u7684\u7ed3\u679c", "search.result.placeholder": "\u952e\u5165\u4ee5\u5f00\u59cb\u641c\u7d22", "search.result.term.missing": "\u7f3a\u5c11", "select.version": "\u9009\u62e9\u5f53\u524d\u7248\u672c"}}</script>
    
    
      <script src="../../assets/javascripts/bundle.6c14ae12.min.js"></script>
      
    
  </body>
</html>